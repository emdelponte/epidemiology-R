{
  "hash": "a12b379fe576c69f5745bb1df2c72dd6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Reliability and accuracy\"\n---\n\n\n\n\n\n\n# Severity data\n\n## Terminology\n\nDisease severity, mainly when expressed in percent area diseased assessed visually, is acknowledged as a more difficult and less time- and cost-effective plant disease variable to obtain. However, errors may occur even when assessing a more objective measure such as incidence. This is the case when an incorrect assignment or confusion of symptoms occur. In either case, the quality of the assessment of any disease variable is very important and should be gauged in the studies. Several terms can be used when evaluating the quality of disease assessments, including reliability, precision, accuracy or agreement.\n\n**Reliability**: The extent to which the same estimates or measurements of diseased specimens obtained under different conditions yield similar results. There are two types. The *inter-rater reliability* (or reproducibility) is a measure of consistency of disease assessment across the same specimens between raters or devices. The *intra-rater* reliability (or repeatability) measures consistency by the same rater or instrument on the same specimens (e.g. two assessments in time by the same rater).\n\n![Two types of reliability of estimates or measures of plant disease intensity](imgs/reliability.png){#fig-reliability.png fig-align=\"center\" width=\"505\"}\n\n**Precision:** A statistical term to express the measure of variability of the estimates or measurements of disease on the same specimens obtained by different raters (or instruments). However, reliable or precise estimates (or measurements) are not necessarily close to an actual value, but precision is a component of accuracy or agreement.\n\n**Accuracy or agreement**: These two terms can be treated as synonymous in plant pathological research. They refer to the closeness (or concordance) of an estimate or measurement to the actual severity value for a specimen on the same scale. Actual values may be obtained using various methods, against which estimates or measurements using an experimental assessment method are compared.\n\nAn analogy commonly used to explain accuracy and precision is the archer shooting arrows at a target and trying to hit the bull's eye (center of the target) with each of five arrows. The figure below is used to demonstrate four situations from the combination of two levels (high and low) for precision and accuracy. The figure was produced using the `ggplot` function of *ggplot2* package.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\ntarget <- \n  ggplot(data.frame(c(1:10),c(1:10)))+\n  geom_point(aes(x = 5, y = 5), size = 71.5, color = \"black\")+\n  geom_point(aes(x = 5, y = 5), size = 70, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 60, color = \"white\")+\n  geom_point(aes(x = 5, y = 5), size = 50, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 40, color = \"white\")+\n  geom_point(aes(x = 5, y = 5), size = 30, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 20, color = \"white\")+\n  geom_point(aes(x = 5, y = 5), size = 10, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 4, color = \"white\")+\n  ylim(0,10)+\n  xlim(0,10)+\n  theme_void()\n\nhahp <- target +\n  labs(subtitle = \"High Accuracy High Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 5, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5, y = 5.2), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5, y = 4.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.8, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.2, y = 5), shape = 4, size =2, color = \"blue\")\n\n\nlahp <- target +\n  labs(subtitle = \"Low Accuracy High Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 6, y = 6), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 6, y = 6.2), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 6, y = 5.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.8, y = 6), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 6.2, y = 6), shape = 4, size =2, color = \"blue\")\n\n\nhalp <- target +\n  labs(subtitle = \"High Accuracy Low Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 5, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5, y = 5.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.8, y = 4.4), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.4, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.6, y = 5.6), shape = 4, size =2, color = \"blue\")\n\nlalp <- target +\n  labs(subtitle = \"Low Accuracy Low Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 5.5, y = 5.5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.5, y = 5.4), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.2, y = 6.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.8, y = 3.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.2, y = 3), shape = 4, size =2, color = \"blue\")\n\n\nlibrary(patchwork)\n(hahp | lahp) /\n(halp | lalp)\n```\n\n::: {.cell-output-display}\n![The accuracy and precision of the archer is determined by the location of the group of arrows](data-accuracy_files/figure-html/fig-target-1.png){#fig-target width=672}\n:::\n:::\n\n\n\n\n\nAnother way to visualize accuracy and precision is via scatter plots for the relationship between the actual values and the estimates.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(r4pde)\ntheme_set(theme_r4pde())\ndat <- \ntibble::tribble(\n  ~actual,   ~ap,   ~ip,   ~ai,   ~ii,\n        0,     0,    10,     0,    25,\n       10,    10,    20,     5,    10,\n       20,    20,    30,    30,    10,\n       30,    30,    40,    30,    45,\n       40,    40,    50,    30,    35,\n       50,    50,    60,    60,    65,\n       60,    60,    70,    50,    30\n  )\n\nap <- dat |> \n  ggplot(aes(actual, ap))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n    geom_smooth(method = \"lm\")+\n   geom_point(color = \"#99cc66\", size = 3)+\n   ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"High Acccuracy High Precision\")\n\nip <- dat |> \n  ggplot(aes(actual, ip))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n  geom_smooth(method = \"lm\", se = F)+\n  geom_point(color = \"#99cc66\", size = 3)+\n  ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"Low Acccuracy High Precision\")\n\nai <- dat |> \n  ggplot(aes(actual, ai))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n  geom_smooth(method = \"lm\", se = F)+\n  geom_point(color = \"#99cc66\", size = 3)+\n  ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"High Acccuracy Low precision\")\n\nii <- dat |> \n  ggplot(aes(actual, ii))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n  geom_smooth(method = \"lm\", se = F)+\n  geom_point(color = \"#99cc66\", size = 3)+\n  ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"Low Acccuracy Low Precision\")\n\nlibrary(patchwork)\n(ap | ip) / (ai | ii)\n```\n\n::: {.cell-output-display}\n![Scatter plots for the relationship between actual and estimated values representing situations of low or high precision and accuracy. The dashed line indicates the perfect concordance and the solid blue line represents the fit of the linear regression model](data-accuracy_files/figure-html/fig-accuracy-1.png){#fig-accuracy width=672}\n:::\n:::\n\n\n\n\n\n## Statistical summaries\n\nA formal assessment of the quality of estimates or measures is made using statistical summaries of the data expressed as indices that represent reliability, precision and accuracy. These indices can further be used to test hypothesis such as if one or another method is superior than the other. The indices or the tests vary according to the nature of the variable, whether continuous, binary or categorical.\n\n### Inter-rater reliability\n\nTo calculate measures of inter-rater reliability (or reproducibility) we will work with a fraction of a larger dataset used in a published [study](https://bsppjournals.onlinelibrary.wiley.com/doi/abs/10.1111/ppa.13148). There, the authors tested the effect of standard area diagrams (SADs) on the reliability and accuracy of visual estimates of severity of soybean rust.\n\nThe selected dataset consists of five columns with 20 rows. The first is the leaf number and the others correspond to assessments of percent soybean rust severity by four raters (R1 to R4). Each row correspond to one symptomatic leaf. Let's assign the tibble to a dataframe called `sbr` (an acronym for soybean rust). Note that the variable is continuous.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nsbr <- tribble(\n~leaf, ~R1, ~R2,  ~R3, ~R4,\n1, 0.6, 0.6,  0.7, 0.6,\n2,   2, 0.7,    5,   1,\n3,   5,   5,    8,   5,\n4,   2,   4,    6,   2,\n5,   6,  14,   10,   7,\n6,   5,   6,   10,   5,\n7,  10,  18, 12.5,  12,\n8,  15,  30,   22,  10,\n9,   7,   2,   12,   8,\n10,  6,   9, 11.5,   8,\n11,  7,   7,   20,   9,\n12,  6,  23,   22,  14,\n13, 10,  35, 18.5,  20,\n14, 19,  10,    9,  10,\n15, 15,  20,   19,  20,\n16, 17,  30,   18,  13,\n17, 19,  53,   33,  38,\n18, 17, 6.8,   15,   9,\n19, 15,  20,   18,  16,\n20, 18,  22,   24,  15\n         )\n```\n:::\n\n\n\n\n\nLet's explore the data using various approaches. First, we can visualize how the individual estimates by the raters differ for a same leaf.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# transform from wide to long format\nsbr2 <- sbr |> \n  pivot_longer(2:5, names_to = \"rater\",\n               values_to = \"estimate\") \n\n# create the plot\nsbr2 |> \n  ggplot(aes(leaf, estimate, color = rater,\n             group = leaf))+\n  geom_line(color = \"black\")+\n  geom_point(size = 2)+\n  labs(y = \"Severity estimate (%)\",\n       x = \"Leaf number\",\n       color = \"Rater\")\n```\n\n::: {.cell-output-display}\n![Visual estimates of soybean rust severity for each leaf by each of four raters](data-accuracy_files/figure-html/fig-raters1-1.png){#fig-raters1 width=672}\n:::\n:::\n\n\n\n\n\nAnother interesting visualization is the correlation matrix of the estimates between all possible pair of raters. The `ggpairs` function of the *GGally* package is handy for this task.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GGally)\n\n\n# create a new dataframe with only raters\nraters <- sbr |> \n  select(2:5)\n\nggpairs(raters)+\n  theme_r4pde()\n```\n\n::: {.cell-output-display}\n![Correlation plots relating severity estimates for all pairs of raters](data-accuracy_files/figure-html/fig-correl-1.png){#fig-correl width=672}\n:::\n:::\n\n\n\n\n\n#### Coefficient of determination\n\nWe noticed earlier that the correlation coefficients varied across all pairs of rater. Sometimes, the means of squared Pearson's R values (R<sup>2</sup>), or the coefficient of determination is used as a measure of inter-rater reliability. We can further examine the pair-wise correlations in more details using the `cor` function,\n\n\n\n\n\n::: {#tbl-correl .cell tbl-cap='Pearson correlation coefficients for all pairs of raters'}\n\n```{.r .cell-code}\nknitr::kable(cor(raters))\n```\n\n::: {.cell-output-display}\n\n\n|   |        R1|        R2|        R3|        R4|\n|:--|---------:|---------:|---------:|---------:|\n|R1 | 1.0000000| 0.6325037| 0.6825936| 0.6756986|\n|R2 | 0.6325037| 1.0000000| 0.8413333| 0.8922181|\n|R3 | 0.6825936| 0.8413333| 1.0000000| 0.8615470|\n|R4 | 0.6756986| 0.8922181| 0.8615470| 1.0000000|\n\n\n:::\n:::\n\n\n\n\n\nThe means of coefficient of determination can be easily obtained as follows.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# All pairwise R2\n\nraters_cor <- reshape2::melt(cor(raters))\n\nraters2 <- raters_cor |> \n  filter(value != 1) \n\n# means of R2\nraters2$value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.6325037 0.6825936 0.6756986 0.6325037 0.8413333 0.8922181 0.6825936\n [8] 0.8413333 0.8615470 0.6756986 0.8922181 0.8615470\n```\n\n\n:::\n\n```{.r .cell-code}\nround(mean(raters2$value^2), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.595\n```\n\n\n:::\n:::\n\n\n\n\n\n#### Intraclass Correlation Coefficient\n\nA common statistic to report in reliability studies is the Intraclass Correlation Coefficient (ICC). There are several formulations for the ICC whose choice depend on the particular experimental design. Following the convention of the seminal work by @shrout1979, there are three main ICCs:\n\n-   One-way random effects model, ICC(1,1): in our context, each leaf is rated by different raters who are considered as sampled from a larger pool of raters (random effects)\n\n-   Two-way random effects model, ICC(2,1): both raters and leaves are viewed as random effects\n\n-   Two-way mixed model, ICC(3,1): raters are considered as fixed effects and leaves are considered as random.\n\nAdditionally, the ICC may depend on whether the ratings are an average or not of several ratings. When an average is considered, these are called ICC(1,k), ICC(2,k) and ICC(3,k).\n\nThe ICC can be computed using the `ICC()` or the `icc()` functions of the *psych* or *irr* packages, respectively. They both provide the coefficient, F value, and the upper and lower bounds of the 95% confidence interval.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(psych)\nic <- ICC(raters)\nknitr::kable(ic$results[1:2]) # only selected columns\n```\n\n::: {.cell-output-display}\n\n\n|                        |type  |       ICC|\n|:-----------------------|:-----|---------:|\n|Single_raters_absolute  |ICC1  | 0.6405024|\n|Single_random_raters    |ICC2  | 0.6464122|\n|Single_fixed_raters     |ICC3  | 0.6919099|\n|Average_raters_absolute |ICC1k | 0.8769479|\n|Average_random_raters   |ICC2k | 0.8797008|\n|Average_fixed_raters    |ICC3k | 0.8998319|\n\n\n:::\n\n```{.r .cell-code}\n# call ic list for full results\n```\n:::\n\n\n\n\n\nThe output of interest is a dataframe with the results of all distinct ICCs. We note that the ICC1 and ICC2 gave very close results. Now, let's obtain the various ICCs using the *irr* package. Differently from the the `ICC()` function, this one requires further specification of the model to use.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(irr)\nicc(raters, \"oneway\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Single Score Intraclass Correlation\n\n   Model: oneway \n   Type : consistency \n\n   Subjects = 20 \n     Raters = 4 \n     ICC(1) = 0.641\n\n F-Test, H0: r0 = 0 ; H1: r0 > 0 \n   F(19,60) = 8.13 , p = 1.8e-10 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.44 < ICC < 0.813\n```\n\n\n:::\n\n```{.r .cell-code}\n# The one used in the SBR paper\nicc(raters, \"twoway\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : consistency \n\n   Subjects = 20 \n     Raters = 4 \n   ICC(C,1) = 0.692\n\n F-Test, H0: r0 = 0 ; H1: r0 > 0 \n   F(19,57) = 9.98 , p = 6.08e-12 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.503 < ICC < 0.845\n```\n\n\n:::\n:::\n\n\n\n\n\n#### Overall Concordance Correlation Coefficient\n\nAnother useful index is the Overall Concordance Correlation Coefficient (OCCC) for evaluating agreement among multiple observers. It was proposed by @barnhart2002 based on the original index proposed by @lin1989, earlier defined in the context of two fixed observers. In the paper, the authors introduced the OCCC in terms of the interobserver variability for assessing agreement among multiple fixed observers. As outcome, and similar to the original CCC, the approach addresses the precision and accuracy indices as components of the OCCC. The `epi.occc` function of the *epiR* packge does the job but it does compute a confidence interval.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(epiR)\nepi.occc(raters, na.rm = FALSE, pairs = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nOverall CCC           0.6372\nOverall precision     0.7843\nOverall accuracy      0.8125\n```\n\n\n:::\n:::\n\n\n\n\n\n### Intrarater reliability\n\nAs defined, the intrarater reliability is also known as repeatability, because it measures consistency by the same rater at repeated assessments (e.g. different times) on the same sample. In some studies, we may be interested in testing whether a new method increases repeatability of assessments by a single rater compared with another one. The same indices used for assessing reproducibility (interrater) can be used to assess repeatability, and these are reported at the rater level.\n\n### Precision\n\nWhen assessing precision, one measures the variability of the estimates (or measurements) of disease on the same sampling units obtained by different raters (or instruments). A very high precision does not mean that the estimates are closer to the actual value (which is given by measures of bias). However, precision is a component of overall accuracy, or agreement. It is given by the Pearson's correlation coefficient.\n\nDifferent from reliability, that requires only the estimates or measures by the raters, now we need a reference (gold standard) value to compare the estimates to. These can be an accurate rater or measures by an instrument. Let's get back to the soybean rust severity estimation dataset and add a column for the (assumed) actual values of severity on each leaf. In that work, the actual severity values were obtained using image analysis.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsbr <- tibble::tribble(\n~leaf, ~actual, ~R1, ~R2,  ~R3, ~R4,\n1,    0.25, 0.6, 0.6,  0.7, 0.6,\n2,     2.5,   2, 0.7,    5,   1,\n3,    7.24,   5,   5,    8,   5,\n4,    7.31,   2,   4,    6,   2,\n5,    9.07,   6,  14,   10,   7,\n6,    11.6,   5,   6,   10,   5,\n7,   12.46,  10,  18, 12.5,  12,\n8,    13.1,  15,  30,   22,  10,\n9,   14.61,   7,   2,   12,   8,\n10,  16.06,   6,   9, 11.5,   8,\n11,   16.7,   7,   7,   20,   9,\n12,   19.5,   6,  23,   22,  14,\n13,  20.75,  10,  35, 18.5,  20,\n14,  23.56,  19,  10,    9,  10,\n15,  23.77,  15,  20,   19,  20,\n16,  24.45,  17,  30,   18,  13,\n17,  25.78,  19,  53,   33,  38,\n18,  26.03,  17, 6.8,   15,   9,\n19,  26.42,  15,  20,   18,  16,\n20,  28.89,  18,  22,   24,  15\n         )\n```\n:::\n\n\n\n\n\nWe can explore visually via scatter plots the relationships between the actual value and the estimates by each rater ([@fig-scater]). To facilitate, we need the data in the long format.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsbr2 <- sbr |> \n  pivot_longer(3:6, names_to = \"rater\",\n               values_to = \"estimate\") \n\nsbr2 |> \n  ggplot(aes(actual, estimate))+\n  geom_point(size = 2, alpha = 0.7)+\n  facet_wrap(~rater)+\n  ylim(0,45)+\n  xlim(0,45)+\n  geom_abline(intercept = 0, slope =1)+\n  theme_r4pde()+\n  labs(x = \"Actual severity (%)\",\n       y = \"Estimate severity (%)\")\n```\n\n::: {.cell-output-display}\n![Scatterplots for the relationship between estimated and actual severity for each rater](data-accuracy_files/figure-html/fig-scater-1.png){#fig-scater width=672}\n:::\n:::\n\n\n\n\n\nThe Pearson's r for the relationship, or the precision of the estimates by each rater, can be obtained using the `correlation` function of the *correlation* package.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecision <- sbr2 |> \n  select(-leaf) |> \n  group_by(rater) |> \n  correlation::correlation() \n\nknitr::kable(precision[1:4])\n```\n\n::: {.cell-output-display}\n\n\n|Group |Parameter1 |Parameter2 |         r|\n|:-----|:----------|:----------|---------:|\n|R1    |actual     |estimate   | 0.8725643|\n|R2    |actual     |estimate   | 0.5845291|\n|R3    |actual     |estimate   | 0.7531983|\n|R4    |actual     |estimate   | 0.7108260|\n\n\n:::\n:::\n\n\n\n\n\nThe mean precision can then be obtained.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(precision$r)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7302795\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n### Accuracy\n\n#### Absolute errors\n\nIt is useful to visualize the errors of the estimates which are obtained by subtracting the estimates from the actual severity values. This plot allows to visualize patterns in over or underestimations across a range of actual severity values.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsbr2 |> \n  ggplot(aes(actual, estimate-actual))+\n  geom_point(size = 3, alpha = 0.7)+\n  facet_wrap(~rater)+\n  geom_hline(yintercept = 0)+\n  theme_r4pde()+\n  labs(x = \"Actual severity (%)\",\n       y = \"Error (Estimate - Actual)\")\n```\n\n::: {.cell-output-display}\n![Error (estimated - actual) of visual severity estimates](data-accuracy_files/figure-html/fig-errors-1.png){#fig-errors width=672}\n:::\n:::\n\n\n\n\n\n#### Concordance correlation coefficient\n\nLin's (1989, 2000) proposed the concordance correlation coefficient (CCC) for agreement on a continuous measure obtained by two methods. The CCC combines measures of both precision and accuracy to determine how far the observed data deviate from the line of perfect concordance. Lin's CCC increases in value as a function of the nearness of the data reduced major axis to the line of perfect concordance (the accuracy of the data) and of the tightness of the data about its reduced major axis (the precision of the data).\n\nThe `epi.ccc` function of the *epiR* package allows to obtain the Lin's CCC statistics. Let's filter only rater 2 and calculate the CCC statistics for this rater.\n\n\n\n\n\n::: {#tbl-ccc .cell tbl-cap='Statitics of the concordance correlation coefficient summarizing accuracy and precision of visual severity estimates of soybean rust for a single rater'}\n\n```{.r .cell-code}\nlibrary(epiR)\n# Only rater 2\nsbr3 <- sbr2 |> filter(rater == \"R2\")\nccc <- epi.ccc(sbr3$actual, sbr3$estimate)\n# Concordance coefficient\nrho <- ccc$rho.c[,1]\n# Bias coefficient\nCb <- ccc$C.b\n# Precision\nr <- ccc$rho.c[,1]/ccc$C.b\n# Scale-shift\nss <- ccc$s.shift\n# Location-shift\nls <- ccc$l.shift\nMetrics <- c(\"Agreement\", \"Bias coefficient\", \"Precision\", \"scale-shift\", \"location-shift\")\nValue <- c(rho, Cb, r, ss, ls)\nres <- data.frame(Metrics, Value)\nknitr::kable(res)\n```\n\n::: {.cell-output-display}\n\n\n|Metrics          |      Value|\n|:----------------|----------:|\n|Agreement        |  0.5230656|\n|Bias coefficient |  0.8948494|\n|Precision        |  0.5845291|\n|scale-shift      |  1.6091178|\n|location-shift   | -0.0666069|\n\n\n:::\n:::\n\n\n\n\n\nNow let's create a function that will allow us to estimate the CCC for all raters in the data frame in the wide format. The function assumes that the first two columns are the actual and estimates and the rest of the columns are the raters, which is the case for our `sbr` dataframe . Let's name this function `ccc_byrater`. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nccc_byrater <- function(data) {\n  long_data <- pivot_longer(data, cols = -c(leaf, actual),\n                            names_to = \"rater\", values_to = \"measurement\")\n  ccc_results <- long_data %>%\n    group_by(rater) %>%\n    summarise(Agreement = as.numeric(epi.ccc(measurement, actual)$rho.c[1]),\n              `Bias coefficient` = epi.ccc(measurement, actual)$C.b,\n              Precision = Agreement / `Bias coefficient`,\n              scale_shift = epi.ccc(measurement, actual)$s.shift,\n              location_shift = epi.ccc(measurement, actual)$l.shift)\n  \n  return(ccc_results)\n}\n```\n:::\n\n\n\n\n\nThen, we use the `ccc_byrater` function with the original `sbr` dataset - or any other dataset in the wide format of similar structure. The output is a dataframe with all CCC statistics.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- ccc_byrater(sbr)\nknitr::kable(results)\n```\n\n::: {.cell-output-display}\n\n\n|rater | Agreement| Bias coefficient| Precision| scale_shift| location_shift|\n|:-----|---------:|----------------:|---------:|-----------:|--------------:|\n|R1    | 0.5968136|        0.6839766| 0.4082065|   1.3652694|      0.9090386|\n|R2    | 0.5230656|        0.8948494| 0.4680649|   0.6214585|      0.0666069|\n|R3    | 0.7306948|        0.9701226| 0.7088635|   1.1028813|      0.2280303|\n|R4    | 0.5861371|        0.8245860| 0.4833205|   1.0044929|      0.6522573|\n\n\n:::\n:::\n\n\n\n\n\n# Incidence data\n\n## Accuracy\n\nIncidence data are binary at the individual level; an individual is diseased or not. Here, different from severity that is estimated, the specimen is classified. Let's create two series of binary data, each being a hypothetical scenario of assignment of 12 plant specimens into two classes: healthy (0) or diseased (1).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\norder <- c(1:12)\nactual <- c(1,1,1,1,1,1,1,1,0,0,0,0)\nclass <- c(0,0,1,1,1,1,1,1,1,0,0,0)\n\ndat_inc <- data.frame(order, actual, class)\ndat_inc \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   order actual class\n1      1      1     0\n2      2      1     0\n3      3      1     1\n4      4      1     1\n5      5      1     1\n6      6      1     1\n7      7      1     1\n8      8      1     1\n9      9      0     1\n10    10      0     0\n11    11      0     0\n12    12      0     0\n```\n\n\n:::\n:::\n\n\n\n\n\nIn the example above, the rater makes 9 accurate classification and misses 3: 2 diseased plants classified as being disease-free (sample 1 and 2), and 1 healthy plant that is wrongly classified as diseased (sample 9).\n\nNotice that there are four outcomes:\n\nTP = true positive, a positive sample correctly classified\\\nTN = true negative, a negative sample correctly classified\\\nFP = false positive, a negative sample classified as positive\\\nFN = false negative, a positive sample classified as positive.\n\nThere are several metrics that can be calculated with the help of a confusion matrix, also known as error matrix. Considering the above outcomes, here is a how a confusion matrix looks like.\n\nSuppose a 2x2 table with notation\n\n|                      | Actual value |         |\n|---------------------:|:------------:|:-------:|\n| Classification value |   Diseased   | Healthy |\n|             Diseased |      TP      |   FP    |\n|              Healthy |      FN      |   TN    |\n\nLet's create this matrix using a function of the caret package.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nattach(dat_inc)\ncm <- confusionMatrix(factor(class), reference = factor(actual))\ncm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 0 1\n         0 3 2\n         1 1 6\n                                          \n               Accuracy : 0.75            \n                 95% CI : (0.4281, 0.9451)\n    No Information Rate : 0.6667          \n    P-Value [Acc > NIR] : 0.3931          \n                                          \n                  Kappa : 0.4706          \n                                          \n Mcnemar's Test P-Value : 1.0000          \n                                          \n            Sensitivity : 0.7500          \n            Specificity : 0.7500          \n         Pos Pred Value : 0.6000          \n         Neg Pred Value : 0.8571          \n             Prevalence : 0.3333          \n         Detection Rate : 0.2500          \n   Detection Prevalence : 0.4167          \n      Balanced Accuracy : 0.7500          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n:::\n\n\n\n\n\nThe function returns the confusion matrix and several statistics such as accuracy = (TP + TN) / (TP + TN + FP + FN). Let's manually calculate the accuracy and compare the results:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTP = 3\nFP = 2\nFN = 1\nTN = 6\naccuracy = (TP+TN)/(TP+TN+FP+FN)\naccuracy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.75\n```\n\n\n:::\n:::\n\n\n\n\n\nTwo other important metrics are sensitivity and specificity.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsensitivity = TP/(TP+FN)\nsensitivity\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.75\n```\n\n\n:::\n\n```{.r .cell-code}\nspecificity = TN/(FP+TN)\nspecificity\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.75\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\nWe can calculate some metrics using the *MixtureMissing* package.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MixtureMissing)\nevaluation_metrics(actual, class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$matr\n       pred_0 pred_1\ntrue_0      3      1\ntrue_1      2      6\n\n$TN\n[1] 3\n\n$FP\n[1] 1\n\n$FN\n[1] 2\n\n$TP\n[1] 6\n\n$TPR\n[1] 0.75\n\n$FPR\n[1] 0.25\n\n$TNR\n[1] 0.75\n\n$FNR\n[1] 0.25\n\n$precision\n[1] 0.8571429\n\n$accuracy\n[1] 0.75\n\n$error_rate\n[1] 0.25\n\n$FDR\n[1] 0.1428571\n```\n\n\n:::\n:::\n\n\n\n\n\n## Reliability\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(psych)\ntab <- table(class, actual)\nphi(tab)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.48\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}