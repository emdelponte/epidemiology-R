{
  "hash": "d44309cd0633dd1a5763b8872d5a910e",
  "result": {
    "markdown": "---\ntitle: \"Ordinal scales\"\neditor_options: \n  chunk_output_type: inline\n---\n\n\n## Introduction\n\nOrdinal scales are organized as rank-ordered numeric classes, with a finite number of such classes. The utilization of ordinal scales is often due to their convenience and speed of rating [@madden2007]. In fact, plant pathologists often encounter situations where direct estimates of the nearest percent severity are time-consuming or impractical, besides being unreliably and inacuratelly estimated. In plant pathological research, there are two commonly used types of ordinal scales: quantitative and qualitative [@Chiang2022].\n\n### Quantitative ordinal\n\nIn the quantitative ordinal scale, each score signifies a defined interval of the percentage scale. The most renowned quantitative ordinal scale is the Horsfall-Barratt (HB) scale, which was developed in the early 1940s when the science of plant pathology was transitioning towards more quantitative methodologies [@hebert1982]. The HB scale partitions the percentage scale into twelve successive, logarithmic-based intervals of severity ranging from 0 to 100%. The intervals increase in size from 0 to 50% and decrease from 50 to 100%.\n\n::: callout-warning\n## Controversy of the H-B scale\n\nThe divisions of the H-B scale were established on two assumptions. The first was the logarithmic relationship between the intensity of a stimulus and the subsequent sensation. The second was the propensity of a rater to focus on smaller objects when observing objects of two colors [@madden2007]. This foundation is based on the so-called Weber-Fechner law. However, there is limited experimental evidence supporting these assumptions. Current evidence indicates a linear relationship, rather than a logarithmic one, between visually estimated and actual severity [@nutter2006a]. Additionally, these authors demonstrated that raters more accurately discriminated disease severity between 25% and 50% than what the H-B scale allowed. New scale structures have been proposed to address the issues associated with the H-B scale [@liu2019; @chiang2014]. The Chiang scale follows a linear relationship with the percentage area diseased at severities greater than 10% (class 6 on the scale).\n:::\n\nLet's input the HB scale data and store as a data frame in R so we can prepare a table and a plot.\n\n\n::: {#tbl-HB .cell tbl-cap='The Horsfal-Barrat quantitative ordinal scale used as a tool for assessing plant disease severity ' hash='data-ordinal_cache/html/tbl-HB_1d83965ff0ad6f47d0041f1a4424a87a'}\n\n```{.r .cell-code}\nHB <- tibble::tribble(\n  ~ordinal, ~'range', ~midpoint,\n  0,          '0',    0,   \n  1,    '0+ to 3',  1.5,   \n  2,    '3+ to 6',  4.5,   \n  3,   '6+ to 12',  9.0,  \n  4,  '12+ to 25', 18.5, \n  5,  '25+ to 50', 37.5, \n  6,  '50+ to 75', 62.5, \n  7,  '75+ to 88', 81.5, \n  8,  '88+ to 94', 91.0, \n  9,  '94+ to 97', 95.5, \n  10,'97+ to 100', 98.5,  \n  11,      '100',   100 \n  )\nknitr::kable(HB, align = \"c\")\n```\n\n::: {.cell-output-display}\n| ordinal |   range    | midpoint |\n|:-------:|:----------:|:--------:|\n|    0    |     0      |   0.0    |\n|    1    |  0+ to 3   |   1.5    |\n|    2    |  3+ to 6   |   4.5    |\n|    3    |  6+ to 12  |   9.0    |\n|    4    | 12+ to 25  |   18.5   |\n|    5    | 25+ to 50  |   37.5   |\n|    6    | 50+ to 75  |   62.5   |\n|    7    | 75+ to 88  |   81.5   |\n|    8    | 88+ to 94  |   91.0   |\n|    9    | 94+ to 97  |   95.5   |\n|   10    | 97+ to 100 |   98.5   |\n|   11    |    100     |  100.0   |\n:::\n:::\n\n\nLet's visualize the different sizes of the percent interval encompassing each score.\n\n\n::: {.cell hash='data-ordinal_cache/html/fig-hb_8fa5af8337df670355843152792be45a'}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(r4pde)\nHB |> \n  ggplot(aes(midpoint, ordinal))+\n  geom_point(size =2)+\n  geom_line()+\n  scale_x_continuous(breaks = c(0, 3, 6, 12, 25, 50, 75, 88, 94, 97))+\n  scale_y_continuous(breaks = c(1:12))+\n  geom_vline(aes(xintercept = 3), linetype = 2)+\n  geom_vline(aes(xintercept = 6), linetype = 2)+\n  geom_vline(aes(xintercept = 12), linetype = 2)+\n  geom_vline(aes(xintercept = 25), linetype = 2)+\n  geom_vline(aes(xintercept = 50), linetype = 2)+\n  geom_vline(aes(xintercept = 75), linetype = 2)+\n  geom_vline(aes(xintercept = 88), linetype = 2)+\n  geom_vline(aes(xintercept = 94), linetype = 2)+\n  geom_vline(aes(xintercept = 97), linetype = 2)+\n  labs(x = \"Percent severity\", y = \"HB score\")+\n  theme_r4pde()\n```\n\n::: {.cell-output-display}\n![Ordinal scores of the Horsfal-Barrat scale](data-ordinal_files/figure-html/fig-hb-1.png){#fig-hb width=672}\n:::\n:::\n\n\nWe can repeat those procedures to visualize the Chiang scale.\n\n\n::: {#tbl-chiang .cell tbl-cap='The Chiang quantitative ordinal scale used as a tool for assessing plant disease severity ' hash='data-ordinal_cache/html/tbl-chiang_d8b6baf5cbf3342dcdb75e6ebe526d57'}\n\n```{.r .cell-code}\nchiang <- tibble::tribble(\n  ~ordinal, ~'range', ~midpoint,\n  0,          '0',     0,   \n  1,  '0+ to 0.1',  0.05,   \n  2,'0.1+ to 0.5',   0.3,   \n  3,  '0.5+ to 1',  0.75,  \n  4,    '1+ to 2',   1.5, \n  5,    '2+ to 5',     3, \n  6,   '5+ to 10',   7.5, \n  7,  '10+ to 20',    15, \n  8,  '20+ to 30',    25, \n  9,  '30+ to 40',    35, \n  10, '40+ to 50',    45,  \n  11, '50+ to 60',    55,\n  12, '60+ to 70',    65,\n  13, '70+ to 80',    75,\n  14, '80+ to 90',    85,\n  15,'90+ to 100',   95\n  )\nknitr::kable(chiang, align = \"c\")\n```\n\n::: {.cell-output-display}\n| ordinal |    range    | midpoint |\n|:-------:|:-----------:|:--------:|\n|    0    |      0      |   0.00   |\n|    1    |  0+ to 0.1  |   0.05   |\n|    2    | 0.1+ to 0.5 |   0.30   |\n|    3    |  0.5+ to 1  |   0.75   |\n|    4    |   1+ to 2   |   1.50   |\n|    5    |   2+ to 5   |   3.00   |\n|    6    |  5+ to 10   |   7.50   |\n|    7    |  10+ to 20  |  15.00   |\n|    8    |  20+ to 30  |  25.00   |\n|    9    |  30+ to 40  |  35.00   |\n|   10    |  40+ to 50  |  45.00   |\n|   11    |  50+ to 60  |  55.00   |\n|   12    |  60+ to 70  |  65.00   |\n|   13    |  70+ to 80  |  75.00   |\n|   14    |  80+ to 90  |  85.00   |\n|   15    | 90+ to 100  |  95.00   |\n:::\n:::\n\n::: {.cell hash='data-ordinal_cache/html/fig-chiagn_766c0447ea96df95377c576ba05dcfdf'}\n\n```{.r .cell-code  code-fold=\"false\"}\nchiang |> \n  ggplot(aes(midpoint, ordinal))+\n  geom_point(size =2)+\n  geom_line()+\n  scale_y_continuous(breaks = c(0:15))+\n  scale_x_continuous(breaks = c(0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100))+\n  geom_vline(aes(xintercept = 0), linetype = 2)+\n  geom_vline(aes(xintercept = 0.1), linetype = 2)+\n  geom_vline(aes(xintercept = 0.5), linetype = 2)+\n  geom_vline(aes(xintercept = 1), linetype = 2)+\n  geom_vline(aes(xintercept = 2), linetype = 2)+\n  geom_vline(aes(xintercept = 5), linetype = 2)+\n  geom_vline(aes(xintercept = 10), linetype = 2)+\n  geom_vline(aes(xintercept = 20), linetype = 2)+\n  geom_vline(aes(xintercept = 30), linetype = 2)+\n   geom_vline(aes(xintercept = 40), linetype = 2)+\n   geom_vline(aes(xintercept = 50), linetype = 2)+\n   geom_vline(aes(xintercept = 60), linetype = 2)+\n   geom_vline(aes(xintercept = 70), linetype = 2)+\n   geom_vline(aes(xintercept = 80), linetype = 2)+\n   geom_vline(aes(xintercept = 90), linetype = 2)+\n   geom_vline(aes(xintercept = 100), linetype = 2)+\n  labs(x = \"Percent severity\", y = \"Chiang score\")+\n  theme_r4pde()\n```\n\n::: {.cell-output-display}\n![Ordinal scores of the Chiang scale](data-ordinal_files/figure-html/fig-chiagn-1.png){#fig-chiagn width=672}\n:::\n:::\n\n\n### Qualitative ordinal\n\nIn the qualitative ordinal scale, each class provides a description of the symptoms. An example is the ordinal 0-3 scale for rating eyespot of wheat developed by [@scott1974].\n\n| Class | Description                                                                                                 |\n|--------------------------|----------------------------------------------|\n| 0     | uninfected                                                                                                  |\n| 1     | slight eyespot (or or more small lesion occupying in total less than half of the circumference of the stem) |\n| 2     | moderate eyespot (one or more lesions occupying at least half the circumference of the stem)                |\n| 3     | severe eyespot (stem completely girdled by lesions; tissue softened so that lodging would really occur)     |\n\n: Ordinal scale for rating eyespot of wheat [@scott1974]\n\n## Disease severity index (DSI)\n\nSometimes, when quantitative or qualitative ordinal scales are used, the scores given to various individual specimens (the observational units) are transformed into an index on a percentage basis, such as the disease severity index (DSI) which is used as a value for the experimental unit further in data analysis. The DSI is a single number that summarizes a large amount of information on disease severity [@chester1950]. The formula for a DSI (%) can be written as follows:\n\n$DSI = \\frac{∑(class \\ freq. \\ ✕ \\ score \\  of \\ class)} {total \\ n \\ ✕ \\ maximal \\ class} ✕ 100$\n\nThe `DSI()` and `DSI2()` are part of the *r4pde* package. Let's see how each function works.\n\nThe `DSI()` allows to automate the calculation of the disease severity index (DSI) in a series of units (e.g. leaves) that are further classified according to ordinal scores. The function requires three arguments:\n\n-   unit = the vector of the number of each unit\n\n-   class = the vector of the scores for the units\n\n-   max = the maximum value of the scale\n\nLet's create a toy data set composed of 12 units where each received an ordinal score. The vectors were arranged as a data frame named scores.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-5_f00d669e93266bc07c21294c07f8e33a'}\n\n```{.r .cell-code}\nunit <- c(1:12)\nclass <- c(2,3,1,1,3,4,5,0,2,5,2,1)\nratings <- data.frame(unit, class)\nknitr::kable(ratings)\n```\n\n::: {.cell-output-display}\n| unit| class|\n|----:|-----:|\n|    1|     2|\n|    2|     3|\n|    3|     1|\n|    4|     1|\n|    5|     3|\n|    6|     4|\n|    7|     5|\n|    8|     0|\n|    9|     2|\n|   10|     5|\n|   11|     2|\n|   12|     1|\n:::\n:::\n\n\nThe ordinal score used in this example has 6 as the maximum score. The function returns the DSI value.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-6_e83a32e367def6c131e62efaf46073d0'}\n\n```{.r .cell-code}\nlibrary(r4pde)\nDSI(ratings$unit, ratings$class, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 40.27778\n```\n:::\n:::\n\n\nLet's now deal with a situation of multiple plots (five replicates) where a fixed number of 12 samples were taken and assessed using an ordinal score. Let's input the data using the `tribble()` function. Note that the data is in the wide format.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-7_be6f3104e93874a18cc162bff386dfe4'}\n\n```{.r .cell-code}\nexp <- tibble::tribble(\n  ~rep, ~`1`, ~`2`, ~`3`, ~`4`, ~`5`, ~`6`, ~`7`, ~`8`, ~`9`, ~`10`, ~`11`,~`12`,\n  1, 2, 3, 1, 1, 3, 4, 5, 0, 2, 5, 2, 1,\n  2, 3, 4, 4, 6, 5, 4, 4, 0, 2, 1, 1, 5,\n  3, 5, 6, 6, 5, 4, 2, 0, 0, 0, 0, 2, 0,\n  4, 5, 6, 0, 0, 0, 3, 3, 2, 1, 0, 2, 3, \n  5, 0, 0, 0, 0, 2, 3, 2, 5, 6, 2, 1, 0,\n)\nknitr::kable(exp)\n```\n\n::: {.cell-output-display}\n| rep|  1|  2|  3|  4|  5|  6|  7|  8|  9| 10| 11| 12|\n|---:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|\n|   1|  2|  3|  1|  1|  3|  4|  5|  0|  2|  5|  2|  1|\n|   2|  3|  4|  4|  6|  5|  4|  4|  0|  2|  1|  1|  5|\n|   3|  5|  6|  6|  5|  4|  2|  0|  0|  0|  0|  2|  0|\n|   4|  5|  6|  0|  0|  0|  3|  3|  2|  1|  0|  2|  3|\n|   5|  0|  0|  0|  0|  2|  3|  2|  5|  6|  2|  1|  0|\n:::\n:::\n\n\nAfter reshaping the data to the long format, we can calculate the DSI for each plot/replicate as follows:\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-8_658ab65c035c4f94a6cdf2a6de015610'}\n\n```{.r .cell-code}\nres <- exp |> \n  pivot_longer(2:13, names_to = \"unit\", values_to = \"class\") |>\n  group_by(rep) |> \n  summarise(DSI = DSI(unit, class, 6))\n```\n:::\n\n\nAnd here we have the results of the DSI for each replicate.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-9_914275928fe2b1ddd73a5b9352408401'}\n\n```{.r .cell-code}\nknitr::kable(res, align = \"c\")\n```\n\n::: {.cell-output-display}\n| rep |   DSI    |\n|:---:|:--------:|\n|  1  | 40.27778 |\n|  2  | 54.16667 |\n|  3  | 41.66667 |\n|  4  | 34.72222 |\n|  5  | 29.16667 |\n:::\n:::\n\n\nNow our data set is organized as the frequency of each class as follows:\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-10_9431637ffa94b2970c4032a0e7878e7e'}\n\n```{.r .cell-code}\nratings2 <- ratings |> \n  dplyr::count(class)\n\nratings2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  class n\n1     0 1\n2     1 3\n3     2 3\n4     3 2\n5     4 1\n6     5 2\n```\n:::\n:::\n\n\nNow we can apply the `DSI2()` function. The function requires three arguments:\n\n-   class = the number of the respective class\n-   freq = the frequency of the class\n-   max = the maximum value of the scale\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-11_f404a0db47a1b1ab3e1b886443bd5acd'}\n\n```{.r .cell-code}\nlibrary(r4pde)\nDSI2(ratings2$class, ratings2$n, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 40.27778\n```\n:::\n:::\n\n\n## Analysis of ordinal data\n\nOrdinal score data typically do not align well with the assumptions of traditional parametric statistical methods. Given this challenge, non-parametric methods have emerged as a compelling alternative for handling ordinal plant pathological data [@shah2004]. Unlike their parametric counterparts, these methods do not rest on the presumption of a specific distribution for the underlying population, offering greater flexibility in accommodating the intricacies inherent to ordinal data. On the other hand, when the conditions are right, parametric methods can also be harnessed effectively.\n\nA common strategy, particularly for ordinal scores, involves converting these values into the mid-points of their corresponding percent scales. This transformation renders the data more amenable to parametric analyses. However, the mid-point conversion has been criticized in the literature as it may amplify the imprecision, especially when the interval size is wide, and because it does not really reflect a true value, but an interval for each estimate [@chiang2023a; @onofri2018].\n\nLet's see some examples of analysis using the mid-point conversion in a parametric framework as well as the non-parametric tests.\n\n#### Example data\n\nWe will use a data set made available in an article on the application of the survival analysis technique to test hypotheses when using quantitative ordinal scales [@chiang2023a]. The data and codes used in the paper can be found in the specified GitHub [repository](https://github.com/StatisticalMethodsinPlantProtection/CompMuCens). The data is structured into four treatments, each with 30 ratings ranging from a score of 1 to 6 on the H-B scale.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-12_b3429d9f045be88007c1e110834e28f3'}\n\n```{.r .cell-code}\n# Create the vectors for the treatments\ntrAs <- c(5,4,2,5,5,4,4,2,5,2,2,3,4,3,2,\n          2,6,2,2,4,2,4,2,4,5,3,4,2,2,3)\ntrBs <- c(5,3,2,4,4,5,4,5,4,4,6,4,5,5,5,\n          2,6,2,3,5,2,6,4,3,2,5,3,5,4,5)\ntrCs <- c(2,3,1,4,1,1,4,1,1,3,2,1,4,1,1,\n          2,5,2,1,3,1,4,2,2,2,4,2,3,2,2)\ntrDs <- c(5,5,4,5,5,6,6,4,6,4,3,5,5,6,4,\n          6,5,6,5,4,5,5,5,3,5,6,5,5,5,6)\n\n# Create the tibble\ndat_ordinal <- tibble::tibble(\n  treatment = rep(c(\"A\", \"B\", \"C\", \"D\"), \n                  each = 30),\n  score = c(trAs, trBs, trCs, trDs)\n)\n\ndat_ordinal\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 120 × 2\n   treatment score\n   <chr>     <dbl>\n 1 A             5\n 2 A             4\n 3 A             2\n 4 A             5\n 5 A             5\n 6 A             4\n 7 A             4\n 8 A             2\n 9 A             5\n10 A             2\n# ℹ 110 more rows\n```\n:::\n:::\n\n\nBecause the ordinal response was obtained using an interval scale, the mid-point of the score was also obtained.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-13_ef985223cbc51ba54c6892a7c1d60b79'}\n\n```{.r .cell-code}\n# Create the vectors for the treatments\ntrAm <- c(18.5,9,1.5,18.5,18.5,9,9,1.5,\n          18.5,1.5,1.5,4.5,9,4.5,1.5,1.5,\n          37.5,1.5,1.5,9,1.5,9,1.5,9,18.5,\n          4.5,9,1.5,1.5,4.5)\ntrBm <- c(18.5,4.5,1.5,9,9,18.5,9,18.5,9,\n          9,37.5,9,18.5,18.5,18.5,1.5,37.5,\n          1.5, 4.5,18.5,1.5,37.5,9,4.5,1.5,\n          18.5,4.5,18.5,9,18.5)\ntrCm <- c(1.5,4.5,0,9,0,0,9,0,0,4.5,1.5,0,9\n          ,0,0,1.5,18.5,1.5,0,4.5,0,9,1.5,1.5,\n          1.5,9,1.5,4.5,1.5,1.5)\ntrDm <- c(18.5,18.5,9,18.5,18.5,37.5,37.5,9,\n          37.5,9,4.5,18.5,18.5,37.5,9,37.5,\n       18.5,37.5,18.5,9,18.5,18.5,18.5,4.5,18.5,\n       37.5,18.5,18.5,18.5,37.5)\n\n# Create the tibble\ndat_ordinal_mp <- tibble::tibble(\n  treatment = rep(c(\"A\", \"B\", \"C\", \"D\"), \n                  each = 30),\n  midpoint = c(trAm, trBm, trCm, trDm)\n)\ndat_ordinal_mp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 120 × 2\n   treatment midpoint\n   <chr>        <dbl>\n 1 A             18.5\n 2 A              9  \n 3 A              1.5\n 4 A             18.5\n 5 A             18.5\n 6 A              9  \n 7 A              9  \n 8 A              1.5\n 9 A             18.5\n10 A              1.5\n# ℹ 110 more rows\n```\n:::\n:::\n\n\nThe visualization using box-plots suggests differences between the treatments.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-14_171248391509aa2e961305a275e96dd9'}\n\n```{.r .cell-code}\np_score <- dat_ordinal |> \n  ggplot(aes(treatment, score))+\n  geom_boxplot()+\n  theme_r4pde()\n\np_mp <- dat_ordinal_mp |> \n  ggplot(aes(treatment, midpoint))+\n  geom_boxplot()+\n  theme_r4pde()\n\nlibrary(patchwork)\np_score | p_mp\n```\n\n::: {.cell-output-display}\n![](data-ordinal_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n#### Parametric (mid-point conversion)\n\nParametric methods can be employed when analyzing data, provided that the underlying assumptions of these methods are satisfied. For data that is based on ordinal scores, one way to apply parametric methods is to convert these scores into the mid-points of their corresponding percent scales. Once converted, the data is better suited for parametric analyses. Among the available techniques, the most frequently utilized is the application of an ANOVA (Analysis of Variance) model. This model is particularly useful for testing the null hypothesis, which posits that there are no significant differences among the treatments being studied.\n\nWe can use the `aov` function to fit the model and check the parametric assumptions using the `DHARMa` package.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-15_ca86cc612234ae42d2d5a4136036e991'}\n\n```{.r .cell-code}\nm1_mp <- aov(midpoint ~ treatment, data = dat_ordinal_mp)\n# normality and homocedasticity tests\nlibrary(DHARMa)\nplot(simulateResiduals(m1_mp))\n```\n\n::: {.cell-output-display}\n![](data-ordinal_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nSince both normality and homoscedasticity assumptions are violated in our initial model, we will halt further analysis using this model. Instead, we will explore an alternative approach that includes data transformation (logit).\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-16_a1ef08cb650ea20452fc2f026af30eeb'}\n\n```{.r .cell-code}\n# data transformation\nlibrary(car)\n# transform midpoint to proportion\ndat_ordinal_mp$midpoint2 <- dat_ordinal_mp$midpoint/100\n\nm2_mp <- aov(logit(midpoint2) ~ treatment, data = dat_ordinal_mp)\nplot(simulateResiduals(m2_mp)) # assumptions are met\n```\n\n::: {.cell-output-display}\n![](data-ordinal_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nNow that both assumptions have been met, we can proceed to use a means comparison test to determine which treatments differ from one another, with the assistance of the {emmeans} package. It's important to note that the results are displayed in the original scale after transformation, specifically when using `type = \"response`.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-17_b830e1960602a083daa9d5e2ce41123a'}\n\n```{.r .cell-code}\nlibrary(emmeans)\nmeans_m2_mp <- emmeans(m2_mp, \"treatment\", type = \"response\")\nmeans_m2_mp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n treatment response      SE  df lower.CL upper.CL\n A           0.0808 0.00983 116   0.0634   0.1026\n B           0.1248 0.01446 116   0.0989   0.1564\n C           0.0461 0.00582 116   0.0358   0.0591\n D           0.2071 0.02173 116   0.1674   0.2535\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n```\n:::\n:::\n\n\nThe best way to visualize the differences between treatments is using the `pwpm` function to display a matrix of estimates, pairwise differences, and P values. Most commonly, the compact letter display is used for means comparison.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-18_861a9739cc4e67f26378a33cfb7273c4'}\n\n```{.r .cell-code}\n# matrix\npwpm(means_m2_mp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         A        B        C        D\nA [0.0808]   0.0530   0.0094   <.0001\nB    0.617 [0.1248]   <.0001   0.0085\nC    1.820    2.953 [0.0461]   <.0001\nD    0.337    0.546    0.185 [0.2071]\n\nRow and column labels: treatment\nUpper triangle: P values   null = 1  adjust = \"tukey\"\nDiagonal: [Estimates] (response)   type = \"response\"\nLower triangle: Comparisons (odds.ratio)   earlier vs. later\n```\n:::\n\n```{.r .cell-code}\n# compact letter display\nlibrary(multcomp)\ncld(means_m2_mp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n treatment response      SE  df lower.CL upper.CL .group\n C           0.0461 0.00582 116   0.0358   0.0591  1    \n A           0.0808 0.00983 116   0.0634   0.1026   2   \n B           0.1248 0.01446 116   0.0989   0.1564   2   \n D           0.2071 0.02173 116   0.1674   0.2535    3  \n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \nP value adjustment: tukey method for comparing a family of 4 estimates \nTests are performed on the log odds ratio scale \nsignificance level used: alpha = 0.05 \nNOTE: Compact letter displays can be misleading\n      because they show NON-findings rather than findings.\n      Consider using 'pairs()', 'pwpp()', or 'pwpm()' instead. \n```\n:::\n:::\n\n\n#### Non-parametric (ordinal score)\n\nBecause ordinal score data generally do not meet the assumptions of traditional parametric statistical methods, non-parametric methods can be considered as an alternative. Such methods have been proposed for analyzing ordinal data in plant pathology [@shah2004]. For our example, when more than two treatments are involved, a Kruskal-Wallis test can be utilized. The `kruskal` function of the {agricolae} package does the job we want. Note that in this case we use the ordinal score directly and not the mid-point value.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-19_b3f799ab0c957c4b3b44ca41f9d15f4d'}\n\n```{.r .cell-code}\nlibrary(agricolae)\nkruskal(dat_ordinal$score, dat_ordinal$treatment, console = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nStudy: dat_ordinal$score ~ dat_ordinal$treatment\nKruskal-Wallis test's\nTies or no Ties\n\nCritical Value: 52.34422\nDegrees of freedom: 3\nPvalue Chisq  : 2.529543e-11 \n\ndat_ordinal$treatment,  means of the ranks\n\n  dat_ordinal.score  r\nA          52.05000 30\nB          69.58333 30\nC          29.61667 30\nD          90.75000 30\n\nPost Hoc Analysis\n\nt-Student: 1.980626\nAlpha    : 0.05\nMinimum Significant Difference: 13.1991 \n\nTreatments with the same letter are not significantly different.\n\n  dat_ordinal$score groups\nD          90.75000      a\nB          69.58333      b\nA          52.05000      c\nC          29.61667      d\n```\n:::\n:::\n\n\n#### Survival analysis\n\nA method based on survival analysis was more recently introduced for the analysis of ordinal data [@chiang2023a]. The authors developed an R script called \"CompMuCens\", which facilitates the comparison of multiple treatment means for plant disease severity data. This is achieved through nonparametric survival analysis using class- or interval-based data derived from a quantitative ordinal scale. The code is accessible in [this repository](https://github.com/StatisticalMethodsinPlantProtection/CompMuCens/tree/main) and has been incorporated into the {r4pde} package.\n\nWe continue with our working example data. Since the expected variable name for the score in the function is `x`, we must adjust our data frame accordingly.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-20_6b5a69ab2284ccbce1b6d0e4352203cb'}\n\n```{.r .cell-code}\nnames(dat_ordinal) <- c(\"treatment\", \"x\")\n```\n:::\n\n\nThe `CompMuCens()` function uses the `ictest()` from the {interval} package to conduct nonparametric survival analysis. Detailed explanation of the function's input and output can be found [here](https://github.com/StatisticalMethodsinPlantProtection/CompMuCens/blob/main/Supplementary%20Data.pdf). We just need to set the `dat` and the `scale` arguments. The scale will be used to convert the scores in to the defined interval which is used as response variable in the analysis. For example, if the score is 2, the respective limits of the interval will be 3 and 6. For 7, the limits will be 75 and 88. The function takes care of this conversion based on the inputted scale values.\n\n\n::: {.cell hash='data-ordinal_cache/html/unnamed-chunk-21_8959ecc828a575c07e0d91d1206d483a'}\n\n```{.r .cell-code}\nlibrary(interval)\nlibrary(r4pde)\n\nscale <- c(0,3,6,12,25,50,75,88,94,97,100, 100)\nCompMuCens(dat_ordinal, scale)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$U.Score\n  treatment      score\n1         D -15.125000\n2         B  -4.541667\n3         A   4.225000\n4         C  15.441667\n\n$Hypothesis.test\n  treat1 treat2 p-value for H0: treat1 ≤ treat2 p-value for H0: treat1 = treat2\n1      D      B                    0.0018767018                              NA\n2      B      A                    0.0113215174                              NA\n3      A      C                    0.0007456484                              NA\n\n$adj.Signif\n[1] 0.01666667\n\n$Conclusion\n[1] \"D>B>A>C\"\n```\n:::\n:::\n\n\nThe outcomes are three: the ordered scores for each treatment, the pairwise comparison between treatments and the significance levels, followed by a conclusion section. In this example all treatments differ from each other.\n\n#### Interpretation\n\nUpon comparing the three methods, it becomes evident that there is a marked distinction in their outcomes. The Kruskal-Wallis and the survival analysis tests suggested a significant difference between treatment A and B, whereas the parametric counterpart did not. However, it's crucial to note that the P-value is borderline significant at 0.0530, being just slightly above the conventional threshold of 0.05. Given this, it appears that the conversion to the mid-point might have resulted in a type II error, wherein a genuine difference between the treatments might have been missed or overlooked. A detailed comparison between the mid-point and survival analysis has been presented in the paper, which is worth a reading [@chiang2023a]\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}