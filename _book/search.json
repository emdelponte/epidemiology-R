[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R4PDE.net",
    "section": "",
    "text": "Welcome\nR for Plant Disease Epidemiology (R4PDE) is an online book rooted in the teachings of the annual graduate course, FIP 602 - Plant Disease Epidemiology, a key part of the curriculum in the Graduate Program in Plant Pathology at Universidade Federal de Viçosa.\nDesigned for those passionate about studying and modeling plant disease epidemics with R, the book offers an exploration of diverse methods for describing, visualizing, and analyzing epidemic data collected over time and space. Readers should ideally have a foundational knowledge of R to best utilize the examples.\nR4PDE is not a resource for learning data science through R, as there are already well-established books such as R for Data Science for that purpose. This book draws on multiple sources, but in some sections, it utilizes data and replicates (with permission) some of the analyses (presented in SAS codes) from The Study of Plant Disease Epidemics (Madden et al. 2007), a highly recommended textbook for anyone wishing to delve deeply into plant disease epidemiology.\nA mix of general and specific R packages are utilized to conduct common plant disease epidemiology data analysis, notably {epifitter} and {epiphy}, both designed by plant pathologists. In conjunction with this book, a new R package {r4pde} has been developed and can be installed from CRAN using:\n\ninstall.packages(\"r4pde\")\n\nThe development version can be installed from GitHub using {pak}:\n\ninstall.packages(\"pak\")\n\nThe Icens package is needed for installation:\n\npak::pkg_install(\"Icens\")\n\n\npak::pkg_install(\"emdelponte/r4pde\")\n\nThis online book is frequently updated and edited. It content is free to use, licensed under a Creative Commons licence, and the code for all analyses can be found on GitHub. Contributions are subject to a Contributor Code of Conduct, and by contributing, you agree to adhere to its terms.\nR for Plant Disease Epidemiology © 2023 by Emerson Medeiros Del Ponte is licensed under CC BY-NC 4.0 \n\n\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. The study of plant disease epidemics. The American Phytopathological Society. https://doi.org/10.1094/9780890545058.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "cite.html",
    "href": "cite.html",
    "title": "How to cite",
    "section": "",
    "text": "The author has opted for self-publishing this book to ensure its accessibility and updatability, reflecting the dynamic nature of the field and the R programming environment. Readers are encouraged to cite this work when referencing or utilizing the methodologies and insights provided in the book. Ensure the accuracy of the details while citing, especially the URL, to direct readers to the correct online source.\nBelow one suggestion of citation style:\n\nDel Ponte, E. M. (2023). R for Plant Disease Epidemiology (R4PDE). Author. https://r4pde.net\n\nCataloguing-in-Publication data prepared by the library of the Federal University of Viçosa, Brazil.",
    "crumbs": [
      "How to cite"
    ]
  },
  {
    "objectID": "author.html",
    "href": "author.html",
    "title": "About the author",
    "section": "",
    "text": "I was born in 1973 in the city of Pelotas, in the southernmost part of Brazil. My interest in agriculture began early, influenced by the lifestyle of some relatives and their friends working as scientists in agricultural sciences. My high school years, spent in a rural school, were instrumental in strengthening this connection. Graduating with a major in agronomy in 1996 was just the beginning. I deepened my interest in plant pathology during graduate studies at the Federal University of Pelotas, one of Brazil’s leading agricultural schools. My engagement with plant diseases solidified during my doctoral work, where I took on the challenge of understanding and modeling Fusarium head blight, a disease of major significance to wheat production.\nThe next steps in my career took me abroad. I spent one year at Cornell University during my doctoral training and later completed a postdoctoral position at Iowa State University, experiences that broadened both my research scope and professional network. In 2006, I returned to Brazil to take a position as assistant professor at the Federal University of Rio Grande do Sul, where I dedicated eight years to teaching, research, and student mentoring in plant disease epidemiology. In 2014, seeking new challenges, I joined the Universidade Federal de Viçosa (UFV) as associate professor. I currently serve as full professor, where I continue to mentor, teach, and guide master’s and doctoral students in the field.\nThroughout my career, I have been a strong advocate for open science. In my lab, transparency is a core principle. We use the R language for statistical analysis and data processing, and we share our scripts and results as preprints and open-access outputs.\nThis commitment to openness led me to adopt open education practices, including the self-publication of the book R for Plant Disease Epidemiology, written entirely with open-source tools. The open format ensures that the book remains accessible, updatable, and aligned with the evolving nature of plant epidemiology and the R environment.\nI dedicate this book to my wife, Isabel, and our son, Vitor, who have been constant sources of support. Their patience and encouragement were essential, especially during long stretches of writing and revision.",
    "crumbs": [
      "About the author"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Writing R for Plant Disease Epidemiology (R4PDE) has been an eye-opening experience, and I couldn’t have done it without the support and kindness of so many people and organizations.\nFirst, I want to thank Helen Pennington for allowing her stunning painting of coffee leaf rust to be featured on the cover. Her artwork beautifully captures the spirit of plant disease epidemiology, adding a visual layer that perfectly complements the book’s content.\nI’m also deeply grateful to Prof. Laurence V. Madden for his generosity in sharing data from The Study of Plant Disease Epidemics. His contribution has truly enriched the analytical sections of this book.\nI’d also like to thank the chapter authors who contributed their expertise. Prof. Ivan A. Lizarazo deserves special mention for his chapter on using remote sensing for disease detection and quantification, which adds valuable insights to the book.\nFinally, the open-source community has played a role in shaping R4PDE. I’m incredibly thankful to everyone who contributed fixes and improvements, whether through pull requests or other feedback. Your collaboration and input have been invaluable. Adam Sparks (@adamhsparks), Remco Stam (@remco-stam), Tiago Olivoto (@TiagoOlivoto), Monalisa De Cól (@Monalisacdc), Juan Edwards Molina (@juanchiem), and Ivan A. Lizarazo.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Defining plant disease\nDisease in plants can be defined as any malfunctioning of host cells and tissues that results from continuous irritation by a pathogenic agent or environmental factor and leads to development of symptoms (Agrios 2005a). When caused by pathogenic agent, the disease results from the combination of three elements: susceptible host plant, a virulent pathogen, and favorable environmental conditions - the famous disease triangle. When a pathogen population establishes and causes disease in a host population, the phenomenon is called an epidemic, or the disease in populations. Among several definitions of epidemic, a comprehensive one is the change in disease intensity in a host population over time and space (Madden et al. 2007).\nThere exist numerous iterations of the disease triangle, incorporating additional elements (e.g., human intervention and time) as points and/or dimensions to provide a more comprehensive representation of an epidemic (Agrios 2005b). We find the disease prism particularly illustrative, where a sequence of stacked triangles represent the evolution of a plant disease through time (Francl 2001).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#defining-plant-disease",
    "href": "intro.html#defining-plant-disease",
    "title": "1  Introduction",
    "section": "",
    "text": "Figure 1.1: The plant disease prism as a model of plant disease epidemics",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#importance-of-epidemics",
    "href": "intro.html#importance-of-epidemics",
    "title": "1  Introduction",
    "section": "1.2 Importance of epidemics",
    "text": "1.2 Importance of epidemics\nEpidemics bear significant economic importance due to their potential to decrease crop yields, diminish product quality, and escalate control costs, contingent on their intensity level. Numerous historical examples of widespread epidemics, reaching pandemic levels and resulting in catastrophic effects on crops, have been documented (Agrios 2005b). The Irish potato famine of 1845–1847, caused by the late blight pathogen (Phytophthora infestans), is a famous example of a well-documented pandemic. This disease notably altered the course of history in Europe and the United States, and was pivotal in the evolution of the science of plant pathology. During the 1840s, the pathogen ravaged potato crops, which were a dietary staple for the Irish. The disease outbreak was triggered by the introduction of a novel, virulent pathogen population that found suitable environmental conditions (cool and wet weather) for infection and development within a dense population of susceptible hosts.\nHowever, there are several reasons why devastating epidemics may continue to unfold. Recent history has seen severe epidemics reaching pandemic levels due to the incursion of pathogens into regions where they had previously been absent (refer to Box 1). Alternatively, new pathogenic strains might emerge as a result of factors driving genetic diversity within the local pathogen population. A case in point is the Ug99 strain of the wheat stem rust, which poses a significant threat to global wheat production. First identified in Uganda in 1998, an asexual lineage has propagated through Africa and the Middle East, causing catastrophic epidemics. Research suggests that Ug99 emerged via somatic hybridization and nuclear exchange between isolates from different lineages (Li et al. 2019). Finally, disease emergence or re-emergence can be influenced by shifts in climatic patterns. For instance, the Fusarium head blight of wheat caused by the fungus Fusarium graminearum. In Southern Brazil, the increased frequency of severe epidemics resulting in greater yield loss since the early 1990s has been linked to alterations in rainfall patterns across decades (Duffeck et al. 2020).\n\n\n\n\n\n\nBox 1: Diseases on the move\n\n\n\nIn Brazil, the soybean rust pathogen (Phakopsora pachyrhizi) first reached southern Brazil in 2002 (Yorinori et al. 2005). The disease spread to all production regions of the country in the following few years, severely reducing yields. To overcome the problem, farmers have relied on massive applications of fungicides on soybeans, which dramatically increased the production costs with the need for sequential fungicide sprays to combat the disease. Total economic loss have been estimated at around US$ 2 billion yearly (Godoy et al. 2016). More recently, wheat blast, a disease that originated in the south of Brazil in 1984, and have been restricted to South America, was firstly spotted in South Asia, Bangladesh, in 2016. Blast epidemics in that occasion devastated more than 15,000 ha of wheat and reduced yield of wheat in the affected field up to 100% (Islam et al. 2019; Malaker et al. 2016). The disease was later found in Zambia, thus also becoming a threat to wheat production in Africa (Tembo et al. 2020). In Brazil, the wheat blast disease is a current threat to expansion of wheat cultivation in the tropics(Cruz and Valent 2017).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#history-of-epidemiology",
    "href": "intro.html#history-of-epidemiology",
    "title": "1  Introduction",
    "section": "1.3 History of Epidemiology",
    "text": "1.3 History of Epidemiology\nBotanical epidemiology, or the study of plant disease epidemics, is a discipline with roots tracing back to the early 1960s. However, its origins can be linked to events from centuries and decades prior. For instance, in 1728, Duhamel de Monceau presented the earliest known epidemiological work on a disease, referred to as ‘Death,’ that afflicted saffron crocus (Rhizoctonia violacea). Fast forward to 1858, a textbook detailing plant diseases, written by Julius Kuhn, made its debut, introducing the concept of an epidemic as illustrated by the Irish late blight epidemics of 1845-46. Subsequently, in 1901, H.M. Ward adopted an ecological perspective to the study of plant diseases in his seminal book, Disease in Plants. By 1946, Gäumann penned the first book exclusively devoted to plant disease epidemiology.\nFurther evolution of this field was marked by the publication of a chapter titled “Analysis of Epidemics” by J.E. Vanderplank in Plant Pathology, vol. 3, edited by Horsfall and Dimond, in 1960. Vanderplank elaborated on his pioneering ideas in his 1963 book, “Plant Diseases: Epidemics and Control”(Vanderplank 1963). He is universally recognized as the foundational figure of plant disease epidemiology (Thresh 1998; Zadoks and Schein 1988), his landmark book being the first to comprehensively describe and quantify plant disease epidemics, and offering a theoretical framework for epidemic analysis.\nIn the same year, the first International Epidemiology Workshop was convened in Pau, France. This event constitutes an important milestone in the historical narrative, significantly contributing to the molding of this emergent discipline.\n\n\n\n\n\n\nFigure 1.2: Group photo of the First International Epidemiology Workshop\n\n\n\nThe International Epidemiology Workshop (IEW) is the principal working group of plant disease epidemiology. This is an organization with a rich history whose members have met approximately every 5 years since 1963. Thus far, 13meetings have been organized/planned:\n1963 - Pau, France\n1971 - Wageningen, The Netherlands\n1979 - Penn State, United States\n1983 - NC State, Raleigh, United States\n1986 - Jerusalem, Israel\n1990 - Giessen, Germany\n1994 - Papendal, The Netherlands\n2001 - Ouro Preto, Brazil\n2005 - Landerneau, France\n2009 - Cornell, Geneva, United States\n2013 - Beijing, China\n2018 - Lillehammer, Norway\n2024 - Iguassu Falls, Brazil",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#other-resources",
    "href": "intro.html#other-resources",
    "title": "1  Introduction",
    "section": "1.4 Other resources",
    "text": "1.4 Other resources\n\n1.4.1 Books\n2006 - The Epidemiology of Plant Diseases\n2007 - The Study of Plant Disease Epidemics\n2017 - Exercises in Plant Disease Epidemiology\n2017 - Application of Information Theory to Epidemiology\n2020 - Emerging Plant Diseases and Global Security\n\n\n\n1.4.2 Online tutorials\nEcology and Epidemiology in R\nPlant Disease Epidemiology - Temporal aspects\nSimulation Modeling in Plant Disease Epidemiology and Crop Loss Analysis\n\n\n1.4.3 Software\nEpicrop - Simulation Modeling of Crop Diseases using a SEIR model\n\n\n\n\nAgrios, G. N. 2005a. INTRODUCTION. In Elsevier, pp. 3–75. https://doi.org/10.1016/b978-0-08-047378-9.50007-5.\n\n\nAgrios, G. N. 2005b. Plant disease epidemiology. In Elsevier, pp. 265–291. https://doi.org/10.1016/b978-0-08-047378-9.50014-2.\n\n\nCruz, C. D., and Valent, B. 2017. Wheat blast disease: danger on the move. Tropical Plant Pathology 42:210–222. https://doi.org/10.1007/s40858-017-0159-z.\n\n\nDuffeck, M. R., Santos Alves, K. dos, Machado, F. J., Esker, P. D., and Del Ponte, E. M. 2020. Modeling Yield Losses and Fungicide Profitability for Managing Fusarium Head Blight in Brazilian Spring Wheat. Phytopathology® 110:370–378. https://doi.org/10.1094/phyto-04-19-0122-r.\n\n\nFrancl, L. J. 2001. The..disease triangle: A plant pathological paradigm revisited. The Plant Health Instructor. https://doi.org/10.1094/phi-t-2001-0517-01.\n\n\nGodoy, C. V., Seixas, C. D. S., Soares, R. M., Marcelino-Guimarães, F. C., Meyer, M. C., and Costamilan, L. M. 2016. Asian soybean rust in brazil: Past, present, and future. Pesquisa Agropecuária Brasileira 51:407–421. https://doi.org/10.1590/s0100-204x2016000500002.\n\n\nIslam, M. T., Kim, K.-H., and Choi, J. 2019. Wheat Blast in Bangladesh: The Current Situation and Future Impacts. The Plant Pathology Journal 35:1–10. https://doi.org/10.5423/ppj.rw.08.2018.0168.\n\n\nLi, F., Upadhyaya, N. M., Sperschneider, J., Matny, O., Nguyen-Phuc, H., Mago, R., Raley, C., Miller, M. E., Silverstein, K. A. T., Henningsen, E., Hirsch, C. D., Visser, B., Pretorius, Z. A., Steffenson, B. J., Schwessinger, B., Dodds, P. N., and Figueroa, M. 2019. Emergence of the Ug99 lineage of the wheat stem rust pathogen through somatic hybridisation. Nature Communications 10. https://doi.org/10.1038/s41467-019-12927-7.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. The study of plant disease epidemics. The American Phytopathological Society. https://doi.org/10.1094/9780890545058.\n\n\nMalaker, P. K., Barma, N. C. D., Tiwari, T. P., Collis, W. J., Duveiller, E., Singh, P. K., Joshi, A. K., Singh, R. P., Braun, H.-J., Peterson, G. L., Pedley, K. F., Farman, M. L., and Valent, B. 2016. First Report of Wheat Blast Caused by Magnaporthe oryzae Pathotype triticum in Bangladesh. Plant Disease 100:2330–2330. https://doi.org/10.1094/pdis-05-16-0666-pdn.\n\n\nTembo, B., Mulenga, R. M., Sichilima, S., M’siska, K. K., Mwale, M., Chikoti, P. C., Singh, P. K., He, X., Pedley, K. F., Peterson, G. L., Singh, R. P., and Braun, H. J. 2020. Detection and characterization of fungus (Magnaporthe oryzae pathotype Triticum) causing wheat blast disease on rain-fed grown wheat (Triticum aestivum L.) in Zambia ed. Zonghua Wang. PLOS ONE 15:e0238724. https://doi.org/10.1371/journal.pone.0238724.\n\n\nThresh, J. M. 1998. In memory of James Edward Vanderplank 19091997. Plant Pathology 47:114–115. https://doi.org/10.1046/j.1365-3059.2998.00220.x.\n\n\nVanderplank, J. 1963. Plant disease epidemics and control. Elsevier. https://doi.org/10.1016/c2013-0-11642-x.\n\n\nYorinori, J. T., Paiva, W. M., Frederick, R. D., Costamilan, L. M., Bertagnolli, P. F., Hartman, G. E., Godoy, C. V., and Nunes, J. 2005. Epidemics of Soybean Rust (Phakopsora pachyrhizi) in Brazil and Paraguay from 2001 to 2003. Plant Disease 89:675–677. https://doi.org/10.1094/pd-89-0675.\n\n\nZadoks, J. C., and Schein, R. D. 1988. James Edward Vanderplank: Maverick* and Innovator. Annual Review of Phytopathology 26:31–37. https://doi.org/10.1146/annurev.py.26.090188.000335.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data-terminology.html",
    "href": "data-terminology.html",
    "title": "2  Disease variables",
    "section": "",
    "text": "2.1 Disease quantification\nStudies on the temporal progression or spatial spread of epidemics cannot be conducted without field-collected data, or, in some cases, simulated data. The study of plant disease quantification, termed Phytopathometry, is a subdivision of plant pathology concerned with the science of disease measurement. It has strong ties to the field of epidemiology (Bock et al. 2021).\nTraditionally, disease quantification has been executed through visual evaluation. However, the past few decades have witnessed significant advancements in imaging and remote sensing technologies (which don’t necessitate contact with the object), leaving a profound impact on this field. As such, disease quantity can now be gauged through estimation (visually, by the human eye) or measurement (through remote sensing technologies such as RGB, MSI, and HSI) Figure 2.1.\nWhile the utilization of digital or remote sensing technology for disease measurement or estimation provides a more objective approach, visual assessment is largely subjective. It is known to vary among human raters, as these raters differ in their innate abilities, training, and how they are influenced by the chosen method (e.g., scales). Disease is estimated or measured on a specimen within a population, or on a sample of specimens drawn from that population. The specimen in question can be a plant organ, an individual plant, a group of plants, a field, or a farm. The specific specimen type also determines the terminology used to describe disease quantity.\nFinally, while developing new or refining existing disease assessment methods, it is crucial to evaluate the reliability of the assessments made by different raters or instruments, as well as their accuracy—specifically, how close the estimations or measurements are to the reference (or gold standard) values. Several methods are available for assessing the reliability, precision, and accuracy of these estimates or measurements (see definitions). The choice of methods depends on the objective of the work, but largely on the type or nature of the data. These considerations will be further discussed.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Disease variables</span>"
    ]
  },
  {
    "objectID": "data-terminology.html#disease-quantification",
    "href": "data-terminology.html#disease-quantification",
    "title": "2  Disease variables",
    "section": "",
    "text": "Figure 2.1: Different approaches used to obtain estimates or measures of plant disease. RGB = red, green, blue; MSI = multispectral imaging; HSI = hyperspectral imaging.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Disease variables</span>"
    ]
  },
  {
    "objectID": "data-terminology.html#disease-variables",
    "href": "data-terminology.html#disease-variables",
    "title": "2  Disease variables",
    "section": "2.2 Disease variables",
    "text": "2.2 Disease variables\nA common term used to reference the quantity of disease, irrespective of how it is expressed, is ‘disease intensity’. This term, however, has minimal practical value as it only implies that the disease is more or less “intense”. We require more specific terminology to standardize the reference to disease quantity and methodology. One of the primary tasks in disease assessment is classifying each specimen, often in a sample or within a population, as diseased or not diseased. This binary (yes/no or 1/0) evaluation may sufficiently express disease intensity if the goal is to ascertain the number or proportion of diseased specimens in a sample or a population.\nThis discussion brings us to two terms: disease incidence and prevalence. Incidence is typically used to denote the proportion or number (count) of plants (or their organs) deemed as observational units at the field scale or below. On the other hand, prevalence refers to the proportion or number of fields or farms with diseased plants within a larger production area or region (Nutter et al. 2006) Figure 2.2. Therefore, prevalence is analogous to incidence, with the only difference being the spatial scale of the sampling unit.\n\n\n\n\n\n\nFigure 2.2: Schematic representation of how prevalence and incidence of plant diseases are calculated depending on the spatial scale of the assessment\n\n\n\nIn many instances, it’s necessary to determine the degree to which a specimen is diseased, a concept defined as disease severity. In certain contexts, severity is narrowly defined as the proportion of the unit that exhibits symptoms (Nutter et al. 2006). However, a more expansive view of severity includes additional metrics such as nominal or ordinal scores, lesion count, and percent area affected (ratio scale). Ordinal scales are broken down into rank-ordered classes (see specific section), defined based on either a percentage scale or descriptions of symptoms (Bock et al. 2021). Occasionally, disease is expressed in terms of (average) lesion size or area, which could be regarded as a measure of severity. These variables represent different levels of measurements that provide varying degrees of information about the disease quantity - from low (nominal scale) to high (ratio scale) Figure 2.3.\n\n\n\n\n\n\nFigure 2.3: Scales and associated levels of measurement used to describe severity of plant diseases",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Disease variables</span>"
    ]
  },
  {
    "objectID": "data-terminology.html#data-types",
    "href": "data-terminology.html#data-types",
    "title": "2  Disease variables",
    "section": "2.3 Data types",
    "text": "2.3 Data types\nThe data used to express disease as incidence or any form of severity measurements can be discrete or continuous in nature.\nDiscrete variables are countable (involving integers) at a particular point in time. In other words, only a finite number of values (nominal or ordinal) is possible, and these cannot be subdivided. For instance, a plant or plant part can be either diseased or not diseased (nominal data). It’s not possible to count 1.5 diseased plants. Furthermore, a plant classified as diseased may exhibit a certain number of lesions (count data), or be categorized into a specific severity class (ordinal data, common in ordinal scales, e.g., 1-9). Disease data in the form of counts often relates to the number of infections per sampling units. Most commonly, these counts refer to the assessed pathogen population, such as the number of airborne or soilborne propagules.\nIn contrast to discrete variables, continuous variables can be measured on a scale and can assume any numeric value between two points. For example, the size of a lesion on a plant can be measured at a very precise scale (cm or mm). An estimate of severity on a percent scale (% diseased area) can take any value between non-zero and 100%. Although incidence at the individual level is discrete, at the sample level it can be treated as continuous, as it can assume any value in proportion or percentage.\nDisease variables can also be characterized by a statistical distribution, which are models that provide the probability of a specific value (or a range of values) being drawn from a particular distribution. Understanding statistical or mathematical distributions is a crucial step in improving our grasp of data collection methods, experiment design, and data analysis processes such as data summarization or hypothesis testing.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Disease variables</span>"
    ]
  },
  {
    "objectID": "data-terminology.html#statistical-distributions-and-simulation",
    "href": "data-terminology.html#statistical-distributions-and-simulation",
    "title": "2  Disease variables",
    "section": "2.4 Statistical distributions and simulation",
    "text": "2.4 Statistical distributions and simulation\n\n2.4.1 Binomial distribution\nFor incidence (and prevalence), the data is binary at the individual level, as there are only two possible outcomes in a trial: the plant or plant part is disease or not diseased. The statistical distribution that best describe the incidence data at the individual level is the binomial distribution.\nLet’s simulate the binomial outcomes for a range of probabilities in a sample of 100 units, using the rbinom() function in R. For a single trial (e.g., status of plants in a single plant row), the size argument is set to 1.\n\nlibrary(tidyverse)\nlibrary(r4pde)\n\n\nset.seed(123) # for reproducibility\nP.1 &lt;- rbinom(100, size = 1, prob = 0.1)\nP.3 &lt;- rbinom(100, size = 1, prob = 0.3)\nP.7 &lt;- rbinom(100, size = 1, prob = 0.7)\nP.9 &lt;- rbinom(100, size = 1, prob = 0.9)\nbinomial_data &lt;- data.frame(P.1, P.3, P.7, P.9)\n\nWe can then visualize the plots.\n\nbinomial_data |&gt;\n  pivot_longer(1:4, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 bins = 10) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\n\n\n\nFigure 2.4: Binomial distribution to describe binary data\n\n\n\n\n\n\n\n2.4.2 Beta distribution\nIn many studies, it’s often useful to express these quantities as a proportion of the total population or sample size, rather than absolute numbers. This helps standardize the data, making it easier to compare between different populations or different time periods.\nFor example, if we’re studying a plant disease, we could express disease incidence as the proportion of plants that are newly diseased during a given time period. Similarly, disease severity could be expressed as the proportion of each plant’s organ area that is affected by the disease. These proportions are ratio variables, as they can take on any value between 0 and 1, and ratios of these variables are meaningful.\nThe Beta distribution is a probability distribution that is defined between 0 and 1, which makes it ideal for modeling data that represents proportions. It’s a flexible distribution, as its shape can take many forms depending on the values of its two parameters, often denoted as alpha and beta.\nLet’s simulate some data using the rbeta() function.\n\nbeta1.5 &lt;- rbeta(n = 1000, shape1 = 1, shape2 = 5)\nbeta5.5 &lt;- rbeta(n = 1000, shape1 = 5, shape2 = 5)\nbeta_data &lt;- data.frame(beta1.5, beta5.5)\n\nNotice that there are two shape parameters in the beta distribution: shape1 and shape2 to be defined. This makes the distribution very flexible and with different potential shapes as we can see below.\n\nbeta_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\",\n                 bins = 15) +\n  scale_x_continuous(limits = c(0, 1)) +\n  facet_wrap( ~ P)+\n  theme_r4pde()\n\n\n\n\n\n\n\nFigure 2.5: Binomial distribution to describe proportion data\n\n\n\n\n\n\n\n2.4.3 Beta-binomial distribution\nThe Beta-Binomial distribution is a mixture of the Binomial distribution with the Beta distribution acting as a prior on the probability parameter of the binomial. Disease probabilities can vary across trials due to a number of unobserved or unmeasured factors. This variability can result in overdispersion, a phenomenon where the observed variance in the data is greater than what the binomial distribution expects.\nThis is where the Beta-Binomial distribution comes in handy. By combining the Beta distribution’s flexibility in modeling probabilities with the Binomial distribution’s discrete event modeling, it provides an extra layer of variability to account for overdispersion. The Beta-Binomial distribution treats the probability of success (disease occurrence in this context) as a random variable itself, following a Beta distribution. This means the probability can vary from trial to trial.\nTherefore, when we observe data that shows more variance than the Beta distribution can account for, or when we believe there are underlying factors causing variability in the probability of disease occurrence, the Beta-Binomial distribution is a more appropriate model. It captures both the variability in success probability as well as the occurrence of the discrete event (disease incidence).\nWhen combined with the Binomial distribution, which handles discrete events (e.g. whether an individual is diseased or not), the Beta-Binomial distribution allows us to make probabilistic predictions about these events. For example, based on prior data (the Beta distribution), we can estimate the likelihood of a particular individual being diseased (the Binomial distribution).\nIn R, the rBetaBin function of the FlexReg package generates random values from the beta-binomial distribution. The arguments of the function are n, or the number of values to generate; if length(n) &gt; 1, the length is taken to be the number required. size is he total number of trials. mu is the mean parameter. It must lie in (0, 1). theta is the overdispersion parameter. It must lie in (0, 1). phi the precision parameter. It is an alternative way to specify the theta parameter. It must be a positive real value.\n\nlibrary(FlexReg) \nbetabin3.6 &lt;- rBetaBin(n = 100, size = 40, mu = .3, theta = .6)\nbetabin7.3 &lt;- rBetaBin(n = 100, size = 40, mu = .7, theta = .3)\nbetabin_data &lt;- data.frame(betabin3.6, betabin7.3)\n\n\nbetabin_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\",\n                 bins = 15) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\n\n\n\nFigure 2.6: Beta-binomial distribution to describe proportion data\n\n\n\n\n\n\n\n2.4.4 Poisson distribution\nWhen conducting studies in epidemiology, specifically plant diseases, researchers often collect data on the number of diseased plants, infected plant parts, or individual symptoms, such as lesions. These variables are counted in whole numbers - 1, 2, 3, etc., making them discrete variables. Discrete variables contrast with continuous variables that can take any value within a defined range and can include fractions or decimals. In addition to being discrete, these variables are also non-negative, meaning they cannot take negative values. After all, you can’t have a negative number of diseased plants or lesions. Given these characteristics, a suitable distribution to model such data is the Poisson distribution. This distribution is particularly suitable for counting the number of times an event occurs in a given time or space.\nIn R, we can used the rpois() function to obtain 100 random observations following a Poisson distribution. For such, we need to inform the number of observation (n = 100) and lambda, the vector of means.\n\npoisson5 &lt;- rpois(100, lambda = 10)\npoisson35 &lt;- rpois(100, lambda = 35)\npoisson_data &lt;- data.frame(poisson5, poisson35)\n\n\npoisson_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\",\n                 bins = 15) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\n\n\n\nFigure 2.7: Poisson distribution to describe count data\n\n\n\n\n\n\n\n2.4.5 Negative binomial distribution\nWhile the Poisson distribution is indeed suitable for modeling count data, it assumes that the mean and variance of the data are equal. However, in real-world scenarios, especially in epidemiology, it is common to encounter overdispersed data - where the variance is greater than the mean. This could occur, for instance, if there’s greater variability in disease incidence across different plant populations than would be expected under the Poisson assumption.\nIn such cases, the Negative Binomial distribution is a better alternative. The Negative Binomial distribution is a discrete probability distribution that models the number of successes in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of failures occurs.\nOne of the key features of the Negative Binomial distribution is its ability to handle overdispersion. Unlike the Poisson distribution, which has one parameter (lambda, representing the mean and variance), the Negative Binomial distribution has two parameters. One parameter is the mean, but the other (often denoted as ‘size’ or ‘shape’) governs the variance independently, allowing it to be larger than the mean if necessary. Thus, it provides greater flexibility than the Poisson distribution for modeling count data and can lead to more accurate results when overdispersion is present.\nIn R, we can use the rnbinom() function to generate random variates from a Negative Binomial distribution. This function requires the number of observations (n), the target for the number of successful trials (size), and the probability of each success (prob).\nHere’s an example:\n\n# Generate 100 random variables from a Negative Binomial distribution\nnegbin14.6 &lt;- rnbinom(n = 100, size = 14, prob = 0.6)\nnegbin50.6 &lt;- rnbinom(n = 100, size = 50, prob = 0.6)\nnegbin_data &lt;- data.frame(negbin14.6, negbin50.6)\n\n\nnegbin_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\", bins = 15) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\n\n\n\nFigure 2.8: Negative binomial distribution to describe overdispersed count data\n\n\n\n\n\n\n\n2.4.6 Gamma distribution\nIn plant disease epidemiology and other fields of study, we may often encounter continuous variables - these are variables that can take on any value within a given range, including both whole numbers and fractions. An example of a continuous variable in this context is lesion size, which can theoretically be any non-negative value.\nOften, researchers use the normal (Gaussian) distribution to model such continuous variables. The normal distribution is symmetric, bell-shaped, and is fully described by its mean and standard deviation. However, a fundamental characteristic of the normal distribution is that it extends from negative infinity to positive infinity. While this is not a problem for many applications, it becomes an issue when the variable being modeled cannot take on negative values - like the size of a lesion.\nThis is where the Gamma distribution can be a good alternative. The Gamma distribution is a two-parameter family of continuous probability distributions, which does not include negative values, making it an appropriate choice for modeling variables like lesion sizes. While it might seem a bit more complicated due to its two parameters, this also allows it a greater flexibility in terms of the variety of shapes and behaviors it can describe. The Gamma distribution is often used in various scientific disciplines, including queuing models, climatology, financial services, and of course, epidemiology. Its main parameters are the shape and scale (or alternatively shape and rate), which control the shape, spread and location of the distribution.\nWe can use the rgamma() function that requires the number of samples (n = 100 in our case) and the shape, or the mean value.\n\ngamma10 &lt;- rgamma(n = 100, shape = 10, scale = 1)\ngamma35 &lt;- rgamma(n = 100, shape = 35, scale = 1)\ngamma_data &lt;- data.frame(gamma10, gamma35)\n\n\ngamma_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\",\n                 bins = 15) +\n  ylim(0, max(gamma_data$gamma35)) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\n\n\n\nFigure 2.9: Gamma distribution to describe continuous data\n\n\n\n\n\n\n\n2.4.7 Simulating ordinal data\nOrdinal data is a statistical data type consisting of numerical scores that fall into a set of categories which are ordered in a meaningful way. This can include survey responses (e.g., strongly disagree to strongly agree), levels of achievement (e.g., poor, average, good, excellent), or, in the case of plant disease, disease severity scales (e.g., 0 to 5, where 0 represents a healthy plant and 5 represents a plant with severe symptoms).\nWhen working with ordinal data, we often need to make assumptions about the distribution of the data. However, unlike continuous data which might be modeled by a normal or Gamma distribution, or count data which might be modeled by a Poisson distribution, ordinal data is discrete and has a clear order but the distances between the categories are not necessarily equal or known. This makes the modeling process slightly different.\nWe can use the sample() function and define the probability associated with each rank. Let’s generate 30 units with a distinct ordinal score. In the first situation, the higher probabilities (0.5) are for scores 4 and 5 and lower (0.1) for scores 0 and 1, and in the second situation is the converse.\n\nordinal1 &lt;- sample(0:5, 30, replace = TRUE, prob = c(0.1, 0.1, 0.2, 0.2, 0.5, 0.5))\nordinal2 &lt;- sample(0:5, 30, replace = TRUE, prob = c(0.5, 0.5, 0.2, 0.2, 0.1, 0.1))\nordinal_data &lt;- data.frame(ordinal1, ordinal2)\n\n\nordinal_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\",\n                 bins = 6) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\n\n\n\nFigure 2.10: Sampling of ordinal data\n\n\n\n\n\n\n\n\n\nBock, C. H., Pethybridge, S. J., Barbedo, J. G. A., Esker, P. D., Mahlein, A.-K., and Del Ponte, E. M. 2021. A phytopathometry glossary for the twenty-first century: towards consistency and precision in intra- and inter-disciplinary dialogues. Tropical Plant Pathology 47:14–24. https://doi.org/10.1007/s40858-021-00454-0.\n\n\nNutter, F. W., Esker, P. D., and Netto, R. A. C. 2006. Disease Assessment Concepts and the Advancements Made in Improving the Accuracy and Precision of Plant Disease Data. European Journal of Plant Pathology 115:95–103. https://doi.org/10.1007/s10658-005-1230-z.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Disease variables</span>"
    ]
  },
  {
    "objectID": "data-ordinal.html",
    "href": "data-ordinal.html",
    "title": "3  Ordinal scales",
    "section": "",
    "text": "3.1 Introduction\nOrdinal scales are organized as rank-ordered numeric classes, with a finite number of such classes. The utilization of ordinal scales is often due to their convenience and speed of rating (Madden et al. 2007). In fact, plant pathologists often encounter situations where direct estimates of the nearest percent severity are time-consuming or impractical, besides being unreliably and inacuratelly estimated. In plant pathological research, there are two commonly used types of ordinal scales: quantitative and qualitative (Chiang and Bock 2021).",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinal scales</span>"
    ]
  },
  {
    "objectID": "data-ordinal.html#introduction",
    "href": "data-ordinal.html#introduction",
    "title": "3  Ordinal scales",
    "section": "",
    "text": "3.1.1 Quantitative ordinal\nIn the quantitative ordinal scale, each score signifies a defined interval of the percentage scale. The most renowned quantitative ordinal scale is the Horsfall-Barratt (HB) scale, which was developed in the early 1940s when the science of plant pathology was transitioning towards more quantitative methodologies (Hebert 1982). The HB scale partitions the percentage scale into twelve successive, logarithmic-based intervals of severity ranging from 0 to 100%. The intervals increase in size from 0 to 50% and decrease from 50 to 100%.\n\n\n\n\n\n\nControversy of the H-B scale\n\n\n\nThe divisions of the H-B scale were established on two assumptions. The first was the logarithmic relationship between the intensity of a stimulus and the subsequent sensation. The second was the propensity of a rater to focus on smaller objects when observing objects of two colors (Madden et al. 2007). This foundation is based on the so-called Weber-Fechner law. However, there is limited experimental evidence supporting these assumptions. Current evidence indicates a linear relationship, rather than a logarithmic one, between visually estimated and actual severity (Nutter and Esker 2006). Additionally, these authors demonstrated that raters more accurately discriminated disease severity between 25% and 50% than what the H-B scale allowed. New scale structures have been proposed to address the issues associated with the H-B scale (Chiang et al. 2014; Liu et al. 2019). The Chiang scale follows a linear relationship with the percentage area diseased at severities greater than 10% (class 6 on the scale).\n\n\nLet’s input the HB scale data and store as a data frame in R so we can prepare a table and a plot.\n\nHB &lt;- tibble::tribble(\n  ~ordinal, ~'range', ~midpoint,\n  0,          '0',    0,   \n  1,    '0+ to 3',  1.5,   \n  2,    '3+ to 6',  4.5,   \n  3,   '6+ to 12',  9.0,  \n  4,  '12+ to 25', 18.5, \n  5,  '25+ to 50', 37.5, \n  6,  '50+ to 75', 62.5, \n  7,  '75+ to 88', 81.5, \n  8,  '88+ to 94', 91.0, \n  9,  '94+ to 97', 95.5, \n  10,'97+ to 100', 98.5,  \n  11,      '100',   100 \n  )\nknitr::kable(HB, align = \"c\")\n\n\n\nTable 3.1: The Horsfal-Barrat quantitative ordinal scale used as a tool for assessing plant disease severity\n\n\n\n\n\n\nordinal\nrange\nmidpoint\n\n\n\n\n0\n0\n0.0\n\n\n1\n0+ to 3\n1.5\n\n\n2\n3+ to 6\n4.5\n\n\n3\n6+ to 12\n9.0\n\n\n4\n12+ to 25\n18.5\n\n\n5\n25+ to 50\n37.5\n\n\n6\n50+ to 75\n62.5\n\n\n7\n75+ to 88\n81.5\n\n\n8\n88+ to 94\n91.0\n\n\n9\n94+ to 97\n95.5\n\n\n10\n97+ to 100\n98.5\n\n\n11\n100\n100.0\n\n\n\n\n\n\n\n\nLet’s visualize the different sizes of the percent interval encompassing each score.\n\n\nCode\nlibrary(tidyverse)\nlibrary(r4pde)\nHB |&gt; \n  ggplot(aes(midpoint, ordinal))+\n  geom_point(size =2)+\n  geom_line()+\n  scale_x_continuous(breaks = c(0, 3, 6, 12, 25, 50, 75, 88, 94, 97))+\n  scale_y_continuous(breaks = c(1:12))+\n  geom_vline(aes(xintercept = 3), linetype = 2)+\n  geom_vline(aes(xintercept = 6), linetype = 2)+\n  geom_vline(aes(xintercept = 12), linetype = 2)+\n  geom_vline(aes(xintercept = 25), linetype = 2)+\n  geom_vline(aes(xintercept = 50), linetype = 2)+\n  geom_vline(aes(xintercept = 75), linetype = 2)+\n  geom_vline(aes(xintercept = 88), linetype = 2)+\n  geom_vline(aes(xintercept = 94), linetype = 2)+\n  geom_vline(aes(xintercept = 97), linetype = 2)+\n  labs(x = \"Percent severity\", y = \"HB score\")+\n  theme_r4pde()\n\n\n\n\n\n\n\n\nFigure 3.1: Ordinal scores of the Horsfal-Barrat scale\n\n\n\n\n\nWe can repeat those procedures to visualize the Chiang scale.\n\nchiang &lt;- tibble::tribble(\n  ~ordinal, ~'range', ~midpoint,\n  0,          '0',     0,   \n  1,  '0+ to 0.1',  0.05,   \n  2,'0.1+ to 0.5',   0.3,   \n  3,  '0.5+ to 1',  0.75,  \n  4,    '1+ to 2',   1.5, \n  5,    '2+ to 5',     3, \n  6,   '5+ to 10',   7.5, \n  7,  '10+ to 20',    15, \n  8,  '20+ to 30',    25, \n  9,  '30+ to 40',    35, \n  10, '40+ to 50',    45,  \n  11, '50+ to 60',    55,\n  12, '60+ to 70',    65,\n  13, '70+ to 80',    75,\n  14, '80+ to 90',    85,\n  15,'90+ to 100',   95\n  )\nknitr::kable(chiang, align = \"c\")\n\n\n\nTable 3.2: The Chiang quantitative ordinal scale used as a tool for assessing plant disease severity\n\n\n\n\n\n\nordinal\nrange\nmidpoint\n\n\n\n\n0\n0\n0.00\n\n\n1\n0+ to 0.1\n0.05\n\n\n2\n0.1+ to 0.5\n0.30\n\n\n3\n0.5+ to 1\n0.75\n\n\n4\n1+ to 2\n1.50\n\n\n5\n2+ to 5\n3.00\n\n\n6\n5+ to 10\n7.50\n\n\n7\n10+ to 20\n15.00\n\n\n8\n20+ to 30\n25.00\n\n\n9\n30+ to 40\n35.00\n\n\n10\n40+ to 50\n45.00\n\n\n11\n50+ to 60\n55.00\n\n\n12\n60+ to 70\n65.00\n\n\n13\n70+ to 80\n75.00\n\n\n14\n80+ to 90\n85.00\n\n\n15\n90+ to 100\n95.00\n\n\n\n\n\n\n\n\n\nchiang |&gt; \n  ggplot(aes(midpoint, ordinal))+\n  geom_point(size =2)+\n  geom_line()+\n  scale_y_continuous(breaks = c(0:15))+\n  scale_x_continuous(breaks = c(0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100))+\n  geom_vline(aes(xintercept = 0), linetype = 2)+\n  geom_vline(aes(xintercept = 0.1), linetype = 2)+\n  geom_vline(aes(xintercept = 0.5), linetype = 2)+\n  geom_vline(aes(xintercept = 1), linetype = 2)+\n  geom_vline(aes(xintercept = 2), linetype = 2)+\n  geom_vline(aes(xintercept = 5), linetype = 2)+\n  geom_vline(aes(xintercept = 10), linetype = 2)+\n  geom_vline(aes(xintercept = 20), linetype = 2)+\n  geom_vline(aes(xintercept = 30), linetype = 2)+\n   geom_vline(aes(xintercept = 40), linetype = 2)+\n   geom_vline(aes(xintercept = 50), linetype = 2)+\n   geom_vline(aes(xintercept = 60), linetype = 2)+\n   geom_vline(aes(xintercept = 70), linetype = 2)+\n   geom_vline(aes(xintercept = 80), linetype = 2)+\n   geom_vline(aes(xintercept = 90), linetype = 2)+\n   geom_vline(aes(xintercept = 100), linetype = 2)+\n  labs(x = \"Percent severity\", y = \"Chiang score\")+\n  theme_r4pde()\n\n\n\n\n\n\n\nFigure 3.2: Ordinal scores of the Chiang scale\n\n\n\n\n\n\n\n3.1.2 Qualitative ordinal\nIn the qualitative ordinal scale, each class provides a description of the symptoms. An example is the ordinal 0-3 scale for rating eyespot of wheat developed by (Scott and Hollins 1974).\n\nOrdinal scale for rating eyespot of wheat (Scott and Hollins 1974)\n\n\n\n\n\n\nClass\nDescription\n\n\n\n\n0\nuninfected\n\n\n1\nslight eyespot (or or more small lesion occupying in total less than half of the circumference of the stem)\n\n\n2\nmoderate eyespot (one or more lesions occupying at least half the circumference of the stem)\n\n\n3\nsevere eyespot (stem completely girdled by lesions; tissue softened so that lodging would really occur)",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinal scales</span>"
    ]
  },
  {
    "objectID": "data-ordinal.html#disease-severity-index-dsi",
    "href": "data-ordinal.html#disease-severity-index-dsi",
    "title": "3  Ordinal scales",
    "section": "3.2 Disease severity index (DSI)",
    "text": "3.2 Disease severity index (DSI)\nSometimes, when quantitative or qualitative ordinal scales are used, the scores given to various individual specimens (the observational units) are transformed into an index on a percentage basis, such as the disease severity index (DSI) which is used as a value for the experimental unit further in data analysis. The DSI is a single number that summarizes a large amount of information on disease severity (Chester 1950). The formula for a DSI (%) can be written as follows:\n\\(DSI = \\frac{∑(class \\ freq. \\ ✕ \\ score \\  of \\ class)} {total \\ n \\ ✕ \\ maximal \\ class} ✕ 100\\)\nThe DSI() and DSI2() are part of the r4pde package. Let’s see how each function works.\nThe DSI() allows to automate the calculation of the disease severity index (DSI) in a series of units (e.g. leaves) that are further classified according to ordinal scores. The function requires three arguments:\n\nunit = the vector of the number of each unit\nclass = the vector of the scores for the units\nmax = the maximum value of the scale\n\nLet’s create a toy data set composed of 12 units where each received an ordinal score. The vectors were arranged as a data frame named scores.\n\nunit &lt;- c(1:12)\nclass &lt;- c(2,3,1,1,3,4,5,0,2,5,2,1)\nratings &lt;- data.frame(unit, class)\nknitr::kable(ratings)\n\n\n\n\nunit\nclass\n\n\n\n\n1\n2\n\n\n2\n3\n\n\n3\n1\n\n\n4\n1\n\n\n5\n3\n\n\n6\n4\n\n\n7\n5\n\n\n8\n0\n\n\n9\n2\n\n\n10\n5\n\n\n11\n2\n\n\n12\n1\n\n\n\n\n\nThe ordinal score used in this example has 6 as the maximum score. The function returns the DSI value.\n\nlibrary(r4pde)\nDSI(ratings$unit, ratings$class, 6)\n\n[1] 40.27778\n\n\nLet’s now deal with a situation of multiple plots (five replicates) where a fixed number of 12 samples were taken and assessed using an ordinal score. Let’s input the data using the tribble() function. Note that the data is in the wide format.\n\nexp &lt;- tibble::tribble(\n  ~rep, ~`1`, ~`2`, ~`3`, ~`4`, ~`5`, ~`6`, ~`7`, ~`8`, ~`9`, ~`10`, ~`11`,~`12`,\n  1, 2, 3, 1, 1, 3, 4, 5, 0, 2, 5, 2, 1,\n  2, 3, 4, 4, 6, 5, 4, 4, 0, 2, 1, 1, 5,\n  3, 5, 6, 6, 5, 4, 2, 0, 0, 0, 0, 2, 0,\n  4, 5, 6, 0, 0, 0, 3, 3, 2, 1, 0, 2, 3, \n  5, 0, 0, 0, 0, 2, 3, 2, 5, 6, 2, 1, 0,\n)\nknitr::kable(exp)\n\n\n\n\nrep\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n1\n2\n3\n1\n1\n3\n4\n5\n0\n2\n5\n2\n1\n\n\n2\n3\n4\n4\n6\n5\n4\n4\n0\n2\n1\n1\n5\n\n\n3\n5\n6\n6\n5\n4\n2\n0\n0\n0\n0\n2\n0\n\n\n4\n5\n6\n0\n0\n0\n3\n3\n2\n1\n0\n2\n3\n\n\n5\n0\n0\n0\n0\n2\n3\n2\n5\n6\n2\n1\n0\n\n\n\n\n\nAfter reshaping the data to the long format, we can calculate the DSI for each plot/replicate as follows:\n\nres &lt;- exp |&gt; \n  pivot_longer(2:13, names_to = \"unit\", values_to = \"class\") |&gt;\n  group_by(rep) |&gt; \n  summarise(DSI = DSI(unit, class, 6))\n\nAnd here we have the results of the DSI for each replicate.\n\nknitr::kable(res, align = \"c\")\n\n\n\n\nrep\nDSI\n\n\n\n\n1\n40.27778\n\n\n2\n54.16667\n\n\n3\n41.66667\n\n\n4\n34.72222\n\n\n5\n29.16667\n\n\n\n\n\nNow our data set is organized as the frequency of each class as follows:\n\nratings2 &lt;- ratings |&gt; \n  dplyr::count(class)\n\nratings2\n\n  class n\n1     0 1\n2     1 3\n3     2 3\n4     3 2\n5     4 1\n6     5 2\n\n\nNow we can apply the DSI2() function. The function requires three arguments:\n\nclass = the number of the respective class\nfreq = the frequency of the class\nmax = the maximum value of the scale\n\n\nlibrary(r4pde)\nDSI2(ratings2$class, ratings2$n, 6)\n\n[1] 40.27778",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinal scales</span>"
    ]
  },
  {
    "objectID": "data-ordinal.html#analysis-of-ordinal-data",
    "href": "data-ordinal.html#analysis-of-ordinal-data",
    "title": "3  Ordinal scales",
    "section": "3.3 Analysis of ordinal data",
    "text": "3.3 Analysis of ordinal data\nOrdinal score data typically do not align well with the assumptions of traditional parametric statistical methods. Given this challenge, non-parametric methods have emerged as a compelling alternative for handling ordinal plant pathological data (Shah and Madden 2004). Unlike their parametric counterparts, these methods do not rest on the presumption of a specific distribution for the underlying population, offering greater flexibility in accommodating the intricacies inherent to ordinal data. On the other hand, when the conditions are right, parametric methods can also be harnessed effectively.\nA common strategy, particularly for ordinal scores, involves converting these values into the mid-points of their corresponding percent scales. This transformation renders the data more amenable to parametric analyses. However, the mid-point conversion has been criticized in the literature as it may amplify the imprecision, especially when the interval size is wide, and because it does not really reflect a true value, but an interval for each estimate (Chiang et al. 2023; Onofri et al. 2018).\nLet’s see some examples of analysis using the mid-point conversion in a parametric framework as well as the non-parametric tests.\n\n3.3.1 Example data\nWe will use a data set made available in an article on the application of the survival analysis technique to test hypotheses when using quantitative ordinal scales (Chiang et al. 2023). The data and codes used in the paper can be found in the specified GitHub repository. The data is structured into four treatments, each with 30 ratings ranging from a score of 1 to 6 on the H-B scale.\n\n# Create the vectors for the treatments\ntrAs &lt;- c(5,4,2,5,5,4,4,2,5,2,2,3,4,3,2,\n          2,6,2,2,4,2,4,2,4,5,3,4,2,2,3)\ntrBs &lt;- c(5,3,2,4,4,5,4,5,4,4,6,4,5,5,5,\n          2,6,2,3,5,2,6,4,3,2,5,3,5,4,5)\ntrCs &lt;- c(2,3,1,4,1,1,4,1,1,3,2,1,4,1,1,\n          2,5,2,1,3,1,4,2,2,2,4,2,3,2,2)\ntrDs &lt;- c(5,5,4,5,5,6,6,4,6,4,3,5,5,6,4,\n          6,5,6,5,4,5,5,5,3,5,6,5,5,5,6)\n\n# Create the tibble\ndat_ordinal &lt;- tibble::tibble(\n  treatment = rep(c(\"A\", \"B\", \"C\", \"D\"), \n                  each = 30),\n  score = c(trAs, trBs, trCs, trDs)\n)\n\ndat_ordinal\n\n# A tibble: 120 × 2\n   treatment score\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 A             5\n 2 A             4\n 3 A             2\n 4 A             5\n 5 A             5\n 6 A             4\n 7 A             4\n 8 A             2\n 9 A             5\n10 A             2\n# ℹ 110 more rows\n\n\nBecause the ordinal response was obtained using an interval scale, the mid-point of the score was also obtained.\n\n# Create the vectors for the treatments\ntrAm &lt;- c(18.5,9,1.5,18.5,18.5,9,9,1.5,\n          18.5,1.5,1.5,4.5,9,4.5,1.5,1.5,\n          37.5,1.5,1.5,9,1.5,9,1.5,9,18.5,\n          4.5,9,1.5,1.5,4.5)\ntrBm &lt;- c(18.5,4.5,1.5,9,9,18.5,9,18.5,9,\n          9,37.5,9,18.5,18.5,18.5,1.5,37.5,\n          1.5, 4.5,18.5,1.5,37.5,9,4.5,1.5,\n          18.5,4.5,18.5,9,18.5)\ntrCm &lt;- c(1.5,4.5,0,9,0,0,9,0,0,4.5,1.5,0,9\n          ,0,0,1.5,18.5,1.5,0,4.5,0,9,1.5,1.5,\n          1.5,9,1.5,4.5,1.5,1.5)\ntrDm &lt;- c(18.5,18.5,9,18.5,18.5,37.5,37.5,9,\n          37.5,9,4.5,18.5,18.5,37.5,9,37.5,\n       18.5,37.5,18.5,9,18.5,18.5,18.5,4.5,18.5,\n       37.5,18.5,18.5,18.5,37.5)\n\n# Create the tibble\ndat_ordinal_mp &lt;- tibble::tibble(\n  treatment = rep(c(\"A\", \"B\", \"C\", \"D\"), \n                  each = 30),\n  midpoint = c(trAm, trBm, trCm, trDm)\n)\ndat_ordinal_mp\n\n# A tibble: 120 × 2\n   treatment midpoint\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 A             18.5\n 2 A              9  \n 3 A              1.5\n 4 A             18.5\n 5 A             18.5\n 6 A              9  \n 7 A              9  \n 8 A              1.5\n 9 A             18.5\n10 A              1.5\n# ℹ 110 more rows\n\n\nThe visualization using box-plots suggests differences between the treatments.\n\np_score &lt;- dat_ordinal |&gt; \n  ggplot(aes(treatment, score))+\n  geom_boxplot(outlier.colour = NA)+\n  geom_point()+\n  theme_r4pde()\n\np_mp &lt;- dat_ordinal_mp |&gt; \n  ggplot(aes(treatment, midpoint))+\n  geom_boxplot(outlier.colour = NA)+\n  geom_point()+\n  theme_r4pde()\n\nlibrary(patchwork)\np_score | p_mp\n\n\n\n\n\n\n\n\n\n\n3.3.2 Parametric (mid-point conversion)\nParametric methods can be employed when analyzing data, provided that the underlying assumptions of these methods are satisfied. For data that is based on ordinal scores, one way to apply parametric methods is to convert these scores into the mid-points of their corresponding percent scales. Once converted, the data is better suited for parametric analyses. Among the available techniques, the most frequently utilized is the application of an ANOVA (Analysis of Variance) model. This model is particularly useful for testing the null hypothesis, which posits that there are no significant differences among the treatments being studied.\nWe can use the aov function to fit the model and check the parametric assumptions using the DHARMa package.\n\nm1_mp &lt;- aov(midpoint ~ treatment, data = dat_ordinal_mp)\n# normality and homocedasticity tests\nlibrary(DHARMa)\nplot(simulateResiduals(m1_mp))\n\n\n\n\n\n\n\n\nSince both normality and homoscedasticity assumptions are violated in our initial model, we will halt further analysis using this model. Instead, we will explore an alternative approach that includes data transformation (logit).\n\n# data transformation\nlibrary(car)\n# transform midpoint to proportion\ndat_ordinal_mp$midpoint2 &lt;- dat_ordinal_mp$midpoint/100\n\nm2_mp &lt;- aov(logit(midpoint2) ~ treatment, data = dat_ordinal_mp)\nplot(simulateResiduals(m2_mp)) # assumptions are met\n\n\n\n\n\n\n\n\nNow that both assumptions have been met, we can proceed to use a means comparison test to determine which treatments differ from one another, with the assistance of the {emmeans} package. It’s important to note that the results are displayed in the original scale after transformation, specifically when using type = \"response.\n\nlibrary(emmeans)\nmeans_m2_mp &lt;- emmeans(m2_mp, \"treatment\", type = \"response\")\nmeans_m2_mp\n\n treatment response      SE  df lower.CL upper.CL\n A           0.0808 0.00983 116   0.0634   0.1026\n B           0.1248 0.01446 116   0.0989   0.1564\n C           0.0461 0.00582 116   0.0358   0.0591\n D           0.2071 0.02173 116   0.1674   0.2535\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nThe best way to visualize the differences between treatments is using the pwpm function to display a matrix of estimates, pairwise differences, and P values. Most commonly, the compact letter display is used for means comparison.\n\n# matrix\npwpm(means_m2_mp)\n\n         A        B        C        D\nA [0.0808]   0.0530   0.0094   &lt;.0001\nB    0.617 [0.1248]   &lt;.0001   0.0085\nC    1.820    2.953 [0.0461]   &lt;.0001\nD    0.337    0.546    0.185 [0.2071]\n\nRow and column labels: treatment\nUpper triangle: P values   null = 1  adjust = \"tukey\"\nDiagonal: [Estimates] (response)   type = \"response\"\nLower triangle: Comparisons (odds.ratio)   earlier vs. later\n\n# compact letter display\nlibrary(multcomp)\ncld(means_m2_mp)\n\n treatment response      SE  df lower.CL upper.CL .group\n C           0.0461 0.00582 116   0.0358   0.0591  1    \n A           0.0808 0.00983 116   0.0634   0.1026   2   \n B           0.1248 0.01446 116   0.0989   0.1564   2   \n D           0.2071 0.02173 116   0.1674   0.2535    3  \n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \nP value adjustment: tukey method for comparing a family of 4 estimates \nTests are performed on the log odds ratio scale \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\n3.3.3 Non-parametric (ordinal score)\nBecause ordinal score data generally do not meet the assumptions of traditional parametric statistical methods, non-parametric methods can be considered as an alternative. Such methods have been proposed for analyzing ordinal data in plant pathology (Shah and Madden 2004). For our example, when more than two treatments are involved, a Kruskal-Wallis test can be utilized. The kruskal function of the {agricolae} package does the job we want. Note that in this case we use the ordinal score directly and not the mid-point value.\n\nlibrary(agricolae)\nkruskal(dat_ordinal$score, dat_ordinal$treatment, console = TRUE)\n\n\nStudy: dat_ordinal$score ~ dat_ordinal$treatment\nKruskal-Wallis test's\nTies or no Ties\n\nCritical Value: 52.34422\nDegrees of freedom: 3\nPvalue Chisq  : 2.529543e-11 \n\ndat_ordinal$treatment,  means of the ranks\n\n  dat_ordinal.score  r\nA          52.05000 30\nB          69.58333 30\nC          29.61667 30\nD          90.75000 30\n\nPost Hoc Analysis\n\nt-Student: 1.980626\nAlpha    : 0.05\nMinimum Significant Difference: 13.1991 \n\nTreatments with the same letter are not significantly different.\n\n  dat_ordinal$score groups\nD          90.75000      a\nB          69.58333      b\nA          52.05000      c\nC          29.61667      d\n\n\n\n\n3.3.4 Survival analysis\nA method based on survival analysis was more recently introduced for the analysis of ordinal data (Chiang et al. 2023). The authors developed an R script called “CompMuCens”, which facilitates the comparison of multiple treatment means for plant disease severity data. This is achieved through nonparametric survival analysis using class- or interval-based data derived from a quantitative ordinal scale. The code is accessible in this repository and has been incorporated into the {r4pde} package.\nWe continue with our working example data. Since the expected variable name for the score in the function is x, we must adjust our data frame accordingly.\n\nnames(dat_ordinal) &lt;- c(\"treatment\", \"x\")\n\nThe CompMuCens() function uses the ictest() from the {interval} package to conduct nonparametric survival analysis. Detailed explanation of the function’s input and output can be found here. We just need to set the dat and the scale arguments. The scale will be used to convert the scores in to the defined interval which is used as response variable in the analysis. For example, if the score is 2, the respective limits of the interval will be 3 and 6. For 7, the limits will be 75 and 88. The function takes care of this conversion based on the inputted scale values.\n\nlibrary(interval)\nlibrary(r4pde)\n\nscale &lt;- c(0,3,6,12,25,50,75,88,94,97,100, 100)\nCompMuCens(dat_ordinal, scale)\n\n$U.Score\n  treatment      score\n1         D -15.125000\n2         B  -4.541667\n3         A   4.225000\n4         C  15.441667\n\n$Hypothesis.test\n  treat1 treat2 p-value for H0: treat1 ≤ treat2 p-value for H0: treat1 = treat2\n1      D      B                    0.0018767018                              NA\n2      B      A                    0.0113215174                              NA\n3      A      C                    0.0007456484                              NA\n\n$adj.Signif\n[1] 0.01666667\n\n$Conclusion\n[1] \"D&gt;B&gt;A&gt;C\"\n\n\nThe outcomes are three: the ordered scores for each treatment, the pairwise comparison between treatments and the significance levels, followed by a conclusion section. In this example all treatments differ from each other.\n\n\n3.3.5 Interpretation\nUpon comparing the three methods, it becomes evident that there is a marked distinction in their outcomes. The Kruskal-Wallis and the survival analysis tests suggested a significant difference between treatment A and B, whereas the parametric counterpart did not. However, it’s crucial to note that the P-value is borderline significant at 0.0530, being just slightly above the conventional threshold of 0.05. Given this, it appears that the conversion to the mid-point might have resulted in a type II error, wherein a genuine difference between the treatments might have been missed or overlooked. A detailed comparison between the mid-point and survival analysis has been presented in the paper, which is worth a reading (Chiang et al. 2023)\n\n\n\n\nChester, K. S. 1950. Plant disease losses : Their appraisal and interpretation /. https://doi.org/10.5962/bhl.title.86198.\n\n\nChiang, K.-S., and Bock, C. H. 2021. Understanding the ramifications of quantitative ordinal scales on accuracy of estimates of disease severity and data analysis in plant pathology. Tropical Plant Pathology 47:58–73. https://doi.org/10.1007/s40858-021-00446-0.\n\n\nChiang, K.-S., Chang, Y. M., Liu, H. I., Lee, J. Y., El Jarroudi, M., and Bock, C. 2023. Survival Analysis as a Basis to Test Hypotheses When Using Quantitative Ordinal Scale Disease Severity Data. Phytopathology®. https://doi.org/10.1094/phyto-02-23-0055-r.\n\n\nChiang, K.-S., Liu, S.-C., Bock, C. H., and Gottwald, T. R. 2014. What Interval Characteristics Make a Good Categorical Disease Assessment Scale? Phytopathology® 104:575–585. https://doi.org/10.1094/phyto-10-13-0279-r.\n\n\nHebert, T. T. 1982. The rationale for the horsfall-barratt plant disease assessment scale. Phytopathology 72:1269. https://doi.org/10.1094/phyto-72-1269.\n\n\nLiu, H. I., Tsai, J. R., Chung, W. H., Bock, C. H., and Chiang, K. S. 2019. Effects of Quantitative Ordinal Scale Design on the Accuracy of Estimates of Mean Disease Severity. Agronomy 9:565. https://doi.org/10.3390/agronomy9090565.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. The study of plant disease epidemics. The American Phytopathological Society. https://doi.org/10.1094/9780890545058.\n\n\nNutter, F. W., and Esker, P. D. 2006. The Role of Psychophysics in Phytopathology: The WeberFechner Law Revisited. European Journal of Plant Pathology 114:199–213. https://doi.org/10.1007/s10658-005-4732-9.\n\n\nOnofri, A., Piepho, H.-P., and Kozak, M. 2018. Analysing censored data in agricultural research: A review with examples and software tips. Annals of Applied Biology 174:3–13. https://doi.org/10.1111/aab.12477.\n\n\nScott, P. R., and Hollins, T. W. 1974. Effects of eyespot on the yield of winter wheat. Annals of Applied Biology 78:269–279. https://doi.org/10.1111/j.1744-7348.1974.tb01506.x.\n\n\nShah, D. A., and Madden, L. V. 2004. Nonparametric Analysis of Ordinal Data in Designed Factorial Experiments. Phytopathology® 94:33–43. https://doi.org/10.1094/phyto.2004.94.1.33.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinal scales</span>"
    ]
  },
  {
    "objectID": "data-actual-severity.html",
    "href": "data-actual-severity.html",
    "title": "4  Measuring foliar severity",
    "section": "",
    "text": "4.1 The actual severity measure\nAmong the various methods to express plant disease severity, the percent area affected (or symptomatic) by the disease is one of the most common, especially when dealing with diseases that affect leaves. In order to evaluate whether visual estimates of plant disease severity are sufficiently accurate (as discussed in the previous chapter), we require the actual severity values. These are also essential when creating Standard Area Diagrams (SADs), which are diagrammatic representations of severity values used as a reference either before or during visual assessment to standardize and produce more accurate results across different raters (Del Ponte et al. 2017).\nThe actual severity values are typically approximated using image analysis, wherein the image is segmented, and each pixel is categorized into one of three classes:\nThe ratio of the diseased area to the total area of the unit (e.g., the entire plant organ or section of the image) yields the proportion of the diseased area, or the percent area affected (when multiplied by 100). Researchers have employed various proprietary or open-source software to determine the actual severity, as documented in a review on Standard Area Diagrams (Del Ponte et al. 2017).\nIn this section, we will utilize the measure_disease() function from the {pliman} (Plant IMage ANalysis) R package (Olivoto 2022), and its variations, to measure the percent area affected. The package was compared with other software for determining plant disease severity across five different plant diseases and was shown to produce accurate results in most cases (Olivoto et al. 2022).\nThere are essentially two methods to measure severity. The first is predicated on image palettes that define each class of the image. The second relies on RGB-based indices (Alves et al. 2021). Let’s explore the first method, as well as an interactive approach to setting color palettes.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Measuring foliar severity</span>"
    ]
  },
  {
    "objectID": "data-actual-severity.html#the-actual-severity-measure",
    "href": "data-actual-severity.html#the-actual-severity-measure",
    "title": "4  Measuring foliar severity",
    "section": "",
    "text": "Diseased (or symptomatic)\nNon-diseased (or healthy)\nBackground (the non-plant portion of the image)",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Measuring foliar severity</span>"
    ]
  },
  {
    "objectID": "data-actual-severity.html#image-palettes",
    "href": "data-actual-severity.html#image-palettes",
    "title": "4  Measuring foliar severity",
    "section": "4.2 Image palettes",
    "text": "4.2 Image palettes\nThe most crucial step is the initial one, where the user needs to correctly define the color palettes for each class. In pliman, the palettes can be separate images representing each of the three classes: background (b), symptomatic (s), and healthy (h).\nThe reference image palettes can be constructed by manually sampling small areas of the image and creating a composite image. As expected, the results may vary depending on how these areas are selected. A study that validated pliman for determining disease severity demonstrated the effect of different palettes prepared independently by three researchers, in which the composite palette (combining the three users) was superior (Olivoto et al. 2022). During the calibration of the palettes, examining the processed masks is crucial to create reference palettes that are the most representative of the respective class.\nIn this example, I manually selected and pasted several sections of images representing each class from a few leaves into a Google slide. Once the image palette was ready, I exported each one as a separate PNG image file (JPG also works). These files were named: sbr_b.png, sbr_h.png, and sbr_s.png. They can be found here in this folder for downloading.\n\n\n\n\n\n\nFigure 4.1: Preparation of image palettes by manually sampling fraction of the images that represent background, heatlhy leaf and lesions\n\n\n\nNow that we have the image palettes, we need to import them into the environment, using image_import() function for further analysis. Let’s create an image object for each palette named h (healthy), s (symptoms) and b (background).\n\nlibrary(pliman)\nh &lt;- image_import(\"imgs/sbr_h.png\")\ns &lt;- image_import(\"imgs/sbr_s.png\")\nb &lt;- image_import(\"imgs/sbr_b.png\")\n\nWe can visualize the imported image palettes using the image_combine() function.\n\nimage_combine(h, s, b, ncol =3)\n\n\n\n\n\n\n\nFigure 4.2: Image palettes created to segment images into background, sypomtoms and healthy area of the image\n\n\n\n\n\nAn alternative way to set the palettes is to use the pick_palette() function. It allows to manually pick the colors for each class by clicking on top of the imported image. We can use one of the original images or a composite images with portions of several leaves. Let’s use here one of the original images and pick the background colors and assigned to b vector.\n\nimg &lt;- image_import(\"imgs/originals/img46.png\")\n\nb &lt;- pick_palette(img, viewer = \"mapview\")\n\nThe original image is displayed and the user needs to select the place marker. It is possible to zoom in the image. After placing the markers via clicking on the background colors multiple times, the user should click on “Done”.\n\n\n\nSoybean leaf displaying symptoms of soybean rust\n\n\n\nimage_combine(b)\n\n\n\n\nImage generating after picking the palette colors for the background of the leaf\n\n\nNow, we can proceed and pick the colors for the other categories following the same logic.\n\n# Symptoms\ns &lt;- pick_palette(img, viewer = \"mapview\")\n\n# healthy\nh &lt;- pick_palette(img, viewer = \"mapview\")",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Measuring foliar severity</span>"
    ]
  },
  {
    "objectID": "data-actual-severity.html#measuring-severity",
    "href": "data-actual-severity.html#measuring-severity",
    "title": "4  Measuring foliar severity",
    "section": "4.3 Measuring severity",
    "text": "4.3 Measuring severity\n\n4.3.1 Single image\n\n4.3.1.1 Using color palettes\nTo determine severity in a single image (e.g. img46.png), the image file needs to be loaded and assigned to an object using the same image_import() function used to load the palettes. We can then visualize the image, again using image_combine().\n\n\n\n\n\n\nTip\n\n\n\nThe collection of images used in this chapter can be found here.\n\n\n\nimg &lt;- image_import(\"imgs/originals/img46.png\")\nimage_combine(img)\n\n\n\n\n\n\n\nFigure 4.3: Imported image for further analysis\n\n\n\n\n\nNow the engaging part starts with the measure_disease() function. Four arguments are required when using the reference image palettes: the image representing the target image and the three images of the color palettes. As the author of the package states, “pliman will take care of all the details!” The severity is the value displayed under ‘symptomatic’ in the output.\n\nset.seed(123)\nmeasure_disease(\n  img = img,\n  img_healthy = h,\n  img_symptoms = s,\n  img_background = b\n)\n\n\n\n\n\n\n\n\nIf we want to show the mask with two colors instead of the original, we can set to FALSE two “show_” arguments:\n\nset.seed(123)\nmeasure_disease(\n  img = img,\n  img_healthy = h,\n  img_symptoms = s,\n  img_background = b,\n  show_contour = FALSE,\n  show_original = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 Multiple images\nMeasuring severity in single images is indeed engaging, but we often deal with multiple images, not just one. Using the above procedure to process each image individually would be time-consuming and potentially tedious.\nTo automate the process, {pliman} offers a batch processing approach. Instead of using the img argument, one can use the pattern argument and define the prefix of the image names. Moreover, we also need to specify the directory where the original files are located.\nIf the user wants to save the processed masks, they should set the save_image argument to TRUE and also specify the directory where the images will be saved. Here’s an example of how to process 10 images of soybean rust symptoms. The output is a list object with the measures of the percent healthy and percent symptomatic area for each leaf in the severity object.\n\npliman &lt;- measure_disease(\n  pattern = \"img\",\n  dir_original = \"imgs/originals\" ,\n  dir_processed = \"imgs/processed\",\n  save_image = TRUE,\n  img_healthy = h,\n  img_symptoms = s,\n  img_background = b,\n  verbose = FALSE,\n  plot = FALSE\n)\n\nDone!\n\n\nElapsed time: 00:00:06\n\nseverity &lt;- pliman$severity\nseverity\n\n     img  healthy symptomatic\n1  img11 70.79655  29.2034481\n2  img35 46.94177  53.0582346\n3  img37 60.47440  39.5256013\n4  img38 79.14060  20.8594011\n5  img46 93.14958   6.8504220\n6   img5 20.53175  79.4682534\n7  img63 97.15669   2.8433141\n8  img67 99.83720   0.1627959\n9  img70 35.56684  64.4331583\n10 img75 93.04453   6.9554686\n\n\nWhen the argument save_image is set to TRUE, the images are all saved in the folder with the standard prefix “proc.”\n\n\n\n\n\n\nFigure 4.4: Images created by pliman and exported to a specific folder\n\n\n\nLet’s have a look at one of the processed images.\n\n\n\n\n\n\nFigure 4.5: Figure created by pliman after batch processing to segment the images and calculate percent area covered by symptoms. The symptomatic area is delinated in the image.\n\n\n\n\n4.3.2.1 More than a target per image\n{pliman} offers a custom function to estimate the severity in multiple targets (e.g. leaf) per image. This is convenient to decrease the time when scanning the specimens, for example. Let’s combine three soybean rust leaves into a single image and import it for processing. We will further set the index_lb (leaf background), save_image to TRUE and inform the directory for the processed images using dir_processed.\n\nimg2 &lt;- image_import(\"imgs/soybean_three.png\")\nimage_combine(img2)\n\n\n\n\n\n\n\nFigure 4.6: Imported image with multiple targets in a single image for further analysis using measure_disease_byl() function of the {pliman} package\n\n\n\n\n\n\n pliman2 &lt;- measure_disease_byl(img = img2,\n                        index_lb = b,\n                        img_healthy = h,\n                        img_symptoms = s, \n                        save_image = TRUE,\n                        dir_processed = \"imgs/proc\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n pliman2$severity\n\n  img leaf  healthy symptomatic\n1 img    1 59.11737   40.882632\n2 img    2 61.36616   38.633841\n3 img    3 93.21756    6.782436\n\n\nThe original image is splited and the individual new images are saved in the proc folder.\n\n\n\nIndividual images of the soybean leaves after processed using the measure_disease_byl function of the {pliman} package.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Measuring foliar severity</span>"
    ]
  },
  {
    "objectID": "data-actual-severity.html#how-good-are-these-measurements",
    "href": "data-actual-severity.html#how-good-are-these-measurements",
    "title": "4  Measuring foliar severity",
    "section": "4.4 How good are these measurements?",
    "text": "4.4 How good are these measurements?\nThese 10 images were previously processed in QUANT software for measuring severity which is also based on image threshold. Let’s create a tibble for the image code and respective “actual” severity - assuming QUANT measures as reference.\n\nlibrary(tidyverse)\nlibrary(r4pde)\nquant &lt;- tribble(\n  ~img, ~actual,\n   \"img5\",     75,\n  \"img11\",     24,\n  \"img35\",     52,\n  \"img37\",     38,\n  \"img38\",     17,\n  \"img46\",      7,\n  \"img63\",    2.5,\n  \"img67\",   0.25,\n  \"img70\",     67,\n  \"img75\",     10\n  )\n\nWe can now combine the two dataframes and produce a scatter plot relating the two measures.\n\ndat &lt;- left_join(severity, quant)\n\nJoining with `by = join_by(img)`\n\ndat %&gt;%\n  ggplot(aes(actual, symptomatic)) +\n  geom_point(size = 3, shape = 16) +\n  ylim(0, 100) +\n  xlim(0, 100) +\n  geom_abline(slope = 1, intercept = 0) +\n  labs(x = \"Quant\",\n       y = \"pliman\")+\n  theme_r4pde()\n\n\n\n\n\n\n\nFigure 4.7: Scatter plot for the relationship between severity values measured by pliman and Quant software\n\n\n\n\n\nThe concordance correlation coefficient is a test for agreement between two observers or two methods (see previous chapter). It is an indication of how accurate the pliman measures are compared with a standard. The coefficient is greater than 0.99 (1.0 is perfect concordance), suggesting an excellent agreement!\n\nlibrary(epiR)\nccc &lt;- epi.ccc(dat$actual, dat$symptomatic)\nccc$rho.c\n\n        est     lower     upper\n1 0.9940835 0.9774587 0.9984566\n\n\nIn conclusion, as mentioned earlier, the most critical step is defining the reference image palettes. A few preliminary runs may be necessary for some images to ensure that the segmentation is being carried out correctly, based on visual judgment. This is not different from any other color-threshold based methods, where the choices made by the user impact the final result and contribute to variation among assessors. The drawbacks are the same as those encountered with direct competitors, namely, the need for images to be taken under uniform and controlled conditions, especially with a contrasting background.\n\n\n\n\nAlves, K. S., Guimarães, M., Ascari, J. P., Queiroz, M. F., Alfenas, R. F., Mizubuti, E. S. G., and Del Ponte, E. M. 2021. RGB-based phenotyping of foliar disease severity under controlled conditions. Tropical Plant Pathology 47:105–117. https://doi.org/10.1007/s40858-021-00448-y.\n\n\nDel Ponte, E. M., Pethybridge, S. J., Bock, C. H., Michereff, S. J., Machado, F. J., and Spolti, P. 2017. Standard Area Diagrams for Aiding Severity Estimation: Scientometrics, Pathosystems, and Methodological Trends in the Last 25 Years. Phytopathology® 107:1161–1174. https://doi.org/10.1094/phyto-02-17-0069-fi.\n\n\nOlivoto, T. 2022. Lights, camera, pliman! An R package for plant image analysis. Methods in Ecology and Evolution 13:789–798. https://doi.org/10.1111/2041-210x.13803.\n\n\nOlivoto, T., Andrade, S. M. P., and M. Del Ponte, E. 2022. Measuring plant disease severity in R: introducing and evaluating the pliman package. Tropical Plant Pathology 47:95–104. https://doi.org/10.1007/s40858-021-00487-5.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Measuring foliar severity</span>"
    ]
  },
  {
    "objectID": "data-accuracy.html",
    "href": "data-accuracy.html",
    "title": "5  Reliability and accuracy",
    "section": "",
    "text": "6 Severity data",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and accuracy</span>"
    ]
  },
  {
    "objectID": "data-accuracy.html#terminology",
    "href": "data-accuracy.html#terminology",
    "title": "5  Reliability and accuracy",
    "section": "6.1 Terminology",
    "text": "6.1 Terminology\nDisease severity, mainly when expressed in percent area diseased assessed visually, is acknowledged as a more difficult and less time- and cost-effective plant disease variable to obtain. However, errors may occur even when assessing a more objective measure such as incidence. This is the case when an incorrect assignment or confusion of symptoms occur. In either case, the quality of the assessment of any disease variable is very important and should be gauged in the studies. Several terms can be used when evaluating the quality of disease assessments, including reliability, precision, accuracy or agreement.\nReliability: The extent to which the same estimates or measurements of diseased specimens obtained under different conditions yield similar results. There are two types. The inter-rater reliability (or reproducibility) is a measure of consistency of disease assessment across the same specimens between raters or devices. The intra-rater reliability (or repeatability) measures consistency by the same rater or instrument on the same specimens (e.g. two assessments in time by the same rater).\n\n\n\n\n\n\nFigure 6.1: Two types of reliability of estimates or measures of plant disease intensity\n\n\n\nPrecision: A statistical term to express the measure of variability of the estimates or measurements of disease on the same specimens obtained by different raters (or instruments). However, reliable or precise estimates (or measurements) are not necessarily close to an actual value, but precision is a component of accuracy or agreement.\nAccuracy or agreement: These two terms can be treated as synonymous in plant pathological research. They refer to the closeness (or concordance) of an estimate or measurement to the actual severity value for a specimen on the same scale. Actual values may be obtained using various methods, against which estimates or measurements using an experimental assessment method are compared.\nAn analogy commonly used to explain accuracy and precision is the archer shooting arrows at a target and trying to hit the bull’s eye (center of the target) with each of five arrows. The figure below is used to demonstrate four situations from the combination of two levels (high and low) for precision and accuracy. The figure was produced using the ggplot function of ggplot2 package.\n\n\nCode\nlibrary(ggplot2)\ntarget &lt;- \n  ggplot(data.frame(c(1:10),c(1:10)))+\n  geom_point(aes(x = 5, y = 5), size = 71.5, color = \"black\")+\n  geom_point(aes(x = 5, y = 5), size = 70, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 60, color = \"white\")+\n  geom_point(aes(x = 5, y = 5), size = 50, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 40, color = \"white\")+\n  geom_point(aes(x = 5, y = 5), size = 30, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 20, color = \"white\")+\n  geom_point(aes(x = 5, y = 5), size = 10, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 4, color = \"white\")+\n  ylim(0,10)+\n  xlim(0,10)+\n  theme_void()\n\nhahp &lt;- target +\n  labs(subtitle = \"High Accuracy High Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 5, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5, y = 5.2), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5, y = 4.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.8, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.2, y = 5), shape = 4, size =2, color = \"blue\")\n\n\nlahp &lt;- target +\n  labs(subtitle = \"Low Accuracy High Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 6, y = 6), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 6, y = 6.2), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 6, y = 5.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.8, y = 6), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 6.2, y = 6), shape = 4, size =2, color = \"blue\")\n\n\nhalp &lt;- target +\n  labs(subtitle = \"High Accuracy Low Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 5, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5, y = 5.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.8, y = 4.4), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.4, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.6, y = 5.6), shape = 4, size =2, color = \"blue\")\n\nlalp &lt;- target +\n  labs(subtitle = \"Low Accuracy Low Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 5.5, y = 5.5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.5, y = 5.4), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.2, y = 6.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.8, y = 3.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.2, y = 3), shape = 4, size =2, color = \"blue\")\n\n\nlibrary(patchwork)\n(hahp | lahp) /\n(halp | lalp)\n\n\n\n\n\n\n\n\nFigure 6.2: The accuracy and precision of the archer is determined by the location of the group of arrows\n\n\n\n\n\nAnother way to visualize accuracy and precision is via scatter plots for the relationship between the actual values and the estimates.\n\n\nCode\nlibrary(tidyverse)\nlibrary(r4pde)\ntheme_set(theme_r4pde())\ndat &lt;- \ntibble::tribble(\n  ~actual,   ~ap,   ~ip,   ~ai,   ~ii,\n        0,     0,    10,     0,    25,\n       10,    10,    20,     5,    10,\n       20,    20,    30,    30,    10,\n       30,    30,    40,    30,    45,\n       40,    40,    50,    30,    35,\n       50,    50,    60,    60,    65,\n       60,    60,    70,    50,    30\n  )\n\nap &lt;- dat |&gt; \n  ggplot(aes(actual, ap))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n    geom_smooth(method = \"lm\")+\n   geom_point(color = \"#99cc66\", size = 3)+\n   ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"High Acccuracy High Precision\")\n\nip &lt;- dat |&gt; \n  ggplot(aes(actual, ip))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n  geom_smooth(method = \"lm\", se = F)+\n  geom_point(color = \"#99cc66\", size = 3)+\n  ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"Low Acccuracy High Precision\")\n\nai &lt;- dat |&gt; \n  ggplot(aes(actual, ai))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n  geom_smooth(method = \"lm\", se = F)+\n  geom_point(color = \"#99cc66\", size = 3)+\n  ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"High Acccuracy Low precision\")\n\nii &lt;- dat |&gt; \n  ggplot(aes(actual, ii))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n  geom_smooth(method = \"lm\", se = F)+\n  geom_point(color = \"#99cc66\", size = 3)+\n  ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"Low Acccuracy Low Precision\")\n\nlibrary(patchwork)\n(ap | ip) / (ai | ii)\n\n\n\n\n\n\n\n\nFigure 6.3: Scatter plots for the relationship between actual and estimated values representing situations of low or high precision and accuracy. The dashed line indicates the perfect concordance and the solid blue line represents the fit of the linear regression model",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and accuracy</span>"
    ]
  },
  {
    "objectID": "data-accuracy.html#statistical-summaries",
    "href": "data-accuracy.html#statistical-summaries",
    "title": "5  Reliability and accuracy",
    "section": "6.2 Statistical summaries",
    "text": "6.2 Statistical summaries\nA formal assessment of the quality of estimates or measures is made using statistical summaries of the data expressed as indices that represent reliability, precision and accuracy. These indices can further be used to test hypothesis such as if one or another method is superior than the other. The indices or the tests vary according to the nature of the variable, whether continuous, binary or categorical.\n\n6.2.1 Inter-rater reliability\nTo calculate measures of inter-rater reliability (or reproducibility) we will work with a fraction of a larger dataset used in a published study. There, the authors tested the effect of standard area diagrams (SADs) on the reliability and accuracy of visual estimates of severity of soybean rust.\nThe selected dataset consists of five columns with 20 rows. The first is the leaf number and the others correspond to assessments of percent soybean rust severity by four raters (R1 to R4). Each row correspond to one symptomatic leaf. Let’s assign the tibble to a dataframe called sbr (an acronym for soybean rust). Note that the variable is continuous.\n\nlibrary(tidyverse)\nsbr &lt;- tribble(\n~leaf, ~R1, ~R2,  ~R3, ~R4,\n1, 0.6, 0.6,  0.7, 0.6,\n2,   2, 0.7,    5,   1,\n3,   5,   5,    8,   5,\n4,   2,   4,    6,   2,\n5,   6,  14,   10,   7,\n6,   5,   6,   10,   5,\n7,  10,  18, 12.5,  12,\n8,  15,  30,   22,  10,\n9,   7,   2,   12,   8,\n10,  6,   9, 11.5,   8,\n11,  7,   7,   20,   9,\n12,  6,  23,   22,  14,\n13, 10,  35, 18.5,  20,\n14, 19,  10,    9,  10,\n15, 15,  20,   19,  20,\n16, 17,  30,   18,  13,\n17, 19,  53,   33,  38,\n18, 17, 6.8,   15,   9,\n19, 15,  20,   18,  16,\n20, 18,  22,   24,  15\n         )\n\nLet’s explore the data using various approaches. First, we can visualize how the individual estimates by the raters differ for a same leaf.\n\n# transform from wide to long format\nsbr2 &lt;- sbr |&gt; \n  pivot_longer(2:5, names_to = \"rater\",\n               values_to = \"estimate\") \n\n# create the plot\nsbr2 |&gt; \n  ggplot(aes(leaf, estimate, color = rater,\n             group = leaf))+\n  geom_line(color = \"black\")+\n  geom_point(size = 2)+\n  labs(y = \"Severity estimate (%)\",\n       x = \"Leaf number\",\n       color = \"Rater\")\n\n\n\n\n\n\n\nFigure 6.4: Visual estimates of soybean rust severity for each leaf by each of four raters\n\n\n\n\n\nAnother interesting visualization is the correlation matrix of the estimates between all possible pair of raters. The ggpairs function of the GGally package is handy for this task.\n\nlibrary(GGally)\n\n\n# create a new dataframe with only raters\nraters &lt;- sbr |&gt; \n  select(2:5)\n\nggpairs(raters)+\n  theme_r4pde()\n\n\n\n\n\n\n\nFigure 6.5: Correlation plots relating severity estimates for all pairs of raters\n\n\n\n\n\n\n6.2.1.1 Coefficient of determination\nWe noticed earlier that the correlation coefficients varied across all pairs of rater. Sometimes, the means of squared Pearson’s R values (R2), or the coefficient of determination is used as a measure of inter-rater reliability. We can further examine the pair-wise correlations in more details using the cor function,\n\nknitr::kable(cor(raters))\n\n\n\nTable 6.1: Pearson correlation coefficients for all pairs of raters\n\n\n\n\n\n\n\nR1\nR2\nR3\nR4\n\n\n\n\nR1\n1.0000000\n0.6325037\n0.6825936\n0.6756986\n\n\nR2\n0.6325037\n1.0000000\n0.8413333\n0.8922181\n\n\nR3\n0.6825936\n0.8413333\n1.0000000\n0.8615470\n\n\nR4\n0.6756986\n0.8922181\n0.8615470\n1.0000000\n\n\n\n\n\n\n\n\nThe means of coefficient of determination can be easily obtained as follows.\n\n# All pairwise R2\n\nraters_cor &lt;- reshape2::melt(cor(raters))\n\nraters2 &lt;- raters_cor |&gt; \n  filter(value != 1) \n\n# means of R2\nraters2$value\n\n [1] 0.6325037 0.6825936 0.6756986 0.6325037 0.8413333 0.8922181 0.6825936\n [8] 0.8413333 0.8615470 0.6756986 0.8922181 0.8615470\n\nround(mean(raters2$value^2), 3)\n\n[1] 0.595\n\n\n\n\n6.2.1.2 Intraclass Correlation Coefficient\nA common statistic to report in reliability studies is the Intraclass Correlation Coefficient (ICC). There are several formulations for the ICC whose choice depend on the particular experimental design. Following the convention of the seminal work by Shrout and Fleiss (1979), there are three main ICCs:\n\nOne-way random effects model, ICC(1,1): in our context, each leaf is rated by different raters who are considered as sampled from a larger pool of raters (random effects)\nTwo-way random effects model, ICC(2,1): both raters and leaves are viewed as random effects\nTwo-way mixed model, ICC(3,1): raters are considered as fixed effects and leaves are considered as random.\n\nAdditionally, the ICC may depend on whether the ratings are an average or not of several ratings. When an average is considered, these are called ICC(1,k), ICC(2,k) and ICC(3,k).\nThe ICC can be computed using the ICC() or the icc() functions of the psych or irr packages, respectively. They both provide the coefficient, F value, and the upper and lower bounds of the 95% confidence interval.\n\nlibrary(psych)\nic &lt;- ICC(raters)\nknitr::kable(ic$results[1:2]) # only selected columns\n\n\n\n\n\ntype\nICC\n\n\n\n\nSingle_raters_absolute\nICC1\n0.6405024\n\n\nSingle_random_raters\nICC2\n0.6464122\n\n\nSingle_fixed_raters\nICC3\n0.6919099\n\n\nAverage_raters_absolute\nICC1k\n0.8769479\n\n\nAverage_random_raters\nICC2k\n0.8797008\n\n\nAverage_fixed_raters\nICC3k\n0.8998319\n\n\n\n\n# call ic list for full results\n\nThe output of interest is a dataframe with the results of all distinct ICCs. We note that the ICC1 and ICC2 gave very close results. Now, let’s obtain the various ICCs using the irr package. Differently from the the ICC() function, this one requires further specification of the model to use.\n\nlibrary(irr)\nicc(raters, \"oneway\")\n\n Single Score Intraclass Correlation\n\n   Model: oneway \n   Type : consistency \n\n   Subjects = 20 \n     Raters = 4 \n     ICC(1) = 0.641\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n   F(19,60) = 8.13 , p = 1.8e-10 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.44 &lt; ICC &lt; 0.813\n\n# The one used in the SBR paper\nicc(raters, \"twoway\")\n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : consistency \n\n   Subjects = 20 \n     Raters = 4 \n   ICC(C,1) = 0.692\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n   F(19,57) = 9.98 , p = 6.08e-12 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.503 &lt; ICC &lt; 0.845\n\n\n\n\n6.2.1.3 Overall Concordance Correlation Coefficient\nAnother useful index is the Overall Concordance Correlation Coefficient (OCCC) for evaluating agreement among multiple observers. It was proposed by Barnhart et al. (2002) based on the original index proposed by Lin (1989), earlier defined in the context of two fixed observers. In the paper, the authors introduced the OCCC in terms of the interobserver variability for assessing agreement among multiple fixed observers. As outcome, and similar to the original CCC, the approach addresses the precision and accuracy indices as components of the OCCC. The epi.occc function of the epiR packge does the job but it does compute a confidence interval.\n\nlibrary(epiR)\nepi.occc(raters, na.rm = FALSE, pairs = TRUE)\n\n\nOverall CCC           0.6372\nOverall precision     0.7843\nOverall accuracy      0.8125\n\n\n\n\n\n6.2.2 Intrarater reliability\nAs defined, the intrarater reliability is also known as repeatability, because it measures consistency by the same rater at repeated assessments (e.g. different times) on the same sample. In some studies, we may be interested in testing whether a new method increases repeatability of assessments by a single rater compared with another one. The same indices used for assessing reproducibility (interrater) can be used to assess repeatability, and these are reported at the rater level.\n\n\n6.2.3 Precision\nWhen assessing precision, one measures the variability of the estimates (or measurements) of disease on the same sampling units obtained by different raters (or instruments). A very high precision does not mean that the estimates are closer to the actual value (which is given by measures of bias). However, precision is a component of overall accuracy, or agreement. It is given by the Pearson’s correlation coefficient.\nDifferent from reliability, that requires only the estimates or measures by the raters, now we need a reference (gold standard) value to compare the estimates to. These can be an accurate rater or measures by an instrument. Let’s get back to the soybean rust severity estimation dataset and add a column for the (assumed) actual values of severity on each leaf. In that work, the actual severity values were obtained using image analysis.\n\nsbr &lt;- tibble::tribble(\n~leaf, ~actual, ~R1, ~R2,  ~R3, ~R4,\n1,    0.25, 0.6, 0.6,  0.7, 0.6,\n2,     2.5,   2, 0.7,    5,   1,\n3,    7.24,   5,   5,    8,   5,\n4,    7.31,   2,   4,    6,   2,\n5,    9.07,   6,  14,   10,   7,\n6,    11.6,   5,   6,   10,   5,\n7,   12.46,  10,  18, 12.5,  12,\n8,    13.1,  15,  30,   22,  10,\n9,   14.61,   7,   2,   12,   8,\n10,  16.06,   6,   9, 11.5,   8,\n11,   16.7,   7,   7,   20,   9,\n12,   19.5,   6,  23,   22,  14,\n13,  20.75,  10,  35, 18.5,  20,\n14,  23.56,  19,  10,    9,  10,\n15,  23.77,  15,  20,   19,  20,\n16,  24.45,  17,  30,   18,  13,\n17,  25.78,  19,  53,   33,  38,\n18,  26.03,  17, 6.8,   15,   9,\n19,  26.42,  15,  20,   18,  16,\n20,  28.89,  18,  22,   24,  15\n         )\n\nWe can explore visually via scatter plots the relationships between the actual value and the estimates by each rater (Figure 6.6). To facilitate, we need the data in the long format.\n\nsbr2 &lt;- sbr |&gt; \n  pivot_longer(3:6, names_to = \"rater\",\n               values_to = \"estimate\") \n\nsbr2 |&gt; \n  ggplot(aes(actual, estimate))+\n  geom_point(size = 2, alpha = 0.7)+\n  facet_wrap(~rater)+\n  ylim(0,45)+\n  xlim(0,45)+\n  geom_abline(intercept = 0, slope =1)+\n  theme_r4pde()+\n  labs(x = \"Actual severity (%)\",\n       y = \"Estimate severity (%)\")\n\n\n\n\n\n\n\nFigure 6.6: Scatterplots for the relationship between estimated and actual severity for each rater\n\n\n\n\n\nThe Pearson’s r for the relationship, or the precision of the estimates by each rater, can be obtained using the correlation function of the correlation package.\n\nprecision &lt;- sbr2 |&gt; \n  select(-leaf) |&gt; \n  group_by(rater) |&gt; \n  correlation::correlation() \n\nknitr::kable(precision[1:4])\n\n\n\n\nGroup\nParameter1\nParameter2\nr\n\n\n\n\nR1\nactual\nestimate\n0.8725643\n\n\nR2\nactual\nestimate\n0.5845291\n\n\nR3\nactual\nestimate\n0.7531983\n\n\nR4\nactual\nestimate\n0.7108260\n\n\n\n\n\nThe mean precision can then be obtained.\n\nmean(precision$r)\n\n[1] 0.7302795\n\n\n\n\n6.2.4 Accuracy\n\n6.2.4.1 Absolute errors\nIt is useful to visualize the errors of the estimates which are obtained by subtracting the estimates from the actual severity values. This plot allows to visualize patterns in over or underestimations across a range of actual severity values.\n\nsbr2 |&gt; \n  ggplot(aes(actual, estimate-actual))+\n  geom_point(size = 3, alpha = 0.7)+\n  facet_wrap(~rater)+\n  geom_hline(yintercept = 0)+\n  theme_r4pde()+\n  labs(x = \"Actual severity (%)\",\n       y = \"Error (Estimate - Actual)\")\n\n\n\n\n\n\n\nFigure 6.7: Error (estimated - actual) of visual severity estimates\n\n\n\n\n\n\n\n6.2.4.2 Concordance correlation coefficient\nLin’s (1989, 2000) proposed the concordance correlation coefficient (CCC) for agreement on a continuous measure obtained by two methods. The CCC combines measures of both precision and accuracy to determine how far the observed data deviate from the line of perfect concordance. Lin’s CCC increases in value as a function of the nearness of the data reduced major axis to the line of perfect concordance (the accuracy of the data) and of the tightness of the data about its reduced major axis (the precision of the data).\nThe epi.ccc function of the epiR package allows to obtain the Lin’s CCC statistics. Let’s filter only rater 2 and calculate the CCC statistics for this rater.\n\nlibrary(epiR)\n# Only rater 2\nsbr3 &lt;- sbr2 |&gt; filter(rater == \"R2\")\nccc &lt;- epi.ccc(sbr3$actual, sbr3$estimate)\n# Concordance coefficient\nrho &lt;- ccc$rho.c[,1]\n# Bias coefficient\nCb &lt;- ccc$C.b\n# Precision\nr &lt;- ccc$C.b*ccc$rho.c[,1]\n# Scale-shift\nss &lt;- ccc$s.shift\n# Location-shift\nls &lt;- ccc$l.shift\nMetrics &lt;- c(\"Agreement\", \"Bias coefficient\", \"Precision\", \"scale-shift\", \"location-shift\")\nValue &lt;- c(rho, Cb, r, ss, ls)\nres &lt;- data.frame(Metrics, Value)\nknitr::kable(res)\n\n\n\nTable 6.2: Statitics of the concordance correlation coefficient summarizing accuracy and precision of visual severity estimates of soybean rust for a single rater\n\n\n\n\n\n\nMetrics\nValue\n\n\n\n\nAgreement\n0.5230656\n\n\nBias coefficient\n0.8948494\n\n\nPrecision\n0.4680649\n\n\nscale-shift\n1.6091178\n\n\nlocation-shift\n-0.0666069\n\n\n\n\n\n\n\n\nNow let’s create a function that will allow us to estimate the CCC for all raters in the data frame in the wide format. The function assumes that the first two columns are the actual and estimates and the rest of the columns are the raters, which is the case for our sbr dataframe . Let’s name this function ccc_byrater.\n\nccc_byrater &lt;- function(data) {\n  long_data &lt;- pivot_longer(data, cols = -c(leaf, actual),\n                            names_to = \"rater\", values_to = \"measurement\")\n  ccc_results &lt;- long_data %&gt;%\n    group_by(rater) %&gt;%\n    summarise(Agreement = as.numeric(epi.ccc(measurement, actual)$rho.c[1]),\n              `Bias coefficient` = epi.ccc(measurement, actual)$C.b,\n              Precision = Agreement * `Bias coefficient`,\n              scale_shift = epi.ccc(measurement, actual)$s.shift,\n              location_shift = epi.ccc(measurement, actual)$l.shift)\n  \n  return(ccc_results)\n}\n\nThen, we use the ccc_byrater function with the original sbr dataset - or any other dataset in the wide format of similar structure. The output is a dataframe with all CCC statistics.\n\nresults &lt;- ccc_byrater(sbr)\nknitr::kable(results)\n\n\n\n\n\n\n\n\n\n\n\n\nrater\nAgreement\nBias coefficient\nPrecision\nscale_shift\nlocation_shift\n\n\n\n\nR1\n0.5968136\n0.6839766\n0.4082065\n1.3652694\n0.9090386\n\n\nR2\n0.5230656\n0.8948494\n0.4680649\n0.6214585\n0.0666069\n\n\nR3\n0.7306948\n0.9701226\n0.7088635\n1.1028813\n0.2280303\n\n\nR4\n0.5861371\n0.8245860\n0.4833205\n1.0044929\n0.6522573",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and accuracy</span>"
    ]
  },
  {
    "objectID": "data-accuracy.html#accuracy-1",
    "href": "data-accuracy.html#accuracy-1",
    "title": "5  Reliability and accuracy",
    "section": "7.1 Accuracy",
    "text": "7.1 Accuracy\nIncidence data are binary at the individual level; an individual is diseased or not. Here, different from severity that is estimated, the specimen is classified. Let’s create two series of binary data, each being a hypothetical scenario of assignment of 12 plant specimens into two classes: healthy (0) or diseased (1).\n\norder &lt;- c(1:12)\nactual &lt;- c(1,1,1,1,1,1,1,1,0,0,0,0)\nclass &lt;- c(0,0,1,1,1,1,1,1,1,0,0,0)\n\ndat_inc &lt;- data.frame(order, actual, class)\ndat_inc \n\n   order actual class\n1      1      1     0\n2      2      1     0\n3      3      1     1\n4      4      1     1\n5      5      1     1\n6      6      1     1\n7      7      1     1\n8      8      1     1\n9      9      0     1\n10    10      0     0\n11    11      0     0\n12    12      0     0\n\n\nIn the example above, the rater makes 9 accurate classification and misses 3: 2 diseased plants classified as being disease-free (sample 1 and 2), and 1 healthy plant that is wrongly classified as diseased (sample 9).\nNotice that there are four outcomes:\nTP = true positive, a positive sample correctly classified\nTN = true negative, a negative sample correctly classified\nFP = false positive, a negative sample classified as positive\nFN = false negative, a positive sample classified as positive.\nThere are several metrics that can be calculated with the help of a confusion matrix, also known as error matrix. Considering the above outcomes, here is a how a confusion matrix looks like.\nSuppose a 2x2 table with notation\n\n\n\n\nActual value\n\n\n\n\n\nClassification value\nDiseased\nHealthy\n\n\nDiseased\nTP\nFP\n\n\nHealthy\nFN\nTN\n\n\n\nLet’s create this matrix using a function of the caret package.\n\nlibrary(caret)\nattach(dat_inc)\ncm &lt;- confusionMatrix(factor(class), reference = factor(actual))\ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 0 1\n         0 3 2\n         1 1 6\n                                          \n               Accuracy : 0.75            \n                 95% CI : (0.4281, 0.9451)\n    No Information Rate : 0.6667          \n    P-Value [Acc &gt; NIR] : 0.3931          \n                                          \n                  Kappa : 0.4706          \n                                          \n Mcnemar's Test P-Value : 1.0000          \n                                          \n            Sensitivity : 0.7500          \n            Specificity : 0.7500          \n         Pos Pred Value : 0.6000          \n         Neg Pred Value : 0.8571          \n             Prevalence : 0.3333          \n         Detection Rate : 0.2500          \n   Detection Prevalence : 0.4167          \n      Balanced Accuracy : 0.7500          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nThe function returns the confusion matrix and several statistics such as accuracy = (TP + TN) / (TP + TN + FP + FN). Let’s manually calculate the accuracy and compare the results:\n\nTP = 3\nFP = 2\nFN = 1\nTN = 6\naccuracy = (TP+TN)/(TP+TN+FP+FN)\naccuracy\n\n[1] 0.75\n\n\nTwo other important metrics are sensitivity and specificity.\n\nsensitivity = TP/(TP+FN)\nsensitivity\n\n[1] 0.75\n\nspecificity = TN/(FP+TN)\nspecificity\n\n[1] 0.75\n\n\nWe can calculate some metrics using the MixtureMissing package.\n\nlibrary(MixtureMissing)\nevaluation_metrics(actual, class)\n\n$matr\n       pred_0 pred_1\ntrue_0      3      1\ntrue_1      2      6\n\n$TN\n[1] 3\n\n$FP\n[1] 1\n\n$FN\n[1] 2\n\n$TP\n[1] 6\n\n$TPR\n[1] 0.75\n\n$FPR\n[1] 0.25\n\n$TNR\n[1] 0.75\n\n$FNR\n[1] 0.25\n\n$precision\n[1] 0.8571429\n\n$accuracy\n[1] 0.75\n\n$error_rate\n[1] 0.25\n\n$FDR\n[1] 0.1428571",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and accuracy</span>"
    ]
  },
  {
    "objectID": "data-accuracy.html#reliability",
    "href": "data-accuracy.html#reliability",
    "title": "5  Reliability and accuracy",
    "section": "7.2 Reliability",
    "text": "7.2 Reliability\n\nlibrary(psych)\ntab &lt;- table(class, actual)\nphi(tab)\n\n[1] 0.48\n\n\n\n\n\n\nBarnhart, H. X., Haber, M., and Song, J. 2002. Overall Concordance Correlation Coefficient for Evaluating Agreement Among Multiple Observers. Biometrics 58:1020–1027. https://doi.org/10.1111/j.0006-341x.2002.01020.x.\n\n\nLin, L. I.-K. 1989. A concordance correlation coefficient to evaluate reproducibility. Biometrics 45:255. https://doi.org/10.2307/2532051.\n\n\nShrout, P. E., and Fleiss, J. L. 1979. Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin 86:420–428. https://doi.org/10.1037/0033-2909.86.2.420.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reliability and accuracy</span>"
    ]
  },
  {
    "objectID": "data-sads.html",
    "href": "data-sads.html",
    "title": "6  Standard area diagrams",
    "section": "",
    "text": "6.1 Definitions\nAccording to a glossary on phytopathometry (Bock et al. 2021), standard area diagram (SAD) can be defined as “a generic term for a pictorial or graphic representation (drawing or true-color photo) of selected disease severities on plants or plant parts (leaves, fruit, flowers, etc.) generally used as an aid for more accurate visual estimation (on the percentage scale) or classification (using an ordinal scale) of severity on a specimen”.\nThe Standard Area Diagrams (SADs), also known as diagrammatic scales, have a long history of use in plant pathology. The concept dates back to the late 1800s when the Cobb scale was developed, featuring five diagrams depicting a range of severity levels of rust pustules on wheat leaves.\nIn the past 20 years, plant pathologists have leveraged advancements in image processing and analysis tools, along with insights from psychophysical and measurement sciences, to develop SADs that are realistic (e.g., true-color photographs), validated, and depict severities that maximize estimation accuracy. SADs have been created in various color formats (black or white, two-color, or true-color) and with varying incremental scales (approximated linear or logarithmic) (Del Ponte et al. 2017).\nSADs have proven beneficial in increasing the accuracy of visual estimates, as estimating percentage areas is generally more challenging than classifying severity into ordinal classes - there are numerous possibilities on the percentage scale, compared to the finite and small number of classes in ordinal scales. A recent quantitative review confirmed that using SADs often results in improved accuracy and precision of visual estimates. However, it also identified factors related to SAD design and structure, disease symptoms, and actual severity that affected the outcomes. In particular, SADs have shown greater utility for raters who are inherently less accurate and for diseases characterized by small and numerous lesions (Del Ponte et al. 2022). Here are examples of SADs in black and white, two-color, and true-color formats:\nMore SADs can be found in the SADBank, a curated collection of articles on SAD development and validation. Click on the image below to get access to the database.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standard area diagrams</span>"
    ]
  },
  {
    "objectID": "data-sads.html#definitions",
    "href": "data-sads.html#definitions",
    "title": "6  Standard area diagrams",
    "section": "",
    "text": "Figure 6.1: Actual photos of symptoms of loquat scab on fruit (left) and a SADs with eight diagrams (right). Each number represents severity as the percent area affected (González-Domínguez et al. 2014)\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: SADs for Glomerella leaf spot on apple leaf. Each number represents severity as the percent area affected (Moreira et al. 2018)\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: SADs for soybean rust. Each number represents severity as the percent area affected (Franceschi et al. 2020)\n\n\n\n\n\n\n\n\n\n\nFigure 6.4: SADBank, a curated collection of articles",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standard area diagrams</span>"
    ]
  },
  {
    "objectID": "data-sads.html#sad-development-and-validation",
    "href": "data-sads.html#sad-development-and-validation",
    "title": "6  Standard area diagrams",
    "section": "6.2 SAD development and validation",
    "text": "6.2 SAD development and validation\nA systematic review of the literature on SADs highlighted the most important aspects related with the development and validation of the tool (Del Ponte et al. 2017). A list of best practices was proposed in the review to guide future research in the area. Follows the most important aspects to be noted:\n\n\n\n\n\n\nBest practices on SADs development\n\n\n\n\nSample a minimum number (e.g., n = 100) of specimens from natural epidemics representing the range of disease severity and typical symptoms observed.\nUse reliable image analysis software to discriminate disease symptoms from healthy areas to calculate percent area affected.\nWhen designing the illustrations for the SAD set, ensure that the individual diagrams are prepared realistically, whether line drawn, actual photos, or computer generated.\nThe number of diagrams should be no less than 6 and no more than 10, distributed approximately linearly, and spaced no more than 15% apart. Additional diagrams (±2) should be included between 0 and 10% severity.\nFor the validation trial, select at least 50 specimens representing the full range of actual severity and symptom patterns.\nWhen selecting raters (a minimum of 15) for validation, make sure they do not have previous experience in using the SAD under evaluation.\nProvide standard instructions on how to recognize the symptoms of the disease and how to assess severity, first without and then with the SAD.\nIdeally repeat the assessment in time, with a 1- or 2-week interval, both without and with the aid, using the same set of raters in order to evaluate the effect of training and experience on gains in accuracy.\nBoth pre- and posttest experiment conditions should be the same to avoid any impact of distraction on accuracy of estimates during the tests.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standard area diagrams</span>"
    ]
  },
  {
    "objectID": "data-sads.html#designing-sads-in-r",
    "href": "data-sads.html#designing-sads-in-r",
    "title": "6  Standard area diagrams",
    "section": "6.3 Designing SADs in R",
    "text": "6.3 Designing SADs in R\nThe diagrams used in a set have been developed using various methods and technologies, ranging from hand-drawn diagrams to actual photographs (Del Ponte et al. 2017). There is an increasing trend towards using actual photos that are digitally analyzed using standard image analysis software to determine the percent area affected. With this approach, a large set of images is analyzed, and some images are chosen to represent the severities in the SAD according to the scale structure.\nIn R, the pliman package has a function called sad() which allows the automatic generation of a SADs with a pre-defined number of diagrams. Firstly, as shown in the previous chapter, the set of images to be selected needs to be analysed using the measure_disease() function. Then, a SADs is automatically generated. In the function, the specimens with the smallest and highest severity will be selected for the SAD. The intermediate diagrams are sampled sequentially to achieve the pre-defined number of images after the severity has been ordered from low to high. More details of the function here.\nLet’s use the same set of 10 soybean leaves, as seen in the previous chapter, depicting the rust symptoms and create the sbr object.\n\nlibrary(pliman)\nh &lt;- image_import(\"imgs/sbr_h.png\")\ns &lt;- image_import(\"imgs/sbr_s.png\")\nb &lt;- image_import(\"imgs/sbr_b.png\")\n\nsbr &lt;- measure_disease(\n  pattern = \"img\",\n  dir_original = \"imgs/originals\" ,\n  dir_processed = \"imgs/processed\",\n  save_image = TRUE,\n  img_healthy = h,\n  img_symptoms = s,\n  img_background = b,\n  show_original = FALSE, # set to TRUE for showing the original.\n  col_background = \"white\", \n  verbose = FALSE,\n  plot = FALSE\n)\n\nWe are ready to run the sad() function to create a SADs with five diagrams side by side. The resulting SADs is in two-color as standard. Set the argument show_original to TRUE for showing the orignal image in the SADs.\n\nsad(sbr, 5, ncol = 5)",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standard area diagrams</span>"
    ]
  },
  {
    "objectID": "data-sads.html#analysis-of-sads-validation-data",
    "href": "data-sads.html#analysis-of-sads-validation-data",
    "title": "6  Standard area diagrams",
    "section": "6.4 Analysis of SADs validation data",
    "text": "6.4 Analysis of SADs validation data\nTo evaluate the effect of SAD on accuracy components, analyze the data, preferably using concordance analysis methods (see chapter), to fully explore which component is affected and to gain insight into the ramification of errors. Linear regression should not be used as the sole method but it could be complementary for comparison with previous literature.\nInferential methods should be used for testing hypotheses related to gain in accuracy. If parametric tests are used (paired t-test for example), make sure to check that the assumptions are not violated. Alternatively, nonparametric tests (Wilcoxon signed rank) or nonparametric bootstrapping should be used when the conditions for parametric tests are not met. More recently, a (parametric) mixed modelling framework has been used to analyse SADs validation data where raters are taken as a random effects in the model (Franceschi et al. 2020; González-Domínguez et al. 2014; Pereira et al. 2020).\n\n6.4.1 Non parametric boostrapping of differences\nBootstrap is a resampling method where large numbers of samples of the same size are repeatedly drawn, with replacement, from a single original sample. It is commonly used when the distribution of a statistic is unknown or complicated and the sample size is too small to draw a valid inference.\nA bootstrap-based equivalence test procedure was first proposed as complementary to parametric (paired t-test) or non-parametric (Wilcoxon) to analyze severity estimation data in a study on the development and validation of a SADs for pecan scab (Yadav et al. 2012). The equivalence test was used to calculate 95% confidence intervals for each statistic by bootstrapping using the percentile method (with an equivalence test, the null hypothesis is the converse of H0, i.e. the null hypothesis is non-equivalence). In that study, the test was used to compare means of the CCC statistics across raters under two conditions: 1) without versus with the SAD; and 2) experienced versus inexperienced raters.\nTo apply the bootstrap-based equivalence test, let’s work with the CCC data for a sample of 20 raters who estimated severity of soybean rust SAD first without and then with the aid. The CCC was calculated as shown here.\n\nlibrary(tidyverse)\nlibrary(r4pde)\n\nsbr &lt;- tibble::tribble(\n  ~rater, ~aided, ~unaided,\n      1,   0.97,     0.85,\n      2,   0.97,     0.85,\n      3,   0.95,     0.82,\n      4,   0.93,     0.69,\n      5,   0.97,     0.84,\n      6,   0.96,     0.86,\n      7,   0.98,     0.78,\n      8,   0.93,     0.72,\n      9,   0.94,     0.67,\n     10,   0.95,     0.53,\n     11,   0.94,     0.78,\n     12,   0.98,     0.89,\n     13,   0.96,      0.8,\n     14,   0.98,     0.87,\n     15,   0.98,      0.9,\n     16,   0.98,     0.87,\n     17,   0.98,     0.84,\n     18,   0.97,     0.86,\n     19,   0.98,     0.89,\n     20,   0.98,     0.78\n  )\n\nLet’s visualize the data using boxplots. Each point in the plot represents a rater.\n\ntheme_set(theme_r4pde())\nsbr |&gt; \n  pivot_longer(2:3, names_to = \"condition\", values_to =\"estimate\") |&gt; \n  ggplot(aes(condition, estimate))+\n  geom_boxplot(outlier.colour = NA)+\n  geom_jitter(width = 0.05, size = 2, alpha = 0.5)+\n  theme_r4pde()+\n  ylim(0.4,1)\n\n\n\n\n\n\n\n\nTo proceed with bootstrapping, we first create a new variable to hold the differences between the means of the estimates (aided minus unaided). If the 95% CI does not include zero, this means that there was a significant improvement in the statistics.\n\n# diff of means\nsbr$diff &lt;- sbr$aided - sbr$unaided\n\nsbr |&gt; \n  ggplot(aes(x= diff))+\n  theme_r4pde()+\n  geom_histogram(bins = 10, color = \"white\")\n\n\n\n\n\n\n\n\nUsing the simpleboot and boot packages of R:\n\nlibrary(simpleboot)\nb.mean &lt;- one.boot(sbr$diff, mean, 999)\nboot::boot.ci(b.mean)\n\nWarning in boot::boot.ci(b.mean): bootstrap variances needed for studentized\nintervals\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = b.mean)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.1226,  0.1950 )   ( 0.1195,  0.1915 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.1275,  0.1995 )   ( 0.1285,  0.2020 )  \nCalculations and Intervals on Original Scale\n\nmean(b.mean$data)\n\n[1] 0.1595\n\nhist(b.mean)\n\n\n\n\n\n\n\n\nUsing the bootstrap package:\n\nlibrary(bootstrap)\nb &lt;- bootstrap(sbr$diff, 999, mean)\nquantile(b$thetastar, c(.025,.975))\n\n    2.5%    97.5% \n0.127975 0.197500 \n\nmean(b$thetastar)\n\n[1] 0.1591947\n\nsd(b$thetastar)\n\n[1] 0.01803024\n\nse &lt;- function(x) sqrt(var(x)/length(x))\nse(b$thetastar)\n\n[1] 0.0005704516\n\n\nBoth procedures shown above have led to similar results. The 95% CIs of the differences did not include zero, so a significant improvement in accuracy can be inferred.\n\n\n6.4.2 Parametric and non-parametric paired sample tests\nWhen two estimates are gathered from the same rater at different times, these data points are not independent. In such situations, a paired sample t-test can be utilized to test if the mean difference between two sets of observations is zero. This test requires each subject (or leaf, in our context) to be measured or estimated twice, resulting in pairs of observations. However, if the assumptions of the test (such as normality) are violated, a non-parametric equivalent, such as the Wilcoxon signed-rank test, also known as the Wilcoxon test, can be employed. This alternative is particularly useful when the data are not normally distributed.\nTo proceed with these tests, we first need to ascertain whether our data are normally distributed. We should also verify whether the variances are equal. Let’s now apply these two tests to our data and compare the results.\n\n# normality test\nshapiro.test(sbr$aided)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sbr$aided\nW = 0.82529, p-value = 0.002111\n\nshapiro.test(sbr$unaided)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sbr$unaided\nW = 0.83769, p-value = 0.003338\n\n# equal variance test\nvar.test(sbr$aided, sbr$unaided)\n\n\n    F test to compare two variances\n\ndata:  sbr$aided and sbr$unaided\nF = 0.037789, num df = 19, denom df = 19, p-value = 1.53e-09\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.01495720 0.09547109\nsample estimates:\nratio of variances \n        0.03778862 \n\n# paired t-test\nt.test(sbr$aided, sbr$unaided, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  sbr$aided and sbr$unaided\nt = 8.812, df = 19, p-value = 3.873e-08\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.1216158 0.1973842\nsample estimates:\nmean difference \n         0.1595 \n\n# Wilcoxon test\nwilcox.test(sbr$aided, sbr$unaided, paired = TRUE)\n\nWarning in wilcox.test.default(sbr$aided, sbr$unaided, paired = TRUE): cannot\ncompute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  sbr$aided and sbr$unaided\nV = 210, p-value = 9.449e-05\nalternative hypothesis: true location shift is not equal to 0\n\n\nAs shown above, the two assumptions were violated, so we could rely more confidently on the non-parametric test.\n\n\n6.4.3 Mixed effects modeling\nMixed models, also known as mixed effects models or multilevel models, are an extension of traditional linear models that are used for analyzing hierarchical or clustered data. These models are particularly useful when dealing with data where observations may not be fully independent, or when the assumption of independence is violated. This happens, for instance, when data are collected over time from the same individuals or units, or when individuals are grouped or nested within higher-level units, such as in our case where measurements are taken by different raters (Brown 2021).\nMixed models enable us to model both fixed and random effects. Fixed effects represent the usual regression parameters that we are primarily interested in estimating, while random effects model the random variation that occurs at different levels of hierarchy or clustering. They allow us to account for variability among different levels of data, like inter-rater variability or intra-subject variability in repeated measures designs.\nIn our context, we consider raters as random effects because we view them as a sample drawn from a larger population of potential raters, and our goal is to generalize our findings to this larger population. If we were to sample additional raters, we would expect these new raters to differ from our current ones. However, by considering raters as a random effect in our model, we can account for this inter-rater variability and make more accurate inferences about the overall population.\nThe random effects component in the mixed model allows us to capture and model the additional variance that is not explicitly accounted for by the fixed effects in our model. In other words, random effects help us to capture and quantify the ‘unexplained’ or ‘residual’ variation that exists within and between the clusters or groups in our data. This could include, for instance, variation in disease measurements that are taken repeatedly from the same subjects. In conclusion, mixed models provide a robust and flexible framework for modeling hierarchical or clustered data, allowing us to effectively account for both fixed and random effects and to make more accurate inferences about our data.\nLet’s start reshaping our data to the long format and assign them to a new data frame.\n\nsbr2 &lt;- sbr |&gt; \n  pivot_longer(2:3, names_to = \"condition\", values_to = \"estimate\")\n\nNow we fit the mixed model using the lmer function of the lme4 package. We will fit the model to the logit of the estimate because they should be bounded between zero and one. Preliminary analysis using non-transformed or log-transformed data resulted in lack of normality of residuals and heterocedasticity (not shown).\n\nlibrary(lme4) \nlibrary(car) # for logit function\nmix &lt;- lmer(logit(estimate) ~ condition + (1 | rater), data = sbr2)\n\n# Check model performance\nlibrary(performance)\ncheck_normality(mix)\n\nOK: residuals appear as normally distributed (p = 0.381).\n\ncheck_heteroscedasticity(mix)\n\nOK: Error variance appears to be homoscedastic (p = 0.961).\n\n# Check effect of condition\ncar::Anova(mix)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: logit(estimate)\n           Chisq Df Pr(&gt;Chisq)    \ncondition 458.44  1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Estimate the means for each group\nlibrary(emmeans)\nem &lt;- emmeans(mix, ~ condition, type =\"response\")\nem\n\n condition response      SE   df lower.CL upper.CL\n aided        0.968 0.00359 25.5    0.959    0.974\n unaided      0.817 0.01719 25.5    0.779    0.849\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n# Contrast the means\npairs(em)\n\n contrast        odds.ratio    SE df null t.ratio p.value\n aided / unaided       6.72 0.597 19    1  21.411  &lt;.0001\n\nDegrees-of-freedom method: kenward-roger \nTests are performed on the log odds ratio scale \n\n# plot the means with 95% CIs\nplot(em) +\n  coord_flip()+\n  xlim(0.7,1)+\n  theme_r4pde()\n\n\n\n\n\n\n\n\nAs shown above, we can reject the null hypothesis that the means are the same between the two groups.\nAlternatively, we could fit GLMMs - generalized linear mixed models, which extend the traditional linear mixed models to accommodate response variables that follow different distributions. They are particularly useful when the response variable does not follow a normal distribution and cannot be adequately transformed to meet the parametric assumptions of traditional linear models. The glmmTMB package in R provides a convenient and flexible platform to fit GLMMs using a variety of distributions (Brooks et al. 2017).\nIn our case, considering our response variable bounded between 0 and 1, a Beta distribution might be a suitable choice. Beta distribution is a continuous probability distribution defined on the interval [0, 1], and is commonly used for modelling variables that represent proportions or percentages.\nThe function glmmTMB() from the glmmTMB package can be used to fit a GLMM with a Beta distribution. In this function, we specify the distribution family as beta_family().\n\nlibrary(glmmTMB)\nmix2 &lt;-  glmmTMB(estimate ~ condition + (1| rater), \n                 data = sbr2, \n                 family = beta_family())\n\nBecause the package performance does not handle the glmmTMB output, we will use the DHARMa package in R which can be particularly useful for checking the assumptions of your GLMM fitted with glmmTMB(). The package provides a convenient way to carry out residual diagnostics for models fitted via maximum likelihood estimation, including GLMMs. This package creates standardized residuals from the observed responses and the predicted responses of a fitted model, and then compares these residuals to a simulated set of residuals under a correct model.\n\nlibrary(DHARMa)\n\nplot(simulateResiduals(mix2))\n\n\n\n\n\n\n\n\nIn this example, simulateResiduals() generates simulated residuals from your fitted model, and the plot creates a plot of these residuals. This showed that the residuals from our model are uniformly distributed, which is an assumption of GLMMs. We can now proceed with the posthoc analysis and noticed that the results are similar to when the response variable was transformed to logit.\n\ncar::Anova(mix2)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: estimate\n           Chisq Df Pr(&gt;Chisq)    \ncondition 400.93  1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlibrary(emmeans)\nem &lt;- emmeans(mix2, ~ condition, type = \"response\")\nem\n\n condition response     SE  df asymp.LCL asymp.UCL\n aided        0.967 0.0043 Inf     0.958     0.975\n unaided      0.814 0.0167 Inf     0.779     0.845\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n# Contrast the means\npairs(em)\n\n contrast        odds.ratio    SE  df null z.ratio p.value\n aided / unaided       6.71 0.638 Inf    1  20.023  &lt;.0001\n\nTests are performed on the log odds ratio scale \n\n\n\n\n\n\nBock, C. H., Pethybridge, S. J., Barbedo, J. G. A., Esker, P. D., Mahlein, A.-K., and Del Ponte, E. M. 2021. A phytopathometry glossary for the twenty-first century: towards consistency and precision in intra- and inter-disciplinary dialogues. Tropical Plant Pathology 47:14–24. https://doi.org/10.1007/s40858-021-00454-0.\n\n\nBrooks, M. E., Kristensen, K., van Benthem, K. J., Magnusson, A., Berg, C. W., Nielsen, A., Skaug, H. J., Maechler, M., and Bolker, B. M. 2017. glmmTMB balances speed and flexibility among packages for zero-inflated generalized linear mixed modeling. The R Journal 9:378–400. https://doi.org/10.32614/RJ-2017-066.\n\n\nBrown, V. A. 2021. An Introduction to Linear Mixed-Effects Modeling in R. Advances in Methods and Practices in Psychological Science 4:251524592096035. https://doi.org/10.1177/2515245920960351.\n\n\nDel Ponte, E. M., Cazón, L. I., Alves, K. S., Pethybridge, S. J., and Bock, C. H. 2022. How much do standard area diagrams improve accuracy of visual estimates of the percentage area diseased? A systematic review and meta-analysis. Tropical Plant Pathology 47:43–57. https://doi.org/10.1007/s40858-021-00479-5.\n\n\nDel Ponte, E. M., Pethybridge, S. J., Bock, C. H., Michereff, S. J., Machado, F. J., and Spolti, P. 2017. Standard Area Diagrams for Aiding Severity Estimation: Scientometrics, Pathosystems, and Methodological Trends in the Last 25 Years. Phytopathology® 107:1161–1174. https://doi.org/10.1094/phyto-02-17-0069-fi.\n\n\nFranceschi, V. T., Alves, K. S., Mazaro, S. M., Godoy, C. V., Duarte, H. S. S., and Del Ponte, E. M. 2020. A new standard area diagram set for assessment of severity of soybean rust improves accuracy of estimates and optimizes resource use. Plant Pathology 69:495–505. https://doi.org/10.1111/ppa.13148.\n\n\nGonzález-Domínguez, E., Martins, R. B., Del Ponte, E. M., Michereff, S. J., García-Jiménez, J., and Armengol, J. 2014. Development and validation of a standard area diagram set to aid assessment of severity of loquat scab on fruit. European Journal of Plant Pathology. https://doi.org/10.1007/s10658-014-0400-2.\n\n\nMoreira, R. R., Silva Silveira Duarte, H. da, and De Mio, L. L. M. 2018. Improving accuracy, precision and reliability of severity estimates of Glomerella leaf spot on apple leaves using a new standard area diagram set. European Journal of Plant Pathology 153:975–982. https://doi.org/10.1007/s10658-018-01610-0.\n\n\nPereira, W. E. L., Andrade, S. M. P. de, Del Ponte, E. M., Esteves, M. B., Canale, M. C., Takita, M. A., Coletta-Filho, H. D., and De Souza, A. A. 2020. Severity assessment in the Nicotiana tabacum-Xylella fastidiosa subsp. pauca pathosystem: design and interlaboratory validation of a standard area diagram set. Tropical Plant Pathology 45:710–722. https://doi.org/10.1007/s40858-020-00401-5.\n\n\nYadav, N. V. S., Vos, S. M. de, Bock, C. H., and Wood, B. W. 2012. Development and validation of standard area diagrams to aid assessment of pecan scab symptoms on fruit. Plant Pathology 62:325–335. https://doi.org/10.1111/j.1365-3059.2012.02641.x.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standard area diagrams</span>"
    ]
  },
  {
    "objectID": "data-training.html",
    "href": "data-training.html",
    "title": "7  Training sessions",
    "section": "",
    "text": "7.1 Training sessions\nIn the same way that Standard Area Diagrams (SADs) can improve the accuracy of visual estimates of disease severity, exposure to a diverse set of diagrams or actual images with known severity values can significantly enhance a rater’s assessment proficiency. The key to this improvement is the frequent exposure to various levels of severity, which enables the rater to better calibrate their judgments over time.\nAs the rater engages with these reference diagrams or images, they develop a mental model of the severity scale. This mental model is continually refined through repeated exposure to a variety of severity values. This iterative learning process allows the rater to adjust their estimations based on the feedback from known values, thus improving their overall accuracy and precision in disease severity estimation.\nSuch a process, often termed ‘training’, is particularly beneficial in scenarios where visual estimation is the primary tool for assessing disease severity. Training raters using sets of reference images is an effective strategy to enhance inter-rater reliability and consistency over time, especially when coupled with other tools like SADs.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Training sessions</span>"
    ]
  },
  {
    "objectID": "data-training.html#software",
    "href": "data-training.html#software",
    "title": "7  Training sessions",
    "section": "7.2 Software",
    "text": "7.2 Software\nIndeed, the use of computerized training sessions in assessing disease severity has a rich history, dating back to the mid-1980s when personal computers were first introduced. These early applications were developed using operating systems like DOS or Windows and involved software like AREAGRAM, DISTRAIN, DISEASE.PRO, ESTIMATE, SEVERITY.PRO, and COMBRO. These programs utilized computerized images with specific and measured disease severities to train raters, as outlined in the review by Bock et al. (2021).\nThe main advantage of these computerized training sessions is that they allow raters to familiarize themselves with various disease severity levels, thereby enhancing their performance in severity estimation. Such training has been proven to significantly improve the accuracy and consistency of disease severity evaluations.\nHowever, a potential limitation of this approach is the short-lived nature of the benefits derived from such training. The skills and proficiency gained from these computerized training sessions may degrade over time, necessitating regular retraining for raters to maintain their performance level. This could be due to the fact that estimation skills, like many other skills, require regular practice for maintenance. Without ongoing exposure to severity scales and continued practice, the accuracy and precision of a rater’s estimates may decline.\nTo address this challenge, it would be beneficial to implement a structured training regimen that includes regular retraining sessions. This could help ensure the continued proficiency of raters in estimating disease severity, thus maintaining the accuracy and reliability of assessments over time. Furthermore, it would be advantageous to investigate the optimal frequency and structure of these training sessions to maximize their effectiveness and sustainability in the long term.\n\n\n\n\n\n\nFigure 7.1: Selected screenshots from Severity.Pro, the disease assessment training program by Forrest W. Nutter (Madden et al. 2021).\n\n\n\n\n7.2.1 Online training tools\nIn Brazil, the “Sistema de treinamento de acuidade visual” was initially developed as a web-based system to train raters in assessing citrus canker. The system has evolved over time and now has a current version that is accessible on both iOS and Android platforms. You can find the current version of the system at this link. This platform provides an interactive training experience to enhance the ability of raters in accurately assessing the severity of citrus canker.\nIn Mexico, a specific application called Validar-PER has been developed to train raters in visually assessing the severity of coffee leaf rust. This application utilizes diagrammatic log-based scales as a standardized approach for severity assessment. You can access the Validar-PER application online here. The application aims to improve the proficiency of raters in evaluating the severity of coffee leaf rust using a systematic and standardized methodology.\n\n\n\n\n\n\nFigure 7.2: Screen of Validar-PER, an online training module for assessing coffee leaf rust severity\n\n\n\n\n\n7.2.2 Training software made with R\n\n7.2.2.1 TraineR\nTraineR, developed by the author of this book, is created using R and Shiny. Its purpose is to train users in assessing disease severity, specifically expressed as the percentage area of an organ (leaf or fruit) affected by lesions.\nTo use the app, users can adjust parameters for organ shape, organ color, as well as lesion shape, lesion color, lesion number, and lesion size. These adjustments will generate a standard area diagram with an ellipsoidal shape.\nTo initiate the training, users should first set the desired number of attempts for the session and click on the “generate new” button. A diagram will then be displayed, and users should input their estimate of the diseased area as a numeric value in percentage. The estimate will be recorded and shown in a table along with the actual value, enabling a comparison between the actual and estimated values.\nUsers can continue generating new diagrams and providing estimates until they reach the defined number of attempts. Once the final attempt is completed, the app will present the accuracy in the form of Lin’s concordance correlation coefficient to the user. Plots depicting the relationship between estimates and actual values, as well as the error of the estimates, will be displayed. Furthermore, comprehensive accuracy statistics are also made available.\nCurrently, the app has certain limitations, including the inability to overlap lesions and a maximum severity representation of approximately 60%. Nonetheless, it remains a valuable educational and demonstration tool.\n\n\n\n\n\n\nFigure 7.3: Screen of TraineR, an online app for training in the assessment of plant disease severity\n\n\n\n\n\n7.2.2.2 Trainer2\nTrainer2 the second generation of TraineR, takes advantage of actual photographs showcasing disease symptoms. This updated version allows for testing the ability of raters to assess disease severity, particularly by evaluating the percentage area affected based on real symptoms captured in the photographs.\nBy utilizing actual images, Trainer2 offers a more realistic and practical approach to training raters. Raters can now evaluate disease severity by visually inspecting the symptoms depicted in the photographs, enhancing their ability to accurately assess the extent of damage in terms of the affected area.\nThe incorporation of real symptoms in Trainer2 serves as a valuable tool for evaluating and refining the skills of raters in disease severity assessment. It provides a more authentic training experience and helps raters become proficient in identifying and quantifying the extent of disease based on visual cues observed in real-life scenarios.\n\n\n\n\n\n\nFigure 7.4: Screen of traineR2, an online for training in the assessment of plant disease severity based on real symptoms captured in photographs\n\n\n\n\n\n\n\nBock, C. H., Chiang, K.-S., and Del Ponte, E. M. 2021. Plant disease severity estimated visually: a century of research, best practices, and opportunities for improving methods and practices to maximize accuracy. Tropical Plant Pathology 47:25–42. https://doi.org/10.1007/s40858-021-00439-z.\n\n\nMadden, L. V., Esker, P. D., and Pethybridge, S. J. 2021. Forrest W. Nutter, Jr.: a career in phytopathometry. Tropical Plant Pathology 47:5–13. https://doi.org/10.1007/s40858-021-00469-7.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Training sessions</span>"
    ]
  },
  {
    "objectID": "data-remote-sensing.html",
    "href": "data-remote-sensing.html",
    "title": "8  Remote sensing",
    "section": "",
    "text": "8.1 Introduction\nRemote sensing techniques, in the sense of gathering & processing of data by a device separated from the object under study, are increasingly providing an important component of the set of technologies available for the study of vegetation systems and their functioning. This is in spite that many applications only provide indirect estimations of the biophysical variables of interest (Jones and Vaughan 2010).\nParticular advantages of remote sensing for vegetation studies are that: (i) it is non-contact and non-destructive; and (ii) observations are easily extrapolated to larger scales. Even at the plant scale, remotely sensed imagery is advantageous as it allows rapid sampling of large number of plants (Jones and Vaughan 2010).\nThis chapter aims at providing a conceptual & practical approach to apply remote sensing data and techniques to infer information useful for monitoring crop diseases. The structure of this chapter is divided into four sections. The first one introduces basic remote sensing concepts and provides a summary of applications of remote sensing of crop diseases. The second one illustrates a case study focused on identification of banana Fusarium wilt from multispectral UAV imagery. The third one illustrates a case study dealing with estimation of cercospora leaf spot disease on table beet. Finally, it concludes with several reflections about potential and limitations of this technology.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Remote sensing</span>"
    ]
  },
  {
    "objectID": "data-remote-sensing.html#remote-sensing-background",
    "href": "data-remote-sensing.html#remote-sensing-background",
    "title": "8  Remote sensing",
    "section": "8.2 Remote sensing background",
    "text": "8.2 Remote sensing background\n\n8.2.1 Optical remote sensing\nOptical remote sensing makes use of the radiation reflected by a surface in the visible (~400-700 nm), the near infrared (700-1300 nm) and shortwave infrared (1300-~3000 nm) parts of the electromagnetic spectrum. Spaceborne & airborne-based remote sensing and field spectroscopy utilize the solar radiation as an illumination source. Lab spectroscopy utilizes a lamp as an artificial illumination source Figure 8.1.\n\n\n\n\n\n\nFigure 8.1: Optical remote sensing via spaceborne sensors, field spectroscopy and laboratory spectroscopy (Adapted from https://pages.cms.hu-berlin.de/EOL/geo_rs/index.htm) .\n\n\n\nThe proportion of the radiation reflected by a surface depends on the surface’s spectral reflection, absorption and transmission properties and varies with wavelength Figure 8.2. These spectral properties in turn depend on the surface’s physical and chemical constituents Figure 8.2. Measuring the reflected radiation hence allows us to draw conclusions on a surface’s characteristic, which is the basic principle behind optical remote sensing.\n\n\n\n\n\n\nFigure 8.2: Reflection, absortion and transmission by a surface (left). Spectral reflectance profile of a vegetation with major factors determining the reflection (right). Source: https://pages.cms.hu-berlin.de/EOL/geo_rs/\n\n\n\n\n\n8.2.2 Vegetation spectral properties\nOptical remote sensing enables the deduction of various vegetation-related characteristics, including biochemical properties (e.g., pigments, water content), structural properties (e.g., leaf area index (LAI), biomass) or process properties (e.g., light use efficiency (LUE)). The ability to deduce these characteristics depends on the ability of a sensor to resolve vegetation spectra. Hyperspectral sensors capture spectral information in hundreds of narrow and contiguous bands in the VIS, NIR and SWIR, and, thus, resolve subtle absorption features caused by specific vegetation constituents (e.g. anthocyanins, carotenoids, lignin, cellulose, proteins). In contrast, multispectral sensors capture spectral information in a few broad spectral bands and, thus, only resolve broader spectral features. Still, multispectral systems like Sentinel-2 have been demonstrated to be useful to derive valuable vegetation properties (e.g., LAI, chlorophyll).\n\n\n\n\n\n\nFigure 8.3: Vegetation spectrum in hyperspectral (ASD FielSpec4, EnMAP) and multispectral (Sentinel-2) resolution as well as characteristic spectral features caused by various constituents and processes (absorption lines shown as grey dashed lines). Source: (Hank et al. 2018)\n\n\n\n\n\n8.2.3 What measures a remote sensor?\nOptical sensors/spectrometers measure the radiation reflected by a surface to a certain solid angle in the physical quantity radiance. The unit of radiance is watts per square meter per steradian (W • m-2 • sr-1) Figure 8.4. In other words, radiance describes the amount of energy (W) that is reflected from a surface (m-2) and arrives at the sensor in a three-dimensional angle (sr-1).\n\n\n\n\n\n\nFigure 8.4: Source: https://pages.cms.hu-berlin.de/EOL/geo_rs/\n\n\n\nA general problem related to the use of radiance as unit of measurement is the variation of radiance values with illumination. For example, the absolute incoming solar radiation varies over the course of the day as a function of the relative position between sun and surface and so does the absolute amount of radiance measured. We can only compare measurements taken a few hours apart or on different dates when we are putting the measured radiance in relation to the incoming illumination.\nThe quotient between measured reflected radiance and measured incoming radiance (Radiancereflected / Radianceincoming) is called reflectance (usually denoted as \\(\\rho\\)). Reflectance provides a stable unit of measurement which is independent from illumination and is the percentage of the total measurable radiation, which has not been absorbed or transmitted.\n\n\n8.2.4 Hyperspectral vs.multispectral imagery\nHyperspectral imaging involves capturing and analyzing data from a large number of narrow, contiguous bands across the electromagnetic spectrum, resulting in a high-resolution spectrum for each pixel in the image. As a result, a hyperspectral camera provides smooth spectra. The spectra provided by multispectral cameras are more like stairs or saw teeth without the ability to depict acute spectral signatures Figure 8.6.\n\n\n8.2.5 Vegetation Indices\nA vegetation index (VI) represents a spectral transformation of two or more bands of spectral imagery into a singleband image. A VI is designed to enhance the vegetation signal with regard to different vegetation properties, while minimizing confounding factors such as soil background reflectance, directional, or atmospheric effects. There are many different VIs, including multispectral broadband indices as well as hyperspectral narrowband indices.\nMost of the multispectral broadband indices make use of the inverse relationship between the lower reflectance in the red (through chlorophyll absorption) and higher reflectance in the near-infrared (through leaf structure) to provide a measure of greenness that can be indirectly related to biochemical or structural vegetation properties (e.g., chlorophyll content, LAI). The Normalized Difference Vegetation Index (NDVI) is one of the most commonly used broadband VIs:\n\\[NDVI = \\frac{\\rho_{nir} - \\rho_{red} }{\\rho_{nir} + \\rho_{red}}\\]\nThe interpretation of the absolute value of the NDVI is highly informative, as it allows the immediate recognition of the areas of the farm or field that have problems. The NDVI is a simple index to interpret: its values vary between -1 and 1, and each value corresponds to a different agronomic situation, regardless of the crop Figure 8.5\n\n\n\n\n\n\nFigure 8.5: Agronomic conditions depending on the values in a NDVI scale",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Remote sensing</span>"
    ]
  },
  {
    "objectID": "data-remote-sensing.html#remote-sensing-of-crop-diseases",
    "href": "data-remote-sensing.html#remote-sensing-of-crop-diseases",
    "title": "8  Remote sensing",
    "section": "8.3 Remote sensing of crop diseases",
    "text": "8.3 Remote sensing of crop diseases\n\n8.3.1 Detection of plant stress\nOne popular use of remote sensing is in diagnosis and monitoring of plant responses to biotic (i.e. disease and insect damage) and abiotic stress (e.g. water stress, heat, high light, pollutants) with hundreds of publications on the topic. It is worth nothing that most available techniques monitor the plant response rather than the stress itself. For example, with some diseases, it is common to estimate changes in canopy cover (using vegetation indices) as measures of “disease” but this measure could also be associated to water deficit (Jones and Vaughan 2010). This highlights the importance of measuring crop conditions in the field & laboratory to collect reliable data and be able to disentangle complex plant responses. Anyway, remote sensing can be used as the first step in site-specific disease control and also to phenotype the reactions of plant genotypes to pathogen attack (Lowe et al. 2017).\n\n\n8.3.2 Optical methods for measuring crop disease\nThere are a variety of optical sensors for the assessment of plant diseases. Sensors can be based only on the visible spectrum (400-700 nm) or on the visible and/or infrared spectrum (700 nm - 1mm). The latter may include near-infrared (NIR) (0.75-1.4 \\(μm\\)), short wavelength infrared (SWIR) (1.4–3 \\(μm\\)), medium wavelength infrared (MWIR) (3-8 \\(μm\\)), or thermal infrared (8-15 \\(μm\\)) Figure 8.6. Sensors record either imaging or non imaging (i.e average) spectral radiance values which need to be converted to reflectance before conducting any crop disease monitoring task.\n\n\n\n\n\n\nFigure 8.6: source: Del Ponte et al. (2024)\n\n\n\nIn a recent chapter of Agrio’s Plant Pathology, Del Ponte et al. (2024) highlights the importance of understanding the basic principles of the interaction of light with plant tissue or the plant canopy as a crucial prerrequisite for the analysis and interpretation for disease assessment. When a plant is infected, there are changes to the phisiology and biochemistry of the host, with the eventual development of disease symptoms and/or signs of the pathogen which may be accompanied by structural and biochemical changes that affect absorbance, transmittance, and reflectance of light Figure 8.8.\n\n\n\n\n\n\nFigure 8.7: source: Del Ponte et al. (2024)\n\n\n\n\n\n8.3.3 Scopes of disease sensing\nThe quantification of typical disease symptoms (disease severity) and assessment of leaves infected by several pathogens are relatively simple for imaging systems but may become a challenge for nonimaging sensors and sensors with inadequate spatial resolution (Oerke 2020). Systematic monitoring of a crop by remote sensors can allow farmers to take preventive actions if infections are detected early.  Remote sensing sensors & processing techniques need to be carefully selected to be capable of (a) detecting a deviation in the crop’s health status brought about by pathogens, (b) identifying the disease, and (c) quantifying the severity of the disease.  Remote sensing can also be effectively used in (d) food quality control Figure 8.8.\n\n\n\n\n\n\nFigure 8.8: Source: (Oerke 2020)\n\n\n\n\n\n8.3.4 Monitoring plant diseases\nSensing of plants for precision disease control is done in large fields or greenhouses where the aim is to detect the occurrence of diseases at the early stages of epidemics, i.e., at low symptom frequency. Lowe et al. (2017) reviewed hyperspectral imaging of plant diseases, focusing on early detection of diseases for crop monitoring. They report several analysis techniques successfully used for the detection of biotic and abiotic stresses with reported levels of accuracy higher than 80%.\n\nStatistical techniques used to detect both biotic and abiotic stresses in crops. Source: Lowe et al. (2017)\n\n\n\n\n\n\nTechnique\nPlant (stress)\n\n\n\n\nQuadratic discriminant analysis (QDA)\nWheat (yellow rust)\n\n\n\nAvacado (laurel wilt)\n\n\nDecision tree (DT)\nAvacado (laurel wilt)\n\n\n\nSugarbeet (cerospora leaf spot)\n\n\n\nSugarbeet (powdery mildew)\n\n\n\nSugarbeet (leaf rust)\n\n\nMultilayer perceptron (MLP)\nWheat (yellow rust)\n\n\nPartial least square regression (PLSR)\nCelery (sclerotinia rot)\n\n\nRaw\n\n\n\nSavitsky-Golay 1st derivative\n\n\n\nSavitsky-Golay 2nd derivative\n\n\n\nPartial least square regression (PLSR)\nWheat (yellow rust)\n\n\nFishers linear determinant analysis\nWheat (aphid)\n\n\n\nWheat (powdery mildew)\n\n\n\nWheat (powdery mildew)\n\n\nErosion and dilation\nCucumber (downey mildew)\n\n\nSpectral angle mapper (SAM)\nSugarbeet (cerospora leaf spot)\n\n\n\nSugarbeet (powdery mildew)\n\n\n\nSugarbeet (leaf rust)\n\n\n\nWheat (head blight)\n\n\nArtificial neural network (ANN)\nSugarbeet (cerospora leaf spot)\n\n\n\nSugarbeet (powdery mildew)\n\n\n\nSugarbeet (leaf rust)\n\n\nSupport vector machine (SVM)\nSugarbeet (cerospora leaf spot)\n\n\n\nSugarbeet (powdery mildew)\n\n\n\nSugarbeet (leaf rust)\n\n\n\nBarley (drought)\n\n\nSpectral information divergence (SID)\nGrapefruit\n\n\n\n(canker, greasy spot, insect\n\n\n\ndamage, scab, wind scar)\n\n\n\nLowe et al. (2017) state that remote sensing of diseases under production conditions is challenging because of variable environmental factors and crop-intrinsic characteristics, e.g., 3D architecture, various growth stages, variety of diseases that may occur simultaneously, and the high sensitivity required to reliably perceive low disease levels suitable for decision-making in disease control. The use of less sensitive systems may be restricted to the assessment of crop damage and yield losses due to diseases.\n\n\n8.3.5 UAV applications for plant disease detection and monitoring\nKouadio et al. (2023) undertook a systematic quantitative literature review to summarize existing literature in UAV-based applications for plant disease detection and monitoring. Results reveal a global disparity in research on the topic, with Asian countries being the top contributing countries. World regions such as Oceania and Africa exhibit comparatively lesser representation. To date, research has largely focused on diseases affecting wheat, sugar beet, potato, maize, and grapevine Figure 8.9. Multispectral, red-green-blue, and hyperspectral sensors were most often used to detect and identify disease symptoms, with current trends pointing to approaches integrating multiple sensors and the use of machine learning and deep learning techniques. The authors suggest that future research should prioritize (i) development of cost-effective and user-friendly UAVs, (ii) integration with emerging agricultural technologies, (iii) improved data acquisition and processing efficiency (iv) diverse testing scenarios, and (v) ethical considerations through proper regulations.\n\n\n\n\n\n\nFigure 8.9: Source: Kouadio et al. (2023)",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Remote sensing</span>"
    ]
  },
  {
    "objectID": "data-remote-sensing.html#disease-detection",
    "href": "data-remote-sensing.html#disease-detection",
    "title": "8  Remote sensing",
    "section": "8.4 Disease detection",
    "text": "8.4 Disease detection\nThis section illustrates the use of unmanned aerial vehicle (UAV) remote sensing imagery for identifying banana wilt disease. Fusarium wilt of banana, also known as “banana cancer”, threatens banana production areas worldwide. Timely and accurate identification of Fusarium wilt disease is crucial for effective disease control and optimizing agricultural planting structure (Pegg et al. 2019).\nA common initial symptom of this disease is the appearance of a faint pale yellow streak at the base of the petiole of the oldest leaf. This is followed by leaf chlorosis which progresses from lower to upper leaves, wilting of leaves and longitudinal splitting of their bases. Pseudostem splitting of leaf bases is more common in young, rapidly growing plants [Pegg et al. (2019)]Figure 8.10.\n\n\n\n\n\n\nFigure 8.10: Cavendish plant affected by Race 1 Foc (D. Peasley). Source: (Pegg et al. 2019)\n\n\n\nYe et al. (2020) made publicly available experimental data (Huichun YE et al. 2022) on wilted banana plants collected in a banana plantation located in Long’an County, Guangxi (China). The data set includes UAV multispectral reflectance data and ground survey data on the incidence of banana wilt disease. The paper by Ye et al. (2020) reports that the banana Fusarium wilt disease can be easily identified using several vegetation indices (VIs) obtained from this data set. Tested VIs include green chlorophyll index (CIgreen), red-edge chlorophyll index (CIRE), normalized difference vegetation index (NDVI), and normalized difference red-edge index (NDRE). The dataset can be downloaded from here.\n\n8.4.1 Software setup\nLet’s start by cleaning up R memory:\n\nrm(list=ls())\n\nThen, we need to install several packages (if they are not installed yet):\n\nlist.of.packages &lt;- c(\"terra\", \n                      \"tidyterra\", \n                      \"stars\", \n                      \"sf\", \n                      \"leaflet\", \n                      \"leafem\", \n                      \"dplyr\", \n                      \"ggplot2\", \n                      \"tidymodels\")\nnew.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\nif(length(new.packages)) install.packages(new.packages)\n\nNow, let’s load all the required packages:\n\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(stars)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidymodels)\n\n\n\n8.4.2 Reading the dataset\nNext code supposes you have already downloaded the Huichun YE et al. (2022) dataset and unzipped its content under the data/banana_data directory.\n\n\n8.4.3 File formats\nLet’s list the files under each subfolder:\n\nlist.files(\"data/banana_data/1_UAV multispectral reflectance\")\n\n[1] \"UAV multispectral reflectance.tfw\"        \n[2] \"UAV multispectral reflectance.tif\"        \n[3] \"UAV multispectral reflectance.tif.aux.xml\"\n[4] \"UAV multispectral reflectance.tif.ovr\"    \n\n\nNote that the .tif file contains an orthophotomosaic of surface reflectance. It was created from UAV images taken with a Micasense Red Edge M camera which has five narrow spectral bands: Blue (465–485 nm), green (550–570 nm), red (653–673 nm), red edge (712–722 nm), and near-infrared (800–880 nm). We assume here that those images have been radiometrically and geometrically corrected.\n\nlist.files(\"data/banana_data/2_Ground survey data of banana Fusarium wilt\")\n\n[1] \"Ground_survey_data_of_banana_Fusarium_wilt.dbf\"    \n[2] \"Ground_survey_data_of_banana_Fusarium_wilt.prj\"    \n[3] \"Ground_survey_data_of_banana_Fusarium_wilt.sbn\"    \n[4] \"Ground_survey_data_of_banana_Fusarium_wilt.sbx\"    \n[5] \"Ground_survey_data_of_banana_Fusarium_wilt.shp\"    \n[6] \"Ground_survey_data_of_banana_Fusarium_wilt.shp.xml\"\n[7] \"Ground_survey_data_of_banana_Fusarium_wilt.shx\"    \n\n\nThis is shapefile with 80 points where the plant health status was collected in same date as the images.\n\nlist.files(\"data/banana_data/3_Boundary of banana planting region\")\n\n[1] \"Boundary_of_banana_planting_region.dbf\"    \n[2] \"Boundary_of_banana_planting_region.prj\"    \n[3] \"Boundary_of_banana_planting_region.sbn\"    \n[4] \"Boundary_of_banana_planting_region.sbx\"    \n[5] \"Boundary_of_banana_planting_region.shp\"    \n[6] \"Boundary_of_banana_planting_region.shp.xml\"\n[7] \"Boundary_of_banana_planting_region.shx\"    \n\n\nThis is a shapefile with one polygon representing the boundary of the study area.\n\n\n8.4.4 Read the orthomosaic and the ground data\nNow, let’s read the orthomosaic using the terra package:\n\n# Open the tif \ntif &lt;- \"data/banana_data/1_UAV multispectral reflectance/UAV multispectral reflectance.tif\"\n\n\nrrr &lt;- terra::rast(tif)\n\nLet’s check what we get:\n\nrrr\n\nclass       : SpatRaster \ndimensions  : 7885, 14420, 5  (nrow, ncol, nlyr)\nresolution  : 0.08, 0.08  (x, y)\nextent      : 779257.9, 780411.5, 2560496, 2561127  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 48N (EPSG:32648) \nsource      : UAV multispectral reflectance.tif \nnames       : UAV mu~ance_1, UAV mu~ance_2, UAV mu~ance_3, UAV mu~ance_4, UAV mu~ance_5 \nmin values  :      0.000000,      0.000000,      0.000000,     0.0000000,      0.000000 \nmax values  :      1.272638,      1.119109,      1.075701,     0.9651694,      1.069767 \n\n\nNote that this is a 5-band multispectral image with 8 cm pixel size.\nNow, let’s read the ground data:\n\nshp &lt;- \"data/banana_data/2_Ground survey data of banana Fusarium wilt/Ground_survey_data_of_banana_Fusarium_wilt.shp\"\nggg &lt;- sf::st_read(shp)\n\nReading layer `Ground_survey_data_of_banana_Fusarium_wilt' from data source \n  `/Users/emersondelponte/Documents/GitHub/epidemiology-R/data/banana_data/2_Ground survey data of banana Fusarium wilt/Ground_survey_data_of_banana_Fusarium_wilt.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 779548.9 ymin: 2560702 xmax: 780097 ymax: 2561020\nProjected CRS: WGS 84 / UTM zone 48N\n\n\nWhat we got?\n\nggg\n\nSimple feature collection with 80 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 779548.9 ymin: 2560702 xmax: 780097 ymax: 2561020\nProjected CRS: WGS 84 / UTM zone 48N\nFirst 10 features:\n   OBJECTID 样点类型   x_经度   y_纬度                 geometry\n1         1 健康植株 107.7326 23.13240 POINT (779838.5 2560800)\n2         2 健康植株 107.7332 23.13316 POINT (779901.2 2560885)\n3         3 健康植株 107.7334 23.13394 POINT (779920.1 2560971)\n4         4 健康植株 107.7326 23.13430 POINT (779837.5 2561010)\n5         5 健康植株 107.7302 23.13225 POINT (779595.2 2560779)\n6         6 健康植株 107.7301 23.13190 POINT (779584.6 2560739)\n7         7 健康植株 107.7300 23.13297 POINT (779569.6 2560857)\n8         8 健康植株 107.7315 23.13301 POINT (779729.4 2560865)\n9         9 健康植株 107.7313 23.13245 POINT (779710.5 2560803)\n10       10 健康植株 107.7349 23.13307 POINT (780078.9 2560879)\n\n\nNote that the attributes are in Chinese language. It seems that we will need to do several changes.\n\n\n8.4.5 Visualizing the data\nAs the orthomosaic is too heavy to visualize, we will need a coarser version of it. Let’s use the terra package for doing it.\n\nrrr8 &lt;- terra::aggregate(rrr, 8)\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n#terra &lt;- resample(elev, template, method='bilinear')\n\nLet’s check the output:\n\nrrr8\n\nclass       : SpatRaster \ndimensions  : 986, 1803, 5  (nrow, ncol, nlyr)\nresolution  : 0.64, 0.64  (x, y)\nextent      : 779257.9, 780411.8, 2560496, 2561127  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 48N (EPSG:32648) \nsource(s)   : memory\nnames       : UAV mu~ance_1, UAV mu~ance_2, UAV mu~ance_3, UAV mu~ance_4, UAV mu~ance_5 \nmin values  :      0.000000,      0.000000,      0.000000,      0.000000,      0.000000 \nmax values  :      1.272638,      1.119109,      1.075701,      0.949925,      1.069767 \n\n\nNote that the pixel size of the aggregated raster is 64 cm. Now, in order to visualize the ground points, we will need a color palette:\n\npal &lt;- colorFactor(\n  palette = c('green',  'red'),\n  domain = ggg$样点类型\n)\n\nThen, we will use the leaflet package to plot the new image and the ground points:\n\nleaflet(data = ggg) |&gt;\n  addProviderTiles(\"Esri.WorldImagery\") |&gt;\n  addRasterImage(rrr8) |&gt;\n  addCircleMarkers(~x_经度, ~y_纬度,\n    radius = 5,\n    label = ~样点类型,\n    fillColor = ~pal(样点类型),  \n    fillOpacity = 1,\n    stroke = F)\n\n\n\n\n\n\n\n8.4.6 Extracting image values at sampled points\nNow we will extract raster values at point locations using the st_extract() function from the {stars} library. It is expected that a value per band is extracted at each point.\nWe need to convert the raster object into a stars object:\n\nsss &lt;- st_as_stars(rrr)\n\nWhat we got?\n\nsss\n\nstars_proxy object with 1 attribute in 1 file(s):\n$`UAV multispectral reflectance.tif`\n[1] \"[...]/UAV multispectral reflectance.tif\"\n\ndimension(s):\n     from    to  offset delta                refsys point x/y\nx       1 14420  779258  0.08 WGS 84 / UTM zone 48N FALSE [x]\ny       1  7885 2561127 -0.08 WGS 84 / UTM zone 48N FALSE [y]\nband    1     5      NA    NA                    NA    NA    \n\n\nBefore conducting the extraction task, it is advisable to collect band values not at a single pixel but at a small window (e.g. 3x3 pixels). Thus, we will start creating 20cm buffers at each site:\n\npoly &lt;- st_buffer(ggg, dist = 0.20)\n\nNow, the extraction task:\n\n# Extract the median value per polygon\nbuf_values &lt;- aggregate(sss, poly, FUN = median) |&gt;\n  st_as_sf()\n\nWhat we got:\n\nbuf_values\n\nSimple feature collection with 80 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 779548.7 ymin: 2560702 xmax: 780097.2 ymax: 2561021\nProjected CRS: WGS 84 / UTM zone 48N\nFirst 10 features:\n   UAV multispectral reflectance.tif.V1 UAV multispectral reflectance.tif.V2\n1                             0.2837147                            0.3570867\n2                             0.3096394                            0.4598360\n3                             0.2652082                            0.3556182\n4                             0.2729177                            0.3626878\n5                             0.3033864                            0.3089822\n6                             0.2993084                            0.3655908\n7                             0.2471483                            0.2859972\n8                             0.2491679                            0.3223873\n9                             0.3213297                            0.4069843\n10                            0.2976966                            0.3430840\n   UAV multispectral reflectance.tif.V3 UAV multispectral reflectance.tif.V4\n1                             0.2049306                            0.6370987\n2                             0.2058005                            0.6609572\n3                             0.1769855                            0.5988070\n4                             0.1828043                            0.7094992\n5                             0.1894684                            0.6805743\n6                             0.1988409                            0.7750074\n7                             0.1880568                            0.5221797\n8                             0.1855947                            0.5934904\n9                             0.2013687                            0.5593004\n10                            0.2144508                            0.8057290\n   UAV multispectral reflectance.tif.V5                       geometry\n1                             0.4779058 POLYGON ((779838.7 2560800,...\n2                             0.5714727 POLYGON ((779901.4 2560885,...\n3                             0.4352523 POLYGON ((779920.3 2560971,...\n4                             0.5149931 POLYGON ((779837.7 2561010,...\n5                             0.4008912 POLYGON ((779595.4 2560779,...\n6                             0.4967270 POLYGON ((779584.8 2560739,...\n7                             0.3308629 POLYGON ((779569.8 2560857,...\n8                             0.4570443 POLYGON ((779729.6 2560865,...\n9                             0.5094757 POLYGON ((779710.7 2560803,...\n10                            0.5005793 POLYGON ((780079.1 2560879,...\n\n\nNote that names of bands are weird:\n\nnames(buf_values)\n\n[1] \"UAV multispectral reflectance.tif.V1\"\n[2] \"UAV multispectral reflectance.tif.V2\"\n[3] \"UAV multispectral reflectance.tif.V3\"\n[4] \"UAV multispectral reflectance.tif.V4\"\n[5] \"UAV multispectral reflectance.tif.V5\"\n[6] \"geometry\"                            \n\n\nLet’s rename band values:\n\nbuf_values |&gt; rename(blue = \"UAV multispectral reflectance.tif.V1\",\n                      red = \"UAV multispectral reflectance.tif.V2\",\n                      green = \"UAV multispectral reflectance.tif.V3\",\n                      redge = \"UAV multispectral reflectance.tif.V4\",\n                      nir = \"UAV multispectral reflectance.tif.V5\") -&gt; buf_values2\n\nNow, we got shorter names per band:\n\nbuf_values2\n\nSimple feature collection with 80 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 779548.7 ymin: 2560702 xmax: 780097.2 ymax: 2561021\nProjected CRS: WGS 84 / UTM zone 48N\nFirst 10 features:\n        blue       red     green     redge       nir\n1  0.2837147 0.3570867 0.2049306 0.6370987 0.4779058\n2  0.3096394 0.4598360 0.2058005 0.6609572 0.5714727\n3  0.2652082 0.3556182 0.1769855 0.5988070 0.4352523\n4  0.2729177 0.3626878 0.1828043 0.7094992 0.5149931\n5  0.3033864 0.3089822 0.1894684 0.6805743 0.4008912\n6  0.2993084 0.3655908 0.1988409 0.7750074 0.4967270\n7  0.2471483 0.2859972 0.1880568 0.5221797 0.3308629\n8  0.2491679 0.3223873 0.1855947 0.5934904 0.4570443\n9  0.3213297 0.4069843 0.2013687 0.5593004 0.5094757\n10 0.2976966 0.3430840 0.2144508 0.8057290 0.5005793\n                         geometry\n1  POLYGON ((779838.7 2560800,...\n2  POLYGON ((779901.4 2560885,...\n3  POLYGON ((779920.3 2560971,...\n4  POLYGON ((779837.7 2561010,...\n5  POLYGON ((779595.4 2560779,...\n6  POLYGON ((779584.8 2560739,...\n7  POLYGON ((779569.8 2560857,...\n8  POLYGON ((779729.6 2560865,...\n9  POLYGON ((779710.7 2560803,...\n10 POLYGON ((780079.1 2560879,...\n\n\n\n\n8.4.7 Computing vegetation indices\nYe et al. (2020) used the following indices Figure 8.11:\n\n\n\n\n\n\nFigure 8.11\n\n\n\nThus, we will compute several of those indices:\n\nbuf_indices &lt;-  buf_values2 |&gt; \n  mutate(ndvi = (nir - red) / (nir+red),\n         ndre = (nir - redge) / (nir+redge),\n          cire = (nir) / (redge-1),\n          sipi = (nir - blue) / (redge-red)\n  ) |&gt; select(ndvi, ndre, cire, sipi)\n\nWhat we got:\n\nbuf_indices\n\nSimple feature collection with 80 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 779548.7 ymin: 2560702 xmax: 780097.2 ymax: 2561021\nProjected CRS: WGS 84 / UTM zone 48N\nFirst 10 features:\n         ndvi        ndre       cire      sipi                       geometry\n1  0.14469485 -0.14277326 -1.3169030 0.6935100 POLYGON ((779838.7 2560800,...\n2  0.10824757 -0.07260820 -1.6855475 1.3018684 POLYGON ((779901.4 2560885,...\n3  0.10069173 -0.15816766 -1.0848951 0.6992264 POLYGON ((779920.3 2560971,...\n4  0.17353146 -0.15884638 -1.7727770 0.6980030 POLYGON ((779837.7 2561010,...\n5  0.12947229 -0.25861496 -1.2550373 0.2623973 POLYGON ((779595.4 2560779,...\n6  0.15207400 -0.21881961 -2.2077477 0.4821950 POLYGON ((779584.8 2560739,...\n7  0.07273232 -0.22427574 -0.6924421 0.3544487 POLYGON ((779569.8 2560857,...\n8  0.17276311 -0.12988254 -1.1243137 0.7667796 POLYGON ((779729.6 2560865,...\n9  0.11183404 -0.04661848 -1.1560613 1.2352333 POLYGON ((779710.7 2560803,...\n10 0.18668031 -0.23359699 -2.5767059 0.4385279 POLYGON ((780079.1 2560879,...\n\n\nNote that the health status is missing in buf_indices. Therefore, we will need to use a spatial join to link such status:\n\nsamples &lt;- st_join(\n  ggg,\n  buf_indices,\n  join = st_intersects)\n\nLet’s check the output:\n\nsamples\n\nSimple feature collection with 80 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 779548.9 ymin: 2560702 xmax: 780097 ymax: 2561020\nProjected CRS: WGS 84 / UTM zone 48N\nFirst 10 features:\n   OBJECTID 样点类型   x_经度   y_纬度       ndvi        ndre       cire\n1         1 健康植株 107.7326 23.13240 0.14469485 -0.14277326 -1.3169030\n2         2 健康植株 107.7332 23.13316 0.10824757 -0.07260820 -1.6855475\n3         3 健康植株 107.7334 23.13394 0.10069173 -0.15816766 -1.0848951\n4         4 健康植株 107.7326 23.13430 0.17353146 -0.15884638 -1.7727770\n5         5 健康植株 107.7302 23.13225 0.12947229 -0.25861496 -1.2550373\n6         6 健康植株 107.7301 23.13190 0.15207400 -0.21881961 -2.2077477\n7         7 健康植株 107.7300 23.13297 0.07273232 -0.22427574 -0.6924421\n8         8 健康植株 107.7315 23.13301 0.17276311 -0.12988254 -1.1243137\n9         9 健康植株 107.7313 23.13245 0.11183404 -0.04661848 -1.1560613\n10       10 健康植株 107.7349 23.13307 0.18668031 -0.23359699 -2.5767059\n        sipi                 geometry\n1  0.6935100 POINT (779838.5 2560800)\n2  1.3018684 POINT (779901.2 2560885)\n3  0.6992264 POINT (779920.1 2560971)\n4  0.6980030 POINT (779837.5 2561010)\n5  0.2623973 POINT (779595.2 2560779)\n6  0.4821950 POINT (779584.6 2560739)\n7  0.3544487 POINT (779569.6 2560857)\n8  0.7667796 POINT (779729.4 2560865)\n9  1.2352333 POINT (779710.5 2560803)\n10 0.4385279 POINT (780078.9 2560879)\n\n\nIt seems we succeeded.\n\nunique(samples$样点类型)\n\n[1] \"健康植株\"   \"枯萎病植株\"\n\n\nLet’s check it:\n\nsamples\n\nSimple feature collection with 80 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 779548.9 ymin: 2560702 xmax: 780097 ymax: 2561020\nProjected CRS: WGS 84 / UTM zone 48N\nFirst 10 features:\n   OBJECTID 样点类型   x_经度   y_纬度       ndvi        ndre       cire\n1         1 健康植株 107.7326 23.13240 0.14469485 -0.14277326 -1.3169030\n2         2 健康植株 107.7332 23.13316 0.10824757 -0.07260820 -1.6855475\n3         3 健康植株 107.7334 23.13394 0.10069173 -0.15816766 -1.0848951\n4         4 健康植株 107.7326 23.13430 0.17353146 -0.15884638 -1.7727770\n5         5 健康植株 107.7302 23.13225 0.12947229 -0.25861496 -1.2550373\n6         6 健康植株 107.7301 23.13190 0.15207400 -0.21881961 -2.2077477\n7         7 健康植株 107.7300 23.13297 0.07273232 -0.22427574 -0.6924421\n8         8 健康植株 107.7315 23.13301 0.17276311 -0.12988254 -1.1243137\n9         9 健康植株 107.7313 23.13245 0.11183404 -0.04661848 -1.1560613\n10       10 健康植株 107.7349 23.13307 0.18668031 -0.23359699 -2.5767059\n        sipi                 geometry\n1  0.6935100 POINT (779838.5 2560800)\n2  1.3018684 POINT (779901.2 2560885)\n3  0.6992264 POINT (779920.1 2560971)\n4  0.6980030 POINT (779837.5 2561010)\n5  0.2623973 POINT (779595.2 2560779)\n6  0.4821950 POINT (779584.6 2560739)\n7  0.3544487 POINT (779569.6 2560857)\n8  0.7667796 POINT (779729.4 2560865)\n9  1.2352333 POINT (779710.5 2560803)\n10 0.4385279 POINT (780078.9 2560879)\n\n\nNow, we will replace the Chinese words for English words:\n\nsamples |&gt; \n  mutate(score = ifelse(样点类型 == '健康植株', 'healthy', 'wilted')) |&gt;\n  rename(east = x_经度,\n         north = y_纬度 ) |&gt;\n  select(OBJECTID,score, ndvi, ndre, cire, sipi) -&gt; nsamples\n\nLet’s check the output:\n\nnsamples\n\nSimple feature collection with 80 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 779548.9 ymin: 2560702 xmax: 780097 ymax: 2561020\nProjected CRS: WGS 84 / UTM zone 48N\nFirst 10 features:\n   OBJECTID   score       ndvi        ndre       cire      sipi\n1         1 healthy 0.14469485 -0.14277326 -1.3169030 0.6935100\n2         2 healthy 0.10824757 -0.07260820 -1.6855475 1.3018684\n3         3 healthy 0.10069173 -0.15816766 -1.0848951 0.6992264\n4         4 healthy 0.17353146 -0.15884638 -1.7727770 0.6980030\n5         5 healthy 0.12947229 -0.25861496 -1.2550373 0.2623973\n6         6 healthy 0.15207400 -0.21881961 -2.2077477 0.4821950\n7         7 healthy 0.07273232 -0.22427574 -0.6924421 0.3544487\n8         8 healthy 0.17276311 -0.12988254 -1.1243137 0.7667796\n9         9 healthy 0.11183404 -0.04661848 -1.1560613 1.2352333\n10       10 healthy 0.18668031 -0.23359699 -2.5767059 0.4385279\n                   geometry\n1  POINT (779838.5 2560800)\n2  POINT (779901.2 2560885)\n3  POINT (779920.1 2560971)\n4  POINT (779837.5 2561010)\n5  POINT (779595.2 2560779)\n6  POINT (779584.6 2560739)\n7  POINT (779569.6 2560857)\n8  POINT (779729.4 2560865)\n9  POINT (779710.5 2560803)\n10 POINT (780078.9 2560879)\n\n\nAs we will not intend to use the geometry in our model, we can remove it:\n\nst_geometry(nsamples) &lt;- NULL\n\nLet’s check the output:\n\nnsamples\n\n   OBJECTID   score       ndvi         ndre       cire        sipi\n1         1 healthy 0.14469485 -0.142773261 -1.3169030   0.6935100\n2         2 healthy 0.10824757 -0.072608196 -1.6855475   1.3018684\n3         3 healthy 0.10069173 -0.158167658 -1.0848951   0.6992264\n4         4 healthy 0.17353146 -0.158846378 -1.7727770   0.6980030\n5         5 healthy 0.12947229 -0.258614958 -1.2550373   0.2623973\n6         6 healthy 0.15207400 -0.218819608 -2.2077477   0.4821950\n7         7 healthy 0.07273232 -0.224275739 -0.6924421   0.3544487\n8         8 healthy 0.17276311 -0.129882540 -1.1243137   0.7667796\n9         9 healthy 0.11183404 -0.046618476 -1.1560613   1.2352333\n10       10 healthy 0.18668031 -0.233596986 -2.5767059   0.4385279\n11       11 healthy 0.18657437 -0.228953927 -1.1653227   0.4062720\n12       12 healthy 0.08643499 -0.140911613 -0.8354715   0.6312134\n13       13 healthy 0.15674017 -0.196911688 -3.7608107   0.5867844\n14       14  wilted 0.11821422 -0.068648057 -0.8355714   1.0732255\n15       15  wilted 0.09439201  0.038426164 -1.6886927   3.9928701\n16       16  wilted 0.11179310  0.099664192 -1.2926303  19.6185081\n17       17  wilted 0.09110226 -0.012808011 -0.6207856   1.8658238\n18       18  wilted 0.05483186  0.059516988 -1.0249783 -53.7256928\n19       19  wilted 0.10296918  0.050902211 -0.5312504  -2.0705492\n20       20  wilted 0.09142250  0.166594498 -1.3738936  -4.5687561\n21       21  wilted 0.08282444  0.150849653 -1.0515823  -2.7751200\n22       22  wilted 0.09622230 -0.026842737 -1.8046361   1.7472896\n23       23  wilted 0.10804655  0.128742778 -3.1489130 -15.9373140\n24       24  wilted 0.11759462 -0.271995471 -0.7296435   0.3251888\n25       25  wilted 0.14612901 -0.170267124 -0.7535340   0.4340166\n26       26  wilted 0.14345746  0.007804645 -1.0917760   1.7195275\n27       27  wilted 0.09668988  0.048839732 -0.8969843   3.4761651\n28       28  wilted 0.16690104  0.009134113 -1.8204254   1.9231470\n29       29  wilted 0.12475996  0.018023671 -1.1985424   2.1736919\n30       30  wilted 0.12287255 -0.144575443 -0.7030899   0.5607103\n31       31  wilted 0.13276824  0.058621948 -1.4800599   3.5889606\n32       32  wilted 0.05301838 -0.056220022 -1.6759833   1.7895973\n33       33  wilted 0.14595352 -0.119623392 -1.4439231   0.8741458\n34       34  wilted 0.11236478 -0.015965114 -1.0488215   1.6813047\n35       35  wilted 0.20042051  0.007888690 -1.9802223   1.6851630\n36       36  wilted 0.14792880 -0.186300667 -0.7084196   0.3644668\n37       37  wilted 0.11381952  0.049444260 -1.1052035   3.7268769\n38       38  wilted 0.12729611  0.049169099 -0.6510980   1.2817601\n39       39  wilted 0.16311398 -0.075256088 -1.1007427   0.4779209\n40       40  wilted 0.12317018 -0.043296886 -0.7492018   0.9707563\n41       41  wilted 0.11177875  0.012897741 -1.8978005   2.8596823\n42       42  wilted 0.09087224  0.100019670 -1.0620017 -29.9002728\n43       43  wilted 0.15791341 -0.005372302 -1.5073174   1.7255063\n44       44  wilted 0.13393783 -0.023050553 -0.8915742   1.0855461\n45       45  wilted 0.06276064 -0.058164786 -1.0170161   1.4100601\n46       46  wilted 0.14991261 -0.080920969 -1.5173127   1.1947273\n47       47  wilted 0.15454129 -0.029567154 -1.4345890   1.8891207\n48       48  wilted 0.17448058  0.053204886 -0.9757240   2.8155912\n49       49 healthy 0.10767838 -0.279952871 -1.2310963   0.2743482\n50       50  wilted 0.11253491 -0.001355745 -1.0156727   2.0230335\n51       51 healthy 0.17156708 -0.130486894 -0.9670619   0.5846040\n52       52 healthy 0.11201780 -0.240080432 -1.5616428   0.3767992\n53       53 healthy 0.11357791 -0.233516246 -0.7266852   0.5174349\n54       54 healthy 0.09640389 -0.197173474 -1.6740870   0.6429254\n55       55 healthy 0.10371600 -0.148771702 -1.7079973   0.6404103\n56       56 healthy 0.13733091 -0.192621610 -1.0920985   0.5104957\n57       57 healthy 0.10494961 -0.141993366 -0.9767054   0.7876503\n58       58 healthy 0.09594242 -0.238072742 -1.2599388   0.4055777\n59       59 healthy 0.12253980 -0.228534207 -0.6240105   0.3125564\n60       60 healthy 0.11191907 -0.200302031 -1.6484365   0.4100274\n61       61 healthy 0.17497565 -0.348438178 -0.8327460   0.2617154\n62       62 healthy 0.06286945 -0.066868925 -1.9457173   0.9082508\n63       63 healthy 0.15875140 -0.180227754 -1.1609626   0.5244801\n64       64 healthy 0.12946220 -0.116778741 -1.1471938   0.6492517\n65       65 healthy 0.13029153 -0.251235648 -1.1507658   0.3096142\n66       66 healthy 0.18309513 -0.237002851 -2.1102911   0.4401729\n67       67 healthy 0.18231241 -0.191220292 -1.5998799   0.5043085\n68       68 healthy 0.12550657 -0.238273304 -0.8386376   0.3827526\n69       69 healthy 0.15353233 -0.215340798 -2.9878736   0.5652307\n70       70 healthy 0.17299827 -0.279709014 -1.4354651   0.3986933\n71       71 healthy 0.10020553 -0.181384162 -1.4309319   0.4048364\n72       72 healthy 0.16533571 -0.222465226 -1.4819059   0.4107726\n73       73 healthy 0.16081598  0.009196845 -1.1312141   2.2696678\n74       74 healthy 0.10260883 -0.288149676 -0.8375206   0.2317196\n75       75 healthy 0.19372839 -0.149982883 -2.4120161   0.7065984\n76       76 healthy 0.17650050 -0.249396644 -1.5425889   0.4227645\n77       77  wilted 0.13466946 -0.014630713 -1.1275266   1.7332531\n78       78  wilted 0.14542428  0.035346511 -1.3640391   2.9550260\n79       79  wilted 0.16581448  0.097763989 -1.0057000   4.8381045\n80       80  wilted 0.07985435  0.045756804 -1.3962020   7.5734216\n\n\nAs our task is a binary classification (i.e any site can be either healthy or wilted), the variable to estimate is a factor (not a character).\nLet’s change the data type of such variable:\n\nnsamples$score = as.factor(nsamples$score) \n\nLet’s check the result:\n\nnsamples\n\n   OBJECTID   score       ndvi         ndre       cire        sipi\n1         1 healthy 0.14469485 -0.142773261 -1.3169030   0.6935100\n2         2 healthy 0.10824757 -0.072608196 -1.6855475   1.3018684\n3         3 healthy 0.10069173 -0.158167658 -1.0848951   0.6992264\n4         4 healthy 0.17353146 -0.158846378 -1.7727770   0.6980030\n5         5 healthy 0.12947229 -0.258614958 -1.2550373   0.2623973\n6         6 healthy 0.15207400 -0.218819608 -2.2077477   0.4821950\n7         7 healthy 0.07273232 -0.224275739 -0.6924421   0.3544487\n8         8 healthy 0.17276311 -0.129882540 -1.1243137   0.7667796\n9         9 healthy 0.11183404 -0.046618476 -1.1560613   1.2352333\n10       10 healthy 0.18668031 -0.233596986 -2.5767059   0.4385279\n11       11 healthy 0.18657437 -0.228953927 -1.1653227   0.4062720\n12       12 healthy 0.08643499 -0.140911613 -0.8354715   0.6312134\n13       13 healthy 0.15674017 -0.196911688 -3.7608107   0.5867844\n14       14  wilted 0.11821422 -0.068648057 -0.8355714   1.0732255\n15       15  wilted 0.09439201  0.038426164 -1.6886927   3.9928701\n16       16  wilted 0.11179310  0.099664192 -1.2926303  19.6185081\n17       17  wilted 0.09110226 -0.012808011 -0.6207856   1.8658238\n18       18  wilted 0.05483186  0.059516988 -1.0249783 -53.7256928\n19       19  wilted 0.10296918  0.050902211 -0.5312504  -2.0705492\n20       20  wilted 0.09142250  0.166594498 -1.3738936  -4.5687561\n21       21  wilted 0.08282444  0.150849653 -1.0515823  -2.7751200\n22       22  wilted 0.09622230 -0.026842737 -1.8046361   1.7472896\n23       23  wilted 0.10804655  0.128742778 -3.1489130 -15.9373140\n24       24  wilted 0.11759462 -0.271995471 -0.7296435   0.3251888\n25       25  wilted 0.14612901 -0.170267124 -0.7535340   0.4340166\n26       26  wilted 0.14345746  0.007804645 -1.0917760   1.7195275\n27       27  wilted 0.09668988  0.048839732 -0.8969843   3.4761651\n28       28  wilted 0.16690104  0.009134113 -1.8204254   1.9231470\n29       29  wilted 0.12475996  0.018023671 -1.1985424   2.1736919\n30       30  wilted 0.12287255 -0.144575443 -0.7030899   0.5607103\n31       31  wilted 0.13276824  0.058621948 -1.4800599   3.5889606\n32       32  wilted 0.05301838 -0.056220022 -1.6759833   1.7895973\n33       33  wilted 0.14595352 -0.119623392 -1.4439231   0.8741458\n34       34  wilted 0.11236478 -0.015965114 -1.0488215   1.6813047\n35       35  wilted 0.20042051  0.007888690 -1.9802223   1.6851630\n36       36  wilted 0.14792880 -0.186300667 -0.7084196   0.3644668\n37       37  wilted 0.11381952  0.049444260 -1.1052035   3.7268769\n38       38  wilted 0.12729611  0.049169099 -0.6510980   1.2817601\n39       39  wilted 0.16311398 -0.075256088 -1.1007427   0.4779209\n40       40  wilted 0.12317018 -0.043296886 -0.7492018   0.9707563\n41       41  wilted 0.11177875  0.012897741 -1.8978005   2.8596823\n42       42  wilted 0.09087224  0.100019670 -1.0620017 -29.9002728\n43       43  wilted 0.15791341 -0.005372302 -1.5073174   1.7255063\n44       44  wilted 0.13393783 -0.023050553 -0.8915742   1.0855461\n45       45  wilted 0.06276064 -0.058164786 -1.0170161   1.4100601\n46       46  wilted 0.14991261 -0.080920969 -1.5173127   1.1947273\n47       47  wilted 0.15454129 -0.029567154 -1.4345890   1.8891207\n48       48  wilted 0.17448058  0.053204886 -0.9757240   2.8155912\n49       49 healthy 0.10767838 -0.279952871 -1.2310963   0.2743482\n50       50  wilted 0.11253491 -0.001355745 -1.0156727   2.0230335\n51       51 healthy 0.17156708 -0.130486894 -0.9670619   0.5846040\n52       52 healthy 0.11201780 -0.240080432 -1.5616428   0.3767992\n53       53 healthy 0.11357791 -0.233516246 -0.7266852   0.5174349\n54       54 healthy 0.09640389 -0.197173474 -1.6740870   0.6429254\n55       55 healthy 0.10371600 -0.148771702 -1.7079973   0.6404103\n56       56 healthy 0.13733091 -0.192621610 -1.0920985   0.5104957\n57       57 healthy 0.10494961 -0.141993366 -0.9767054   0.7876503\n58       58 healthy 0.09594242 -0.238072742 -1.2599388   0.4055777\n59       59 healthy 0.12253980 -0.228534207 -0.6240105   0.3125564\n60       60 healthy 0.11191907 -0.200302031 -1.6484365   0.4100274\n61       61 healthy 0.17497565 -0.348438178 -0.8327460   0.2617154\n62       62 healthy 0.06286945 -0.066868925 -1.9457173   0.9082508\n63       63 healthy 0.15875140 -0.180227754 -1.1609626   0.5244801\n64       64 healthy 0.12946220 -0.116778741 -1.1471938   0.6492517\n65       65 healthy 0.13029153 -0.251235648 -1.1507658   0.3096142\n66       66 healthy 0.18309513 -0.237002851 -2.1102911   0.4401729\n67       67 healthy 0.18231241 -0.191220292 -1.5998799   0.5043085\n68       68 healthy 0.12550657 -0.238273304 -0.8386376   0.3827526\n69       69 healthy 0.15353233 -0.215340798 -2.9878736   0.5652307\n70       70 healthy 0.17299827 -0.279709014 -1.4354651   0.3986933\n71       71 healthy 0.10020553 -0.181384162 -1.4309319   0.4048364\n72       72 healthy 0.16533571 -0.222465226 -1.4819059   0.4107726\n73       73 healthy 0.16081598  0.009196845 -1.1312141   2.2696678\n74       74 healthy 0.10260883 -0.288149676 -0.8375206   0.2317196\n75       75 healthy 0.19372839 -0.149982883 -2.4120161   0.7065984\n76       76 healthy 0.17650050 -0.249396644 -1.5425889   0.4227645\n77       77  wilted 0.13466946 -0.014630713 -1.1275266   1.7332531\n78       78  wilted 0.14542428  0.035346511 -1.3640391   2.9550260\n79       79  wilted 0.16581448  0.097763989 -1.0057000   4.8381045\n80       80  wilted 0.07985435  0.045756804 -1.3962020   7.5734216\n\n\nA simple summary of the extracted data can be useful:\n\nnsamples |&gt;\n  group_by(score) |&gt;\n  summarize(n())\n\n# A tibble: 2 × 2\n  score   `n()`\n  &lt;fct&gt;   &lt;int&gt;\n1 healthy    40\n2 wilted     40\n\n\nThis mean the dataset is balanced which is very good.\n\n\n8.4.8 Saving the extracted dataset\nNow, let’s save the nsamples object. Just in case R crashes due to lack of memory.\n\n#uncomment if needed\n#st_write(nsamples, \"./banana_data/nsamples.csv\", overwrite=TRUE)\n\n\n\n8.4.9 Classification of Fusarium wilt using machine learning (ML)\nThe overall process to classify the crop disease under study will be conducted using the tidymodels framework which is an extension of the tidyverse suite. It is especially focused towards providing a generalized way to define, run and optimize ML models in R.\n\n8.4.9.1 Exploratory analysis\nAs a first step in modeling, it’s always a good idea to visualize the data. Let’s start with a boxplot to displays the distribution of a vegetation index. It visualizes five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually.\n\np &lt;- ggplot(nsamples, aes(score, ndre))+\n  r4pde::theme_r4pde()\n\nWarning: replacing previous import 'car::recode' by 'dplyr::recode' when\nloading 'r4pde'\n\np + geom_boxplot()\n\n\n\n\n\n\n\n\nNext, we will do a scatterplot to visualize the indices NDRE and CIRE:\n\nggplot(nsamples) +\n  aes(x = ndre, y = cire, color = score) +\n  geom_point(shape = 16, size = 4) +\n  labs(x = \"NDRE\", y = \"CIRI\") +\n  r4pde::theme_r4pde() +\n  scale_color_manual(values = c(\"#71b075\", \"#ba0600\"))\n\n\n\n\n\n\n\n\n\n\n8.4.9.2 Splitting the data\nNext step is to divide the data into a training and a test set. The set.seed() function can be used for reproducibility of the computations that are dependent on random numbers. By default, the training/testing split is 0.75 to 0.25.\n\nset.seed(42)\ndata_split &lt;- initial_split(data = nsamples)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\nLet’s check the result:\n\ndata_train\n\n   OBJECTID   score       ndvi         ndre       cire        sipi\n1        49 healthy 0.10767838 -0.279952871 -1.2310963   0.2743482\n2        65 healthy 0.13029153 -0.251235648 -1.1507658   0.3096142\n3        25  wilted 0.14612901 -0.170267124 -0.7535340   0.4340166\n4        74 healthy 0.10260883 -0.288149676 -0.8375206   0.2317196\n5        18  wilted 0.05483186  0.059516988 -1.0249783 -53.7256928\n6        80  wilted 0.07985435  0.045756804 -1.3962020   7.5734216\n7        47  wilted 0.15454129 -0.029567154 -1.4345890   1.8891207\n8        24  wilted 0.11759462 -0.271995471 -0.7296435   0.3251888\n9        71 healthy 0.10020553 -0.181384162 -1.4309319   0.4048364\n10       37  wilted 0.11381952  0.049444260 -1.1052035   3.7268769\n11       20  wilted 0.09142250  0.166594498 -1.3738936  -4.5687561\n12       26  wilted 0.14345746  0.007804645 -1.0917760   1.7195275\n13        3 healthy 0.10069173 -0.158167658 -1.0848951   0.6992264\n14       41  wilted 0.11177875  0.012897741 -1.8978005   2.8596823\n15       27  wilted 0.09668988  0.048839732 -0.8969843   3.4761651\n16       36  wilted 0.14792880 -0.186300667 -0.7084196   0.3644668\n17       72 healthy 0.16533571 -0.222465226 -1.4819059   0.4107726\n18       31  wilted 0.13276824  0.058621948 -1.4800599   3.5889606\n19       45  wilted 0.06276064 -0.058164786 -1.0170161   1.4100601\n20        5 healthy 0.12947229 -0.258614958 -1.2550373   0.2623973\n21       70 healthy 0.17299827 -0.279709014 -1.4354651   0.3986933\n22       34  wilted 0.11236478 -0.015965114 -1.0488215   1.6813047\n23       28  wilted 0.16690104  0.009134113 -1.8204254   1.9231470\n24       40  wilted 0.12317018 -0.043296886 -0.7492018   0.9707563\n25       68 healthy 0.12550657 -0.238273304 -0.8386376   0.3827526\n26       33  wilted 0.14595352 -0.119623392 -1.4439231   0.8741458\n27       42  wilted 0.09087224  0.100019670 -1.0620017 -29.9002728\n28       73 healthy 0.16081598  0.009196845 -1.1312141   2.2696678\n29       30  wilted 0.12287255 -0.144575443 -0.7030899   0.5607103\n30       43  wilted 0.15791341 -0.005372302 -1.5073174   1.7255063\n31       15  wilted 0.09439201  0.038426164 -1.6886927   3.9928701\n32       22  wilted 0.09622230 -0.026842737 -1.8046361   1.7472896\n33        8 healthy 0.17276311 -0.129882540 -1.1243137   0.7667796\n34       79  wilted 0.16581448  0.097763989 -1.0057000   4.8381045\n35        4 healthy 0.17353146 -0.158846378 -1.7727770   0.6980030\n36       75 healthy 0.19372839 -0.149982883 -2.4120161   0.7065984\n37       76 healthy 0.17650050 -0.249396644 -1.5425889   0.4227645\n38       58 healthy 0.09594242 -0.238072742 -1.2599388   0.4055777\n39       61 healthy 0.17497565 -0.348438178 -0.8327460   0.2617154\n40       46  wilted 0.14991261 -0.080920969 -1.5173127   1.1947273\n41       59 healthy 0.12253980 -0.228534207 -0.6240105   0.3125564\n42       35  wilted 0.20042051  0.007888690 -1.9802223   1.6851630\n43       53 healthy 0.11357791 -0.233516246 -0.7266852   0.5174349\n44       23  wilted 0.10804655  0.128742778 -3.1489130 -15.9373140\n45       69 healthy 0.15353233 -0.215340798 -2.9878736   0.5652307\n46        6 healthy 0.15207400 -0.218819608 -2.2077477   0.4821950\n47       39  wilted 0.16311398 -0.075256088 -1.1007427   0.4779209\n48        2 healthy 0.10824757 -0.072608196 -1.6855475   1.3018684\n49       60 healthy 0.11191907 -0.200302031 -1.6484365   0.4100274\n50       56 healthy 0.13733091 -0.192621610 -1.0920985   0.5104957\n51       62 healthy 0.06286945 -0.066868925 -1.9457173   0.9082508\n52       21  wilted 0.08282444  0.150849653 -1.0515823  -2.7751200\n53       55 healthy 0.10371600 -0.148771702 -1.7079973   0.6404103\n54       64 healthy 0.12946220 -0.116778741 -1.1471938   0.6492517\n55       57 healthy 0.10494961 -0.141993366 -0.9767054   0.7876503\n56       10 healthy 0.18668031 -0.233596986 -2.5767059   0.4385279\n57       48  wilted 0.17448058  0.053204886 -0.9757240   2.8155912\n58       54 healthy 0.09640389 -0.197173474 -1.6740870   0.6429254\n59        1 healthy 0.14469485 -0.142773261 -1.3169030   0.6935100\n60       17  wilted 0.09110226 -0.012808011 -0.6207856   1.8658238\n\n\n\n\n8.4.9.3 Defining the model\nWe will use a logistic regression which is a simple model. It may be useful to have a look at this explanation of such a model.\n\nspec_lr &lt;-\nlogistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  set_mode(\"classification\")\n\n\n\n8.4.9.4 Defining the recipe\nThe recipe() function to be used here has two arguments:\n\nA formula. Any variable on the left-hand side of the tilde (~) is considered the model outcome (here, outcome). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (.) to indicate all other variables as predictors.\nThe data. A recipe is associated with the data set used to create the model. This will typically be the training set, so data = data_train here.\n\n\nrecipe_lr &lt;-\n  recipe(score ~ ., data_train) |&gt;\n  add_role(OBJECTID, new_role = \"id\") |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_corr(all_predictors())\n\n\n\n8.4.9.5 Evaluating model performance\nNext, we need to specify what we would like to see for determining the performance of the model. Different modelling algorithms have different types of metrics. Because we have a binary classification problem (healthy vs. wilted classification), we will chose the AUC - ROC evaluation metric here.\n\n\n8.4.9.6 Combining model and recipe into a workflow\nWe will want to use our recipe across several steps as we train and test our model. We will:\n\nProcess the recipe using the training set: This involves any estimation or calculations based on the training set. For our recipe, the training set will be used to determine which predictors should be converted to dummy variables and which predictors will have zero-variance in the training set, and should be slated for removal.\nApply the recipe to the training set: We create the final predictor set on the training set.\nApply the recipe to the test set: We create the final predictor set on the test set. Nothing is recomputed and no information from the test set is used here; the dummy variable and zero-variance results from the training set are applied to the test set.\n\nTo simplify this process, we can use a model workflow, which pairs a model and recipe together. This is a straightforward approach because different recipes are often needed for different models, so when a model and recipe are bundled, it becomes easier to train and test workflows. We’ll use the workflows package from tidymodels to bundle our model with our recipe.\nNow we are ready to setup our complete modelling workflow. This workflow contains the model specification and the recipe.\n\nwf_bana_wilt &lt;-\n  workflow(\n    spec = spec_lr,\n    recipe_lr\n    )\n\nwf_bana_wilt\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_zv()\n• step_corr()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n8.4.9.7 Fitting the logistic regression model\nNow we use the workflow previously created to fit the model on our training data. We use the training partition of the data.\n\nfit_lr &lt;- wf_bana_wilt  |&gt; \n  fit(data = data_train)\n\nLet’s check the output:\n\nfit_lr\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_zv()\n• step_corr()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)     OBJECTID         ndvi         ndre         cire         sipi  \n    7.64191     -0.02527      4.33597     27.50498      3.19965     -0.11597  \n\nDegrees of Freedom: 59 Total (i.e. Null);  54 Residual\nNull Deviance:      83.18 \nResidual Deviance: 32.8     AIC: 44.8\n\n\nNow, we will use the fitted model to estimate health status in the training data:\n\nrf_training_pred &lt;- \n  predict(fit_lr, data_train) |&gt; \n  bind_cols(predict(fit_lr, data_train, type = \"prob\")) |&gt; \n  # Add the true outcome data back in\n  bind_cols(data_train |&gt; \n              select(score))\n\nWhat we got?\n\nrf_training_pred\n\n# A tibble: 60 × 4\n   .pred_class .pred_healthy .pred_wilted score  \n   &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;  \n 1 healthy        0.992           0.00816 healthy\n 2 healthy        0.983           0.0169  healthy\n 3 wilted         0.378           0.622   wilted \n 4 healthy        0.988           0.0119  healthy\n 5 wilted         0.00000607      1.00    wilted \n 6 wilted         0.132           0.868   wilted \n 7 wilted         0.182           0.818   wilted \n 8 healthy        0.910           0.0905  wilted \n 9 healthy        0.966           0.0345  healthy\n10 wilted         0.0100          0.990   wilted \n# ℹ 50 more rows\n\n\nLet’s estimate the training accuracy:\n\nrf_training_pred |&gt; # training set predictions\n  accuracy(truth = score, .pred_class) -&gt; acc_train\nacc_train\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary          0.85\n\n\nThe accuracy of the model on the training data is 0.85 which is above 0.5 (mere chance). This basically means that the model was able to learn predictive patterns from the training data. To see if the model is able to generalize what it learned when exposed to new data, we evaluate the model on our hold-out (or so-called test data). We created a test dataset when splitting the data at the start of the modelling.\n\n\n8.4.9.8 Evaluating the model on test data\nNow, we will use the fitted model to estimate health status in the testing data:\n\nlr_testing_pred &lt;- \n  predict(fit_lr, data_test) |&gt; \n  bind_cols(predict(fit_lr, data_test, type = \"prob\")) |&gt; \n  bind_cols(data_test |&gt; select(score))\n\nWhat we got:\n\nlr_testing_pred\n\n# A tibble: 20 × 4\n   .pred_class .pred_healthy .pred_wilted score  \n   &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;  \n 1 healthy          0.656       0.344     healthy\n 2 wilted           0.0587      0.941     healthy\n 3 healthy          0.870       0.130     healthy\n 4 wilted           0.251       0.749     healthy\n 5 healthy          1.00        0.0000730 healthy\n 6 wilted           0.0425      0.957     wilted \n 7 wilted           0.0171      0.983     wilted \n 8 wilted           0.000527    0.999     wilted \n 9 wilted           0.0207      0.979     wilted \n10 healthy          0.513       0.487     wilted \n11 wilted           0.00174     0.998     wilted \n12 wilted           0.0294      0.971     wilted \n13 wilted           0.0341      0.966     wilted \n14 wilted           0.414       0.586     healthy\n15 healthy          0.992       0.00792   healthy\n16 healthy          0.880       0.120     healthy\n17 healthy          0.999       0.00142   healthy\n18 healthy          0.976       0.0242    healthy\n19 wilted           0.112       0.888     wilted \n20 wilted           0.0713      0.929     wilted \n\n\nLet’s compute the testing accuracy:\n\nlr_testing_pred |&gt;                   # test set predictions\n  accuracy(score, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary           0.8\n\n\nThe resulting accuracy is similar to the accuracy on the training data. It is good for a first go and a relatively simple classification model.\n\n## Let's plot the AUC-ROC \nlr_testing_pred |&gt; \n  roc_curve(truth = score, .pred_wilted, event_level=\"second\") |&gt; \n  mutate(model = \"Logistic Regression\") |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\n8.4.10 Conclusions\nIn this section, we trained and tested a logistic regression model (LGM) using four spectral indices as predictor variables (i.e. NDVI, NDRE, CIRE and SIPI). Compare this section results, in terms of equation and accuracy, with the individual LGMs tested by [Ye et al. (2020)]Figure 8.12.\n\n\n\n\n\n\nFigure 8.12\n\n\n\nNote that we have not tested other ML algorithms. But there are a lot of them available from the tidymodels framework (e.g. random forests, support vector machines, gradient boosting machines).\nTo conclude, this section illustrated how to use VIs derived from UAV-based multispectral imagery and ground data to develop an identification model for detecting banana Fusarium wilt. The results showed that a simple logistic regression model is able to identify Fusarium wilt of banana from several VIs with a good accuracy. However, before going too optimistic, I would suggest to study the Ye et al. (2020) paper and critically evaluate their experiment design, methods and results.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Remote sensing</span>"
    ]
  },
  {
    "objectID": "data-remote-sensing.html#disease-quantification",
    "href": "data-remote-sensing.html#disease-quantification",
    "title": "8  Remote sensing",
    "section": "8.5 Disease quantification",
    "text": "8.5 Disease quantification\n\n8.5.1 Introduction\nThis section illustrates the use of UAV multispecral imagery for estimating the severity of cercospora leaf spot (CLS) disease in table beet, the most destructive fungal disease of table beet (Skaracis et al. 2010; Tan et al. 2023). The CLS disease causes rapid defoliation and significant crop loss may occur through the inability to harvest with top-pulling machinery. It may also render the produce unsaleable for the fresh market (Skaracis et al. 2010).\nSaif et al. (2024) recently published on Mendeley data UAV imagery & ground truth CLS data collected a several table beef plots at Cornell AgriTech, Geneva, New York, USA. Note that, in this study, UAV multispectral images were collected using a Micasense Red Edge camera, similar to the one used in the case study for the detection of Fusarium wilt on banana. I have not found any scientific paper estimating CLS severity from this dataset. However, in Saif et al. (2023), a similar dataset was used to forecast table beet root yield at Cornell Agritech. While it is just a guess, it is worth visualizing the plots in the latter study.\n\n\n\n\n\n\nFigure 8.13: Color mosaic of third flight hyperspectral image. The visualization bands are the following: red (639 nm), green (550 nm), and blue (470 nm). The crops in the red boxes in the figure were the plots under study. Source: (Saif et al. 2023)\n\n\n\nThis section aims at estimating CLS leaf severity using vegetation indices (VIs) derived from the multispectral UAV imagery as spectral covariates. The section comprises two parts: (i) Part 1 creates a spectral covariate table to summarize the complete UAV imagery & ground truth data; and (ii) Part 2 trains and tests a machine learning model to estimate CLS severity.\n\n\n8.5.2 CLS severity on leaves\nIt is very important to visualize how different levels of table beet CLS disease severity look in RGB color leaf images. The figure below is a standard area diagram set (SADs) that is used by raters during the assessment of visual severity to increase their accuracy and reliability of the estimates.\n\n\n\n\n\n\nFigure 8.14: Standard area diagram set for the severity of table beet cercospora leaf spot. Source: (Del Ponte et al. 2019)\n\n\n\n\n\n8.5.3 Software setup\nLet’s start by cleaning up R memory:\n\nrm(list=ls())\n\nThen, we need to install several packages (if they are not installed yet):\n\nlist.of.packages &lt;- c(\"readr\",\"terra\", \"tidyterra\", \"stars\", \"sf\", \"leaflet\", \"leafem\", \"dplyr\", \"ggplot2\", \"tidymodels\")\nnew.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\nif(length(new.packages)) install.packages(new.packages)\n\nNow, let’s load all the required packages:\n\nlibrary(readr)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(stars)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidymodels)\n\n\n\n8.5.4 Read the dataset\nThe following code assumes you have already downloaded the dataset and unzipped its content under the data/cercospora_data directory. What files are in that folder?\n\nlist.files(\"data/cercospora_data\")\n\n [1] \"CLS_DS.csv\"          \"D0_2021.csv\"         \"D1_2021.csv\"        \n [4] \"D2_2021.csv\"         \"D3_2021.csv\"         \"D4_2021.csv\"        \n [7] \"fcovar_2021.csv\"     \"multispec_2021_2022\" \"multispec_2023\"     \n[10] \"ncovar_2021.csv\"     \"README.md\"          \n\n\nNote that there is one CLS_DS.csv file with the CLS ground data and two folders with the UAV multispectral images. We can infer that 2021, 2022, 2023 refer to the image acquisition years. At this point, it is very important to check the README file. I will summarize the following points:\n\n8.5.4.1 File naming convention\nEach image file is named according to the plot number and the date of capture, using the format plt_rYYYYMMDD, where:\n\n‘plt’ stands for the plot number.\n‘YYYYMMDD’ represents the date on which the image was captured (year, month, day).\n\nFor example, the file name 5_r20210715 corresponds to an image taken on July 15, 2021, from study plot 5.\n\n\n8.5.4.2 CLS Severity Data\nFile named CLS_DS contain visual assessments of CLS disease severity noted for each plot.\n\n\n\n8.5.5 Inspect the format of each file\nLet’s list the first images under an image folder:\n\nlist.files(\"data/cercospora_data/multispec_2021_2022/\")[1:15]\n\n [1] \"1_r20210707.tif\"         \"1_r20210707.tif.aux.xml\"\n [3] \"1_r20210715.tif\"         \"1_r20210720.tif\"        \n [5] \"1_r20210802.tif\"         \"1_r20210825.tif\"        \n [7] \"1_r20210825.tif.aux.xml\" \"1_r20220707.tif\"        \n [9] \"1_r20220715.tif\"         \"1_r20220726.tif\"        \n[11] \"1_r20220810.tif\"         \"1_r20220818.tif\"        \n[13] \"10_r20210707.tif\"        \"10_r20210715.tif\"       \n[15] \"10_r20210720.tif\"       \n\n\nNote that, for 2021, there are six dates of image acquisition: 20210707, 20210715, 20210720,20210726, 20210802,20210825\n\n\n8.5.6 Read the orthomosaics\nNow, let’s read several plot images using the terra package:\n\n# Open a tif collected on study plot 5 on  7th July 2021\ntif1 &lt;- \"data/cercospora_data/multispec_2021_2022/1_r20210707.tif\"\ntif2 &lt;- \"data/cercospora_data/multispec_2021_2022/2_r20210707.tif\"\ntif3 &lt;- \"data/cercospora_data/multispec_2021_2022/3_r20210707.tif\"\ntif4 &lt;- \"data/cercospora_data/multispec_2021_2022/4_r20210707.tif\"\ntif5 &lt;- \"data/cercospora_data/multispec_2021_2022/5_r20210707.tif\"\ntif6 &lt;- \"data/cercospora_data/multispec_2021_2022/6_r20210707.tif\"\ntif7 &lt;- \"data/cercospora_data/multispec_2021_2022/7_r20210707.tif\"\ntif8 &lt;- \"data/cercospora_data/multispec_2021_2022/8_r20210707.tif\"\ntif9 &lt;- \"data/cercospora_data/multispec_2021_2022/9_r20210707.tif\"\ntif10 &lt;- \"data/cercospora_data/multispec_2021_2022/10_r20210707.tif\"\n\nIt may be convenient to increase image pixel size (to make the raster lighter):\n\np01 &lt;- terra::rast(tif1) %&gt;% aggregate(20)\np02 &lt;- terra::rast(tif2) %&gt;% aggregate(20)\np03 &lt;- terra::rast(tif3) %&gt;% aggregate(20)\np04 &lt;- terra::rast(tif4) %&gt;% aggregate(20)\np05 &lt;- terra::rast(tif5) %&gt;% aggregate(20)\np06 &lt;- terra::rast(tif6) %&gt;% aggregate(20)\np07 &lt;- terra::rast(tif7) %&gt;% aggregate(20)\np08 &lt;- terra::rast(tif8) %&gt;% aggregate(20)\np09 &lt;- terra::rast(tif9) %&gt;% aggregate(20)\np10 &lt;- terra::rast(tif10) %&gt;% aggregate(20)\n\nLet’s check what we get:\n\np01\n\nclass       : SpatRaster \ndimensions  : 15, 9, 5  (nrow, ncol, nlyr)\nresolution  : 0.212, 0.212  (x, y)\nextent      : 334196.9, 334198.8, 4748990, 4748993  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 18N (EPSG:32618) \nsource(s)   : memory\nnames       : 1_r20210707_1, 1_r20210707_2, 1_r20210707_3, 1_r20210707_4, 1_r20210707_5 \nmin values  :    0.01288329,    0.02743819,    0.01721323,    0.06647335,     0.1167282 \nmax values  :    0.03016389,    0.06923942,    0.07651012,    0.18791453,     0.4155874 \n\n\nNote that each image has 5 bands with ~21 cm pixel size.\nAs the images have been split in plots, it may be useful to “mosaic” them.\n\n# with many SpatRasters, make a SpatRasterCollection from a list\nrlist &lt;- list(p01, p02, p03, p04, p05, p06, p07, p08, p09, p10)\nrsrc &lt;- sprc(rlist)\n\nm &lt;- merge(rsrc)\n\nWhat we got?\n\nm\n\nclass       : SpatRaster \ndimensions  : 209, 24, 5  (nrow, ncol, nlyr)\nresolution  : 0.212, 0.212  (x, y)\nextent      : 334193.8, 334198.9, 4748990, 4749035  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 18N (EPSG:32618) \nsource(s)   : memory\nnames       : 1_r20210707_1, 1_r20210707_2, 1_r20210707_3, 1_r20210707_4, 1_r20210707_5 \nmin values  :    0.01390121,    0.02954650,    0.01960831,     0.0706145,     0.1283448 \nmax values  :    0.03741143,    0.08793678,    0.08386827,     0.2208381,     0.5030925 \n\n\nLet’s know band names:\n\nnames(m)\n\n[1] \"1_r20210707_1\" \"1_r20210707_2\" \"1_r20210707_3\" \"1_r20210707_4\"\n[5] \"1_r20210707_5\"\n\n\nWe will rename band names:\n\n# Rename\nm2 &lt;- m %&gt;%\n  rename(blue = \"1_r20210707_1\", green = \"1_r20210707_2\", \n         red = \"1_r20210707_3\", redge= \"1_r20210707_4\",\n         nir = \"1_r20210707_5\")\n\nLet’s get a summary of the mosaic:\n\nsummary(m2)\n\n      blue           green            red            redge      \n Min.   :0.014   Min.   :0.030   Min.   :0.020   Min.   :0.071  \n 1st Qu.:0.019   1st Qu.:0.052   1st Qu.:0.029   1st Qu.:0.113  \n Median :0.022   Median :0.058   Median :0.041   Median :0.138  \n Mean   :0.023   Mean   :0.058   Mean   :0.043   Mean   :0.139  \n 3rd Qu.:0.026   3rd Qu.:0.064   3rd Qu.:0.056   3rd Qu.:0.164  \n Max.   :0.037   Max.   :0.088   Max.   :0.084   Max.   :0.221  \n NA's   :3896    NA's   :3896    NA's   :3896    NA's   :3896   \n      nir       \n Min.   :0.128  \n 1st Qu.:0.216  \n Median :0.291  \n Mean   :0.298  \n 3rd Qu.:0.377  \n Max.   :0.503  \n NA's   :3896   \n\n\n\nhist(m2)\n\n\n\n\n\n\n\n\nLet’s visualize the mosaic:\n\nggplot() +\n  geom_spatraster(data = m2) +\n  facet_wrap(~lyr, ncol = 5) +\n   r4pde::theme_r4pde(font_size = 10)+\n  scale_fill_whitebox_c(\n    palette = \"muted\",\n    labels = scales::label_number(suffix = \"%\"),\n    n.breaks = 10,\n    guide = guide_legend(reverse = TRUE)\n  ) +\n  labs(\n    fill = \"\",\n    title = \"UAV multispectral mosaic\",\n    subtitle = \"07.07.2021\"\n  )\n\nWarning: replacing previous import 'car::recode' by 'dplyr::recode' when\nloading 'r4pde'\n\n\n\n\n\n\n\n\n\nLet’s read all images collected at a later date:\n\n# Open all images collected on  25th August 2021\ntif1 &lt;- \"data/cercospora_data/multispec_2021_2022/1_r20210825.tif\"\ntif2 &lt;- \"data/cercospora_data/multispec_2021_2022/2_r20210825.tif\"\ntif3 &lt;- \"data/cercospora_data/multispec_2021_2022/3_r20210825.tif\"\ntif4 &lt;- \"data/cercospora_data/multispec_2021_2022/4_r20210825.tif\"\ntif5 &lt;- \"data/cercospora_data/multispec_2021_2022/5_r20210825.tif\"\ntif6 &lt;- \"data/cercospora_data/multispec_2021_2022/6_r20210825.tif\"\ntif7 &lt;- \"data/cercospora_data/multispec_2021_2022/7_r20210825.tif\"\ntif8 &lt;- \"data/cercospora_data/multispec_2021_2022/8_r20210825.tif\"\ntif9 &lt;- \"data/cercospora_data/multispec_2021_2022/9_r20210825.tif\"\ntif10 &lt;- \"data/cercospora_data/multispec_2021_2022/10_r20210825.tif\"\n\n\np01 &lt;- terra::rast(tif1) %&gt;% aggregate(20)\np02 &lt;- terra::rast(tif2) %&gt;% aggregate(20)\np03 &lt;- terra::rast(tif3) %&gt;% aggregate(20)\np04 &lt;- terra::rast(tif4) %&gt;% aggregate(20)\np05 &lt;- terra::rast(tif5) %&gt;% aggregate(20)\np06 &lt;- terra::rast(tif6) %&gt;% aggregate(20)\np07 &lt;- terra::rast(tif7) %&gt;% aggregate(20)\np08 &lt;- terra::rast(tif8) %&gt;% aggregate(20)\np09 &lt;- terra::rast(tif9) %&gt;% aggregate(20)\np10 &lt;- terra::rast(tif10) %&gt;% aggregate(20)\n\n\n# with many SpatRasters, make a SpatRasterCollection from a list\nrlist &lt;- list(p01, p02, p03, p04, p05, p06, p07, p08, p09, p10)\nrsrc &lt;- sprc(rlist)\n\nmm &lt;- merge(rsrc)\n\n\nnames(mm)\n\n[1] \"1_r20210825_1\" \"1_r20210825_2\" \"1_r20210825_3\" \"1_r20210825_4\"\n[5] \"1_r20210825_5\"\n\n\n\n# Rename\nmm2 &lt;- mm %&gt;%\n  rename(blue = \"1_r20210825_1\", green = \"1_r20210825_2\", \n         red = \"1_r20210825_3\", redge= \"1_r20210825_4\",\n         nir = \"1_r20210825_5\")\n\nNow, a visualization task:\n\nggplot() +\n  geom_spatraster(data = mm2) +\n  facet_wrap(~lyr, ncol = 5) +\n  r4pde::theme_r4pde(font_size = 10)+\n  scale_fill_whitebox_c(\n    palette = \"muted\",\n    labels = scales::label_number(suffix = \"%\"),\n    n.breaks = 10,\n    guide = guide_legend(reverse = TRUE)\n  ) +\n  labs(\n    fill = \"\",\n    title = \"UAV multispectral mosaic\",\n    subtitle = \"08.25.2021\"\n  )\n\n\n\n\n\n\n\n\n\n\n8.5.7 Reading severity data\nNow, let’s read the ground CLS data:\n\nfile &lt;- \"data/cercospora_data/CLS_DS.csv\"\ncls &lt;- readr::read_csv(file, col_names = TRUE, show_col_types = FALSE)\n\nWhat we got?\n\ncls\n\n# A tibble: 136 × 9\n    Plot    D0    D1    D2    D3    D4    D5    D6  year\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     0 0.25   0.9   4.58  8.75  21      NA  2021\n 2     2     0 0.75   3.68 12.4  20.8   45.4    NA  2021\n 3     3     0 2.92   7.38 18    34.6   65.2    NA  2021\n 4     4     0 0.75   5.45 17.4  42.6   75.5    NA  2021\n 5     5     0 3.38   3.45 15.4  17.2   55.2    NA  2021\n 6     6     0 0.8    1.38  8.8  12.2   15.4    NA  2021\n 7     7     0 0.375  3.05  9.1  13     31.0    NA  2021\n 8     8     0 3.7    8.4  29.4  41.2   71.8    NA  2021\n 9     9     0 2.38   4.28 13.2  15.6   38.1    NA  2021\n10    10     0 2.22   2.28 30.2  17.3   68      NA  2021\n# ℹ 126 more rows\n\n\nNote that columns D0 to D6 refer to the disease severity recorded in percentage at different dates for each study plot. Note also that this object is structured in wide format (i.e. each row represents a single plot for each year).\n\nunique(cls$Plot)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n[51] 51 52 53 54 55 56\n\n\nLet’s know the values stored in the year attribute:\n\nunique(cls$year)\n\n[1] 2021 2022 2023\n\n\n\n\n8.5.8 Preparing a set of spectral covariables per plot & date\nThe 2021 data collection dates are 20210707, 20210715, 20210720, 20210802,and 20210825.\nLet’s start creating a list of images per date for 2021:\n\n# Create a list .tif files collected at each date:\n# These are the DXfiles\nD0files &lt;- Sys.glob(\"data/cercospora_data/multispec_2021_2022/*_r20210707.tif\")\nD1files &lt;- Sys.glob(\"data/cercospora_data/multispec_2021_2022/*_r20210715.tif\")\nD2files &lt;- Sys.glob(\"data/cercospora_data/multispec_2021_2022/*_r20210720.tif\")\nD3files &lt;- Sys.glob(\"data/cercospora_data/multispec_2021_2022/*_r20210802.tif\")\nD4files &lt;- Sys.glob(\"data/cercospora_data/multispec_2021_2022/*_r20210825.tif\")\n\nNow, define date variables:\n\n# These are the DX variables\n(D0 &lt;- as.Date('7/7/2021',format='%m/%d/%Y'))\n\n[1] \"2021-07-07\"\n\n(D1 &lt;- as.Date('7/15/2021',format='%m/%d/%Y'))\n\n[1] \"2021-07-15\"\n\n(D2 &lt;- as.Date('7/20/2021',format='%m/%d/%Y'))\n\n[1] \"2021-07-20\"\n\n(D3 &lt;- as.Date('8/02/2021',format='%m/%d/%Y'))\n\n[1] \"2021-08-02\"\n\n(D4 &lt;- as.Date('8/25/2021',format='%m/%d/%Y'))\n\n[1] \"2021-08-25\"\n\n\nThe following block of code should be executed five times (one per each DXfiles & each DX variable).\nThe code reads each image included in a given DXfiles, computes several vegetation indices (VIs) for such image, and obtain the average value of original bands & VIs per each. All values are stored in a vector object.\n\n# Read all files for each date\nlista &lt;- lapply(D0files, function(x) rast(x)) \n# Compute global statistics\noutput &lt;- vector(\"double\", 5)  # 1. output\nfor (i in seq_along(lista)) {            # 2. sequence\n  img &lt;- lista[[i]]\n  r &lt;- clamp(img, 0, 1)\n  ndvi &lt;- (r[[5]]-r[[3]])/(r[[5]]+r[[3]])\n  ndre = (r[[5]]-r[[4]]) / (r[[5]]+r[[4]])\n  cire = (r[[5]]) / (r[[4]]-1)\n  sipi = (r[[5]] - r[[1]]) / (r[[4]]-r[[3]])\n  min &lt;- minmax(ndvi)[1]\n  max &lt;- minmax(ndvi)[2]\n  avg &lt;- global(ndvi, fun=\"mean\", na.rm=TRUE)[,1]\n  #dev &lt;- global(ndvi, fun=\"std\")\n  #mndvi &lt;- (ndvi-min)/(max-min)\n  mndvi &lt;- 2 + log((ndvi-avg)/(max-min))\n  nimg &lt;- c(r,ndvi, ndre, cire, sipi, mndvi)\n  output[i] &lt;- global(nimg, fun=\"mean\", na.rm=TRUE) \n  #output[i] &lt;- global(nimg, mean, na.rm=TRUE) \n}\n\nThe following block of code combine all vectors produced previously into a matrix as a previous step to put all data into a dataframe with meaningful colummn names. It also add colummns storing each plot ID as well as the corresponding data. Note the the code also needs to be executed five times (one per each DX variable).\n\n#combine all vectors into a matrix\nmat &lt;- do.call(\"rbind\",output) #combine all vectors into a matrix\n#convert matrix to data frame\ndf &lt;- as.data.frame(mat)\n#specify column names\ncolnames(df) &lt;- c('blue', 'green', 'red', 'redge', 'nir', 'ndvi', \n                  'ndre', 'cire', 'sipi', 'mndvi')\ndate = D0\nplot &lt;- seq(1:40)\ndate &lt;- rep(date,40)\nnewdf &lt;- cbind(df,plot,date)\n\nWhat we got?\n\nnewdf\n\n         blue      green        red     redge       nir      ndvi      ndre\n1  0.02343657 0.05959132 0.04362133 0.1444027 0.3131252 0.6988392 0.3515343\n2  0.02282100 0.05970168 0.04427105 0.1432050 0.3003496 0.6909051 0.3404127\n3  0.02229088 0.06034372 0.04296553 0.1441267 0.3037592 0.6985894 0.3424855\n4  0.02211413 0.06289556 0.04265936 0.1493579 0.3175088 0.7124582 0.3477512\n5  0.02311490 0.05985614 0.04618947 0.1392655 0.2849417 0.6763379 0.3314924\n6  0.02237584 0.06117992 0.04150024 0.1462904 0.3117826 0.7198424 0.3504247\n7  0.02086133 0.05870957 0.03996187 0.1412853 0.3017970 0.7235433 0.3521370\n8  0.02132975 0.05750314 0.03989218 0.1370614 0.2956135 0.7157179 0.3550798\n9  0.02188219 0.05869401 0.04093412 0.1413798 0.3023368 0.7115535 0.3503842\n10 0.02260319 0.05975366 0.04357307 0.1429067 0.2992025 0.6962978 0.3402171\n11 0.02089238 0.04880216 0.04060995 0.1215226 0.2446871 0.6569093 0.3221139\n12 0.02350888 0.05890453 0.04759297 0.1368886 0.2807072 0.6557864 0.3276572\n13 0.02218707 0.05638867 0.04349103 0.1341888 0.2819767 0.6780262 0.3441717\n14 0.02275558 0.05684062 0.04544145 0.1339135 0.2753768 0.6666547 0.3337416\n15 0.02272742 0.05738843 0.04389310 0.1379283 0.2873050 0.6838893 0.3418451\n16 0.02140498 0.05666796 0.04005207 0.1370657 0.2937609 0.7116084 0.3544429\n17 0.02086186 0.05736772 0.03807191 0.1398500 0.2991122 0.7290118 0.3560801\n18 0.02015755 0.05423116 0.03848824 0.1311203 0.2787107 0.7109047 0.3537186\n19 0.02193068 0.05637152 0.03996171 0.1306193 0.2834647 0.7008749 0.3578544\n20 0.02214865 0.05725453 0.04177672 0.1331511 0.2830351 0.6914292 0.3488766\n21 0.02122350 0.05473366 0.04193676 0.1295753 0.2737450 0.6830712 0.3463346\n22 0.02200368 0.05443210 0.04510647 0.1327359 0.2787727 0.6671694 0.3387558\n23 0.02334062 0.05691194 0.04427723 0.1309551 0.2608062 0.6588717 0.3212313\n24 0.02211155 0.06021804 0.03976967 0.1403651 0.3081556 0.7156574 0.3615820\n25 0.02174683 0.05562259 0.03816646 0.1312835 0.2858236 0.7057869 0.3560048\n26 0.02082576 0.05881460 0.03393639 0.1407333 0.3136037 0.7548543 0.3704691\n27 0.02192849 0.06039847 0.03532052 0.1438516 0.3201219 0.7514753 0.3694975\n28 0.02302246 0.06453867 0.03590782 0.1454413 0.3360538 0.7627826 0.3862154\n29 0.02060561 0.05634195 0.03435760 0.1348848 0.3139023 0.7577029 0.3915793\n30 0.02453648 0.05776692 0.04101824 0.1400822 0.3165895 0.7123657 0.3729309\n31 0.02449699 0.06516875 0.04320010 0.1488943 0.3369249 0.7188490 0.3734588\n32 0.02375349 0.06403329 0.04276755 0.1462964 0.3370367 0.7210620 0.3804087\n33 0.02255618 0.05711345 0.04411843 0.1383896 0.2971490 0.6880898 0.3481446\n34 0.02217878 0.05772793 0.04388026 0.1337953 0.2885867 0.6798194 0.3490214\n35 0.02187461 0.05617408 0.04287472 0.1365350 0.2917336 0.6953013 0.3481033\n36 0.02296074 0.05817811 0.04563811 0.1391187 0.2921268 0.6801861 0.3393280\n37 0.02267559 0.05957335 0.04213828 0.1420391 0.3082079 0.7142905 0.3568940\n38 0.02360669 0.05999425 0.04294853 0.1433218 0.3103642 0.7052194 0.3528355\n39 0.02364231 0.06191251 0.04298429 0.1483199 0.3220375 0.7163665 0.3560244\n40 0.02427710 0.06139374 0.04394927 0.1444968 0.3085851 0.6985304 0.3476681\n         cire     sipi      mndvi plot       date\n1  -0.3789603 3.641733 0.33578281    1 2021-07-07\n2  -0.3623515 3.550125 0.38310911    2 2021-07-07\n3  -0.3674174 3.484420 0.30046194    3 2021-07-07\n4  -0.3865963 3.431203 0.36371401    4 2021-07-07\n5  -0.3408283 3.558089 0.44423685    5 2021-07-07\n6  -0.3768460 3.376905 0.24863497    6 2021-07-07\n7  -0.3618540 3.407052 0.30100962    7 2021-07-07\n8  -0.3531518 3.494425 0.34119340    8 2021-07-07\n9  -0.3637379 3.481473 0.32563500    9 2021-07-07\n10 -0.3605456 3.516463 0.39810832   10 2021-07-07\n11 -0.2870937 3.673143 0.49929140   11 2021-07-07\n12 -0.3354612 3.761214 0.58121969   12 2021-07-07\n13 -0.3377332 3.773541 0.52614654   13 2021-07-07\n14 -0.3282154 3.695583 0.50885882   14 2021-07-07\n15 -0.3454207 3.635421 0.39704537   15 2021-07-07\n16 -0.3519775 3.536311 0.40365150   16 2021-07-07\n17 -0.3597412 3.381570 0.19228050   17 2021-07-07\n18 -0.3315227 3.570230 0.37412852   18 2021-07-07\n19 -0.3374564 3.779183 0.21921958   19 2021-07-07\n20 -0.3374060 3.622386 0.26701154   20 2021-07-07\n21 -0.3250037 3.703904 0.39212223   21 2021-07-07\n22 -0.3321942 3.766054 0.43835034   22 2021-07-07\n23 -0.3098876 3.691888 0.32447853   23 2021-07-07\n24 -0.3720515 3.512840 0.37734386   24 2021-07-07\n25 -0.3404875 3.509041 0.35422784   25 2021-07-07\n26 -0.3779853 3.204817 0.14699663   26 2021-07-07\n27 -0.3867478 3.270867 0.08401903   27 2021-07-07\n28 -0.4060356 3.304020 0.01329941   28 2021-07-07\n29 -0.3747234 3.590456 0.09783340   29 2021-07-07\n30 -0.3813865 4.120632 0.31838109   30 2021-07-07\n31 -0.4099608 3.819633 0.35157164   31 2021-07-07\n32 -0.4091370 3.878319 0.33094172   32 2021-07-07\n33 -0.3558508 3.668159 0.40724176   33 2021-07-07\n34 -0.3443164 4.378597 0.44631347   34 2021-07-07\n35 -0.3486557 3.563385 0.37868256   35 2021-07-07\n36 -0.3494958 3.645456 0.44505296   36 2021-07-07\n37 -0.3711740 3.527677 0.30651449   37 2021-07-07\n38 -0.3742929 3.591207 0.22357258   38 2021-07-07\n39 -0.3906997 3.483071 0.21173498   39 2021-07-07\n40 -0.3740427 3.568325 0.26238290   40 2021-07-07\n\n\nThe following block of code writes to disk a DX_2021.csv file. Note that the append parameter is set to FALSE in order to overwrite any existing data.\n\nwrite_csv(\n  newdf,\n  \"data/cercospora_data/D0_2021.csv\",\n  na = \"NA\",\n  col_names = TRUE,\n  append = FALSE\n)\n\n\n\n8.5.9 Put all covariate data in a single object\nNow, we will read all the recently created csv files and put all covariate data values into a single object.\nNext code obtains a list with the path of every csv file matching the D*_2021.csv expression:\n\n(csv_files &lt;- Sys.glob(\"data/cercospora_data/D*_2021.csv\"))\n\n[1] \"data/cercospora_data/D0_2021.csv\" \"data/cercospora_data/D1_2021.csv\"\n[3] \"data/cercospora_data/D2_2021.csv\" \"data/cercospora_data/D3_2021.csv\"\n[5] \"data/cercospora_data/D4_2021.csv\"\n\n\nNow, we will read all files in the list:\n\n(df.2021 &lt;- do.call(rbind,lapply(csv_files,read.csv)))\n\n          blue      green        red     redge       nir      ndvi      ndre\n1   0.02343657 0.05959132 0.04362133 0.1444027 0.3131252 0.6988392 0.3515343\n2   0.02282100 0.05970168 0.04427105 0.1432050 0.3003496 0.6909051 0.3404127\n3   0.02229088 0.06034372 0.04296553 0.1441267 0.3037592 0.6985894 0.3424855\n4   0.02211413 0.06289556 0.04265936 0.1493579 0.3175088 0.7124582 0.3477512\n5   0.02311490 0.05985614 0.04618947 0.1392655 0.2849417 0.6763379 0.3314924\n6   0.02237584 0.06117992 0.04150024 0.1462904 0.3117826 0.7198424 0.3504247\n7   0.02086133 0.05870957 0.03996187 0.1412853 0.3017970 0.7235433 0.3521370\n8   0.02132975 0.05750314 0.03989218 0.1370614 0.2956135 0.7157179 0.3550798\n9   0.02188219 0.05869401 0.04093412 0.1413798 0.3023368 0.7115535 0.3503842\n10  0.02260319 0.05975366 0.04357307 0.1429067 0.2992025 0.6962978 0.3402171\n11  0.02089238 0.04880216 0.04060995 0.1215226 0.2446871 0.6569093 0.3221139\n12  0.02350888 0.05890453 0.04759297 0.1368886 0.2807072 0.6557864 0.3276572\n13  0.02218707 0.05638867 0.04349103 0.1341888 0.2819767 0.6780262 0.3441717\n14  0.02275558 0.05684062 0.04544145 0.1339135 0.2753768 0.6666547 0.3337416\n15  0.02272742 0.05738843 0.04389310 0.1379283 0.2873050 0.6838893 0.3418451\n16  0.02140498 0.05666796 0.04005207 0.1370657 0.2937609 0.7116084 0.3544429\n17  0.02086186 0.05736772 0.03807191 0.1398500 0.2991122 0.7290118 0.3560801\n18  0.02015755 0.05423116 0.03848824 0.1311203 0.2787107 0.7109047 0.3537186\n19  0.02193068 0.05637152 0.03996171 0.1306193 0.2834647 0.7008749 0.3578544\n20  0.02214865 0.05725453 0.04177672 0.1331511 0.2830351 0.6914292 0.3488766\n21  0.02122350 0.05473366 0.04193676 0.1295753 0.2737450 0.6830712 0.3463346\n22  0.02200368 0.05443210 0.04510647 0.1327359 0.2787727 0.6671694 0.3387558\n23  0.02334062 0.05691194 0.04427723 0.1309551 0.2608062 0.6588717 0.3212313\n24  0.02211155 0.06021804 0.03976967 0.1403651 0.3081556 0.7156574 0.3615820\n25  0.02174683 0.05562259 0.03816646 0.1312835 0.2858236 0.7057869 0.3560048\n26  0.02082576 0.05881460 0.03393639 0.1407333 0.3136037 0.7548543 0.3704691\n27  0.02192849 0.06039847 0.03532052 0.1438516 0.3201219 0.7514753 0.3694975\n28  0.02302246 0.06453867 0.03590782 0.1454413 0.3360538 0.7627826 0.3862154\n29  0.02060561 0.05634195 0.03435760 0.1348848 0.3139023 0.7577029 0.3915793\n30  0.02453648 0.05776692 0.04101824 0.1400822 0.3165895 0.7123657 0.3729309\n31  0.02449699 0.06516875 0.04320010 0.1488943 0.3369249 0.7188490 0.3734588\n32  0.02375349 0.06403329 0.04276755 0.1462964 0.3370367 0.7210620 0.3804087\n33  0.02255618 0.05711345 0.04411843 0.1383896 0.2971490 0.6880898 0.3481446\n34  0.02217878 0.05772793 0.04388026 0.1337953 0.2885867 0.6798194 0.3490214\n35  0.02187461 0.05617408 0.04287472 0.1365350 0.2917336 0.6953013 0.3481033\n36  0.02296074 0.05817811 0.04563811 0.1391187 0.2921268 0.6801861 0.3393280\n37  0.02267559 0.05957335 0.04213828 0.1420391 0.3082079 0.7142905 0.3568940\n38  0.02360669 0.05999425 0.04294853 0.1433218 0.3103642 0.7052194 0.3528355\n39  0.02364231 0.06191251 0.04298429 0.1483199 0.3220375 0.7163665 0.3560244\n40  0.02427710 0.06139374 0.04394927 0.1444968 0.3085851 0.6985304 0.3476681\n41  0.02052956 0.05020432 0.02973388 0.1338569 0.3185785 0.7882094 0.3981185\n42  0.01859800 0.05052206 0.03105854 0.1318898 0.3058317 0.7805916 0.3922774\n43  0.01981702 0.05348824 0.03288278 0.1322381 0.3022270 0.7638127 0.3819167\n44  0.02000617 0.05614472 0.03134368 0.1402745 0.3260600 0.7887921 0.3904000\n45  0.02095564 0.05658479 0.03321928 0.1439913 0.3291102 0.7828250 0.3835718\n46  0.02237261 0.05968790 0.03353345 0.1488491 0.3470870 0.7903656 0.3917955\n47  0.02100918 0.05998822 0.03180204 0.1560300 0.3643333 0.8096700 0.3941080\n48  0.02181143 0.05995075 0.03243477 0.1516184 0.3587694 0.8000431 0.3988671\n49  0.02132649 0.05706844 0.03089749 0.1471923 0.3448123 0.8004532 0.3926974\n50  0.02159894 0.05753348 0.03114798 0.1416646 0.3302551 0.7919107 0.3914513\n51  0.02038298 0.04942617 0.03205623 0.1217379 0.2899206 0.7603175 0.3967210\n52  0.02266377 0.05830423 0.03712530 0.1481954 0.3369567 0.7581285 0.3783657\n53  0.02015731 0.04875384 0.03535302 0.1201503 0.2717107 0.7356596 0.3799353\n54  0.02165452 0.05001698 0.03766276 0.1215014 0.2670488 0.7155948 0.3650533\n55  0.02265802 0.05245606 0.03529764 0.1328123 0.3006981 0.7540973 0.3802748\n56  0.02051525 0.05213257 0.03268753 0.1382642 0.3256237 0.7833498 0.3957619\n57  0.02128752 0.05645641 0.03279428 0.1483382 0.3501462 0.7989040 0.4000442\n58  0.02095490 0.05585342 0.03271893 0.1444677 0.3498758 0.7997719 0.4113192\n59  0.02185206 0.05682143 0.03272919 0.1417155 0.3463017 0.7920995 0.4109469\n60  0.02202972 0.05714222 0.03436180 0.1461714 0.3441786 0.7825692 0.3957799\n61  0.02098234 0.05483850 0.03241813 0.1348340 0.3203631 0.7814717 0.3997993\n62  0.01901755 0.04792084 0.02924172 0.1168986 0.2677576 0.7610976 0.3815744\n63  0.02388539 0.05884962 0.03938045 0.1462760 0.3194442 0.7426530 0.3626077\n64  0.02140199 0.05705007 0.03633223 0.1389278 0.3217712 0.7609113 0.3886500\n65  0.02201808 0.05953653 0.03894399 0.1374054 0.3171746 0.7461247 0.3899800\n66  0.02075849 0.05788808 0.03145918 0.1467951 0.3700974 0.8162132 0.4289832\n67  0.02015069 0.05689458 0.03134979 0.1515357 0.3765375 0.8211904 0.4239588\n68  0.02026395 0.05835547 0.03137467 0.1573338 0.3931594 0.8284231 0.4268455\n69  0.01992303 0.05484262 0.03180804 0.1509191 0.3793238 0.8212715 0.4326179\n70  0.02065776 0.05741422 0.03358031 0.1470773 0.3617977 0.7967555 0.4185559\n71  0.02220296 0.06168994 0.03496599 0.1541648 0.3697565 0.7967047 0.4086072\n72  0.02038571 0.05698300 0.03187045 0.1479112 0.3529448 0.8037041 0.4055701\n73  0.01967439 0.04903127 0.02905272 0.1239711 0.2874766 0.7748027 0.3851905\n74  0.02137720 0.05585394 0.03576933 0.1306864 0.3214592 0.7630346 0.4159502\n75  0.01963094 0.04868798 0.03043499 0.1312665 0.3107001 0.7820182 0.3978929\n76  0.02113100 0.05204241 0.03232803 0.1335988 0.3020883 0.7639990 0.3756051\n77  0.02048588 0.05105247 0.03119704 0.1411513 0.3349993 0.7918959 0.3975757\n78  0.02243174 0.05674670 0.03130302 0.1461549 0.3674505 0.8055775 0.4201542\n79  0.02187014 0.05383427 0.03043378 0.1401986 0.3415687 0.7977970 0.4083110\n80  0.02188459 0.05444195 0.02963870 0.1365380 0.3385607 0.8036543 0.4163733\n81  0.02108185 0.05315597 0.03629927 0.1615935 0.4061288 0.8110457 0.4222097\n82  0.02107638 0.05629456 0.03690022 0.1572443 0.3762103 0.7876093 0.4011143\n83  0.02178106 0.05907055 0.03867815 0.1628041 0.3881885 0.7835115 0.3986314\n84  0.02047992 0.06098736 0.03665144 0.1691727 0.4158930 0.8096072 0.4135806\n85  0.02129717 0.05862935 0.03736545 0.1607825 0.3928958 0.7985510 0.4127268\n86  0.02194515 0.06127932 0.03643291 0.1634409 0.4002293 0.8077606 0.4146739\n87  0.02097622 0.06260348 0.03316698 0.1759699 0.4279086 0.8375735 0.4146203\n88  0.02082045 0.05968328 0.03347267 0.1692592 0.4194361 0.8302575 0.4193664\n89  0.02052110 0.05855188 0.03285183 0.1667946 0.4183311 0.8334915 0.4252922\n90  0.02118406 0.05952772 0.03409333 0.1669941 0.4164679 0.8290411 0.4224324\n91  0.02066534 0.05198353 0.03630887 0.1469969 0.3602886 0.7819583 0.4100563\n92  0.02286327 0.06025029 0.03771215 0.1674188 0.4105733 0.8133905 0.4146804\n93  0.02418177 0.06143854 0.04705647 0.1632921 0.3503713 0.7270033 0.3567380\n94  0.02357276 0.05738955 0.04634505 0.1583690 0.3343389 0.7189932 0.3482989\n95  0.02251601 0.05564380 0.04242904 0.1626545 0.3609011 0.7562440 0.3706428\n96  0.02082985 0.05493789 0.03747858 0.1581995 0.3747444 0.7851167 0.3976251\n97  0.02073972 0.05667054 0.03645775 0.1673818 0.3937831 0.8025926 0.3977367\n98  0.02024999 0.05755859 0.03581590 0.1662561 0.4091635 0.8133004 0.4166660\n99  0.02082345 0.05814276 0.03632322 0.1663019 0.4077442 0.8067838 0.4127645\n100 0.02130631 0.05952263 0.03733125 0.1691712 0.4058952 0.7983298 0.4039078\n101 0.02079688 0.05889001 0.03666624 0.1697102 0.3959562 0.8020224 0.3937777\n102 0.02154315 0.05398097 0.03832096 0.1526989 0.3727947 0.7756811 0.4090068\n103 0.02068967 0.05666765 0.04165745 0.1675508 0.3665856 0.7608506 0.3641168\n104 0.02287252 0.06036144 0.04135329 0.1691424 0.3596196 0.7572024 0.3523070\n105 0.02327678 0.05737544 0.04043948 0.1639254 0.3273054 0.7462585 0.3248970\n106 0.02055421 0.05506976 0.03165367 0.1679061 0.3830571 0.8245731 0.3848886\n107 0.02045120 0.05556580 0.03147341 0.1721244 0.3911810 0.8288812 0.3848388\n108 0.01936702 0.05359267 0.03041155 0.1670385 0.4003922 0.8384306 0.4065918\n109 0.01892969 0.05400171 0.03181207 0.1714764 0.3980453 0.8328814 0.3948891\n110 0.01908086 0.05353836 0.03287591 0.1710066 0.3968207 0.8205218 0.3920708\n111 0.02038547 0.05592239 0.03392618 0.1781330 0.4067277 0.8204882 0.3854746\n112 0.02008451 0.05720333 0.03208596 0.1750760 0.4149822 0.8331989 0.4011998\n113 0.02067345 0.05449945 0.03739998 0.1559458 0.3971303 0.7953698 0.4269298\n114 0.01985213 0.05373437 0.03486695 0.1637985 0.3662829 0.7934669 0.3733396\n115 0.02132429 0.05593877 0.03814748 0.1559295 0.3894839 0.7920895 0.4204294\n116 0.02137609 0.05544646 0.03941156 0.1535504 0.3745666 0.7752996 0.4094564\n117 0.02173253 0.05606452 0.03581073 0.1600026 0.3982399 0.8091809 0.4198104\n118 0.01983960 0.05239147 0.03299102 0.1556517 0.3980271 0.8196374 0.4295256\n119 0.01958886 0.05078259 0.03284344 0.1529905 0.3931337 0.8197849 0.4326471\n120 0.02143953 0.05433169 0.03362620 0.1559428 0.4089219 0.8242763 0.4413151\n121 0.02456960 0.05829370 0.03337980 0.2035489 0.4674467 0.8522272 0.3940106\n122 0.02227492 0.05626306 0.03402620 0.1924762 0.4087379 0.8252356 0.3618694\n123 0.02523303 0.06403575 0.03893872 0.1969021 0.4110210 0.8061321 0.3530707\n124 0.02431507 0.06981933 0.03149349 0.2080660 0.4400168 0.8538962 0.3608609\n125 0.02721698 0.06959043 0.04073738 0.2240817 0.4563009 0.8229697 0.3463280\n126 0.02798405 0.07293051 0.04063069 0.2243524 0.4512955 0.8183378 0.3413659\n127 0.02362352 0.06819463 0.03388499 0.2225483 0.4584950 0.8516509 0.3536612\n128 0.02337971 0.06235514 0.03212167 0.2017982 0.4440984 0.8520046 0.3771044\n129 0.02266283 0.06217227 0.03157511 0.2027005 0.4475631 0.8549204 0.3785745\n130 0.02366451 0.06000658 0.03321898 0.1894632 0.4401444 0.8441077 0.4001559\n131 0.02513064 0.06334645 0.03448836 0.1972631 0.4274775 0.8350754 0.3716506\n132 0.02377771 0.05588092 0.03341388 0.1794289 0.4061702 0.8257592 0.3845869\n133 0.02449490 0.05809191 0.04298490 0.1784245 0.3147830 0.7299304 0.2795985\n134 0.02731799 0.05988043 0.04708922 0.1774982 0.3350545 0.7195767 0.3055429\n135 0.02619083 0.05744756 0.03728376 0.1842603 0.3627074 0.7868434 0.3266340\n136 0.02581965 0.06053502 0.03775902 0.1914101 0.3971750 0.8019641 0.3518380\n137 0.02561499 0.06698112 0.04009003 0.2271990 0.4421288 0.8138778 0.3278621\n138 0.02400703 0.06453540 0.03525491 0.2070071 0.4443489 0.8351464 0.3707701\n139 0.02158047 0.05805170 0.02776623 0.1873562 0.4307834 0.8663875 0.3984368\n140 0.02182744 0.05941715 0.02958924 0.1884719 0.4182472 0.8503351 0.3807011\n141 0.02015692 0.05312046 0.02761516 0.1659301 0.3789922 0.8446382 0.3930457\n142 0.02510567 0.06159803 0.03564669 0.1842460 0.3929318 0.8109585 0.3607708\n143 0.01993199 0.04180365 0.02775356 0.1207182 0.2949298 0.8009226 0.4183965\n144 0.02300851 0.05705218 0.03661990 0.1733665 0.3311235 0.7724715 0.3138201\n145 0.02476484 0.05523785 0.03972681 0.1718589 0.3296071 0.7534944 0.3139303\n146 0.02154290 0.05242739 0.03053955 0.1765510 0.3719669 0.8277281 0.3569453\n147 0.02210420 0.05258662 0.03051192 0.1727336 0.3759095 0.8276559 0.3698586\n148 0.02330147 0.05844441 0.03332284 0.1898374 0.4210377 0.8347262 0.3800209\n149 0.02282903 0.05852711 0.03503826 0.2062768 0.4417818 0.8371609 0.3690559\n150 0.02201556 0.05756078 0.03422850 0.1881062 0.4182142 0.8332135 0.3817618\n151 0.02389892 0.05898878 0.03347903 0.1847862 0.4078247 0.8310272 0.3799784\n152 0.02083712 0.05540730 0.02953161 0.1768605 0.3941414 0.8413166 0.3825045\n153 0.02458310 0.06176598 0.03476932 0.1976596 0.4174477 0.8280028 0.3586799\n154 0.01718078 0.04930132 0.02856733 0.1486271 0.3652903 0.8334064 0.4221488\n155 0.02748465 0.06946837 0.04363258 0.2318181 0.4706404 0.8123663 0.3418064\n156 0.02747831 0.07332335 0.04389518 0.2264001 0.4458781 0.7968273 0.3272567\n157 0.02521912 0.06154772 0.03600436 0.2019781 0.4287934 0.8282891 0.3619903\n158 0.02434867 0.06283113 0.03451642 0.2081286 0.4614013 0.8472470 0.3825518\n159 0.02530022 0.05947328 0.03604093 0.2065795 0.4364578 0.8318662 0.3616867\n160 0.02323038 0.06380880 0.03683221 0.2106691 0.4795214 0.8422059 0.3925382\n161 0.03244730 0.06243503 0.07069395 0.1331881 0.2240197 0.5263451 0.2603764\n162 0.03220764 0.06421227 0.07358433 0.1305611 0.2126103 0.4872965 0.2416332\n163 0.03148392 0.06400177 0.06896637 0.1465971 0.2389044 0.5390938 0.2399182\n164 0.03167203 0.06808206 0.06547172 0.1751177 0.3086365 0.6280323 0.2732817\n165 0.03320107 0.06398255 0.07626774 0.1272121 0.1949513 0.4500080 0.2154965\n166 0.03155779 0.06765107 0.06388105 0.1640356 0.2689753 0.6018466 0.2430515\n167 0.03049306 0.06404963 0.06585743 0.1490913 0.2448651 0.5740527 0.2454515\n168 0.03252230 0.06384611 0.06648974 0.1375281 0.2243945 0.5383073 0.2420900\n169 0.02987872 0.05902407 0.06794822 0.1251337 0.2040113 0.5055839 0.2431990\n170 0.03446312 0.06983649 0.07757790 0.1447033 0.2299757 0.5042939 0.2310344\n171 0.03317173 0.06795603 0.06613100 0.1632761 0.3060204 0.6192177 0.2999994\n172 0.03227812 0.06842014 0.07143898 0.1660777 0.2665647 0.5637743 0.2309203\n173 0.03398287 0.07115492 0.07250453 0.1389441 0.2177167 0.5067603 0.2260623\n174 0.02865669 0.06429306 0.05747477 0.1574716 0.2590684 0.6237297 0.2431701\n175 0.03021290 0.06066011 0.06666227 0.1384870 0.2170173 0.5271643 0.2225675\n176 0.02967587 0.06158339 0.06095244 0.1500869 0.2465646 0.5848673 0.2416536\n177 0.02884645 0.05705891 0.06533360 0.1202913 0.1869982 0.4882258 0.2250775\n178 0.03276704 0.06359781 0.07456907 0.1251366 0.1891989 0.4570189 0.2115906\n179 0.02708242 0.05999907 0.05766794 0.1504990 0.2450337 0.5946147 0.2374369\n180 0.02766003 0.05688758 0.06456498 0.1280580 0.2001966 0.5069556 0.2206031\n181 0.02838966 0.05722613 0.06488701 0.1271373 0.1992304 0.5115626 0.2244217\n182 0.03559652 0.06614677 0.07592767 0.1315287 0.2143333 0.4807597 0.2449726\n183 0.03122141 0.06389911 0.07011235 0.1232556 0.1820245 0.4541211 0.1994603\n184 0.02919906 0.06398205 0.06071808 0.1454490 0.2416222 0.5913800 0.2485037\n185 0.02982668 0.06742336 0.06261795 0.1664833 0.2768481 0.6132203 0.2448847\n186 0.02978697 0.05961399 0.06366181 0.1285860 0.2094327 0.5358823 0.2420457\n187 0.02802117 0.05635194 0.05964353 0.1242803 0.2039935 0.5432331 0.2473246\n188 0.02804431 0.05990398 0.05714082 0.1520841 0.2605731 0.6137755 0.2602729\n189 0.03115430 0.06222985 0.06680599 0.1226242 0.1971786 0.5040743 0.2421152\n190 0.03017063 0.05987192 0.06688301 0.1147182 0.1808972 0.4625647 0.2312153\n191 0.02969164 0.06141537 0.06461569 0.1333502 0.2173871 0.5265676 0.2410969\n192 0.03245863 0.06580556 0.07369124 0.1179781 0.1793431 0.4283260 0.2142952\n193 0.03780640 0.06896099 0.08072512 0.1249145 0.2023160 0.4538598 0.2488864\n194 0.03015803 0.06140500 0.06845817 0.1301699 0.2018116 0.4929597 0.2222330\n195 0.03273967 0.06385214 0.07184862 0.1220430 0.2069285 0.4991207 0.2646952\n196 0.03340377 0.06654732 0.07474162 0.1302466 0.2149186 0.4934244 0.2531492\n197 0.03576925 0.07247605 0.07037803 0.1731286 0.3140637 0.6212680 0.2859256\n198 0.02975048 0.05792985 0.06628184 0.1342174 0.2345209 0.5560936 0.2760421\n199 0.03765677 0.07335121 0.08514252 0.1333163 0.2088120 0.4441819 0.2313678\n200 0.03487981 0.06780985 0.07374642 0.1484053 0.2625147 0.5622498 0.2799192\n          cire       sipi         mndvi plot       date\n1   -0.3789603  3.6417334  0.3357828097    1 2021-07-07\n2   -0.3623515  3.5501252  0.3831091052    2 2021-07-07\n3   -0.3674174  3.4844196  0.3004619363    3 2021-07-07\n4   -0.3865963  3.4312028  0.3637140071    4 2021-07-07\n5   -0.3408283  3.5580892  0.4442368504    5 2021-07-07\n6   -0.3768460  3.3769052  0.2486349703    6 2021-07-07\n7   -0.3618540  3.4070518  0.3010096154    7 2021-07-07\n8   -0.3531518  3.4944250  0.3411934009    8 2021-07-07\n9   -0.3637379  3.4814727  0.3256349981    9 2021-07-07\n10  -0.3605456  3.5164631  0.3981083214   10 2021-07-07\n11  -0.2870937  3.6731431  0.4992914029   11 2021-07-07\n12  -0.3354612  3.7612144  0.5812196925   12 2021-07-07\n13  -0.3377332  3.7735408  0.5261465375   13 2021-07-07\n14  -0.3282154  3.6955830  0.5088588230   14 2021-07-07\n15  -0.3454207  3.6354208  0.3970453680   15 2021-07-07\n16  -0.3519775  3.5363115  0.4036515029   16 2021-07-07\n17  -0.3597412  3.3815695  0.1922804965   17 2021-07-07\n18  -0.3315227  3.5702305  0.3741285225   18 2021-07-07\n19  -0.3374564  3.7791832  0.2192195769   19 2021-07-07\n20  -0.3374060  3.6223864  0.2670115421   20 2021-07-07\n21  -0.3250037  3.7039035  0.3921222267   21 2021-07-07\n22  -0.3321942  3.7660544  0.4383503430   22 2021-07-07\n23  -0.3098876  3.6918876  0.3244785295   23 2021-07-07\n24  -0.3720515  3.5128404  0.3773438604   24 2021-07-07\n25  -0.3404875  3.5090407  0.3542278410   25 2021-07-07\n26  -0.3779853  3.2048166  0.1469966333   26 2021-07-07\n27  -0.3867478  3.2708674  0.0840190302   27 2021-07-07\n28  -0.4060356  3.3040204  0.0132994128   28 2021-07-07\n29  -0.3747234  3.5904557  0.0978333987   29 2021-07-07\n30  -0.3813865  4.1206319  0.3183810870   30 2021-07-07\n31  -0.4099608  3.8196332  0.3515716385   31 2021-07-07\n32  -0.4091370  3.8783187  0.3309417213   32 2021-07-07\n33  -0.3558508  3.6681589  0.4072417552   33 2021-07-07\n34  -0.3443164  4.3785975  0.4463134659   34 2021-07-07\n35  -0.3486557  3.5633852  0.3786825647   35 2021-07-07\n36  -0.3494958  3.6454562  0.4450529648   36 2021-07-07\n37  -0.3711740  3.5276774  0.3065144945   37 2021-07-07\n38  -0.3742929  3.5912072  0.2235725778   38 2021-07-07\n39  -0.3906997  3.4830712  0.2117349834   39 2021-07-07\n40  -0.3740427  3.5683248  0.2623829017   40 2021-07-07\n41  -0.3774776  3.2284252  0.0205864548    1 2021-07-15\n42  -0.3619397  3.2776905  0.0250170201    2 2021-07-15\n43  -0.3575166  3.2530421  0.0061431869    3 2021-07-15\n44  -0.3893297  3.1506252 -0.0686704756    4 2021-07-15\n45  -0.3944119  3.1047354 -0.0450803877    5 2021-07-15\n46  -0.4183130  3.1291518 -0.1398443835    6 2021-07-15\n47  -0.4437158  3.0358186 -0.3200177751    7 2021-07-15\n48  -0.4349809  3.1511394 -0.1140547795    8 2021-07-15\n49  -0.4147639  3.0784810 -0.1478313020    9 2021-07-15\n50  -0.3948047  3.1059172 -0.1254230424   10 2021-07-15\n51  -0.3379536  3.4080718  0.1445930197   11 2021-07-15\n52  -0.4078889  3.3016841  0.1423226946   12 2021-07-15\n53  -0.3158736  3.4946728  0.2427405562   13 2021-07-15\n54  -0.3105774  3.5074244  0.2966248439   14 2021-07-15\n55  -0.3555785  3.2987067 -0.0224570025   15 2021-07-15\n56  -0.3880112  3.2228077  0.0539492552   16 2021-07-15\n57  -0.4222104  3.1715413 -0.1999835409   17 2021-07-15\n58  -0.4200386  3.2830352 -0.0812479746   18 2021-07-15\n59  -0.4149174  3.3226200 -0.2049412495   19 2021-07-15\n60  -0.4144386  3.2185240 -0.1793357879   20 2021-07-15\n61  -0.3798012  3.2956169 -0.1324388877   21 2021-07-15\n62  -0.3103250  3.1944364  0.0458092649   22 2021-07-15\n63  -0.3849424  3.2210765  0.0183683589   23 2021-07-15\n64  -0.3839650  3.3667520  0.1305738483   24 2021-07-15\n65  -0.3775903  3.5396714  0.0378045532   25 2021-07-15\n66  -0.4459105  3.3799673 -0.3042757279   26 2021-07-15\n67  -0.4561446  3.2797138 -0.3881285964   27 2021-07-15\n68  -0.4798461  3.2995708 -0.3992933887   28 2021-07-15\n69  -0.4606623  3.4749287 -0.3792228663   29 2021-07-15\n70  -0.4377452  3.6391912 -0.0678144716   30 2021-07-15\n71  -0.4505760  3.3470019 -0.1745530817   31 2021-07-15\n72  -0.4264487  3.2119745 -0.2386634167   32 2021-07-15\n73  -0.3357306  3.1785080  0.0282156325   33 2021-07-15\n74  -0.3794890  0.8857437  0.0438634149   34 2021-07-15\n75  -0.3678764  3.2910615  0.0274106440   35 2021-07-15\n76  -0.3581278  3.1829013  0.1502160095   36 2021-07-15\n77  -0.4016235  3.2542421 -0.0794766912   37 2021-07-15\n78  -0.4426622  3.3495199 -0.2078983833   38 2021-07-15\n79  -0.4079300  3.2923091 -0.1346557992   39 2021-07-15\n80  -0.4025057  3.4355865 -0.2510148625   40 2021-07-15\n81  -0.4919465  3.2868877 -0.1696912521    1 2021-07-20\n82  -0.4570928  3.3063430 -0.0483153431    2 2021-07-20\n83  -0.4759719  3.2721096 -0.0953645846    3 2021-07-20\n84  -0.5115462  3.2726187 -0.1508310510    4 2021-07-20\n85  -0.4786064  3.3875816 -0.1012477650    5 2021-07-20\n86  -0.4887544  3.3604522 -0.2495194123    6 2021-07-20\n87  -0.5301999  3.0603599 -0.5378942151    7 2021-07-20\n88  -0.5148507  3.1401341 -0.3193034281    8 2021-07-20\n89  -0.5111246  3.1736160 -0.3029425461    9 2021-07-20\n90  -0.5076720  3.1717032 -0.3581289293   10 2021-07-20\n91  -0.4313389  3.4655221  0.0765274948   11 2021-07-20\n92  -0.4998867  3.1356637 -0.1852615254   12 2021-07-20\n93  -0.4303369  3.3440652  0.2412537373   13 2021-07-20\n94  -0.4078967  3.2326150  0.2240251204   14 2021-07-20\n95  -0.4421298  3.1950988 -0.0651335365   15 2021-07-20\n96  -0.4558926  3.2769642  0.0090607709   16 2021-07-20\n97  -0.4847610  3.1366127 -0.2128007022   17 2021-07-20\n98  -0.5018691  3.2937568 -0.2144020219   18 2021-07-20\n99  -0.5008288  3.2861279 -0.2931720787   19 2021-07-20\n100 -0.5013951  3.2573966 -0.0064584782   20 2021-07-20\n101 -0.4884182  3.0984898 -0.1350575729   21 2021-07-20\n102 -0.4512147  3.4944543  0.1324019681   22 2021-07-20\n103 -0.4521960  3.1149674 -0.0759637430   23 2021-07-20\n104 -0.4455444  3.0532771 -0.1119407775   24 2021-07-20\n105 -0.4020562  2.6983660 -0.0819634859   25 2021-07-20\n106 -0.4711470  2.7941891 -0.3987374946   26 2021-07-20\n107 -0.4841401  2.7822565 -0.4709627746   27 2021-07-20\n108 -0.4912187  2.9218889 -0.5005099302   28 2021-07-20\n109 -0.4918450  2.8617071 -0.4394932170   29 2021-07-20\n110 -0.4908861  2.9287081 -0.1964393774   30 2021-07-20\n111 -0.5076947  2.8636631 -0.3757807604   31 2021-07-20\n112 -0.5153835  2.9271096 -0.4935354112   32 2021-07-20\n113 -0.4804441  3.6148074  0.0619995742   33 2021-07-20\n114 -0.4488988  2.9330460 -0.0512569619   34 2021-07-20\n115 -0.4709222  3.5840775  0.0467589503   35 2021-07-20\n116 -0.4519072  3.5994402  0.1295138163   36 2021-07-20\n117 -0.4832397  3.3237758 -0.1673312381   37 2021-07-20\n118 -0.4799606  3.3668422 -0.2913712994   38 2021-07-20\n119 -0.4717806  3.4102617 -0.2423790191   39 2021-07-20\n120 -0.4919446  3.5110216 -0.4109303033   40 2021-07-20\n121 -0.6086618  2.7323016 -0.9329644398    1 2021-08-02\n122 -0.5293005  2.6311856 -0.4702134438    2 2021-08-02\n123 -0.5336928  2.6533929 -0.4919324211    3 2021-08-02\n124 -0.5770462  2.4966731 -0.9219435727    4 2021-08-02\n125 -0.6192123  2.5574666 -0.8734865582    5 2021-08-02\n126 -0.6129088  2.4911791 -0.4580022808    6 2021-08-02\n127 -0.6176537  2.4604690 -0.8471248176    7 2021-08-02\n128 -0.5782458  2.6137202 -0.8700951548    8 2021-08-02\n129 -0.5828264  2.6162938 -0.7559168262    9 2021-08-02\n130 -0.5625170  2.8505451 -1.0814628753   10 2021-08-02\n131 -0.5539207  2.6595102 -0.5201193156   11 2021-08-02\n132 -0.5123947  2.7928116 -0.4141809408   12 2021-08-02\n133 -0.3999454  2.4424561  0.0056911166   13 2021-08-02\n134 -0.4254006  2.7329886 -0.0245878304   14 2021-08-02\n135 -0.4621115  2.5299667 -0.4527557321   15 2021-08-02\n136 -0.5139527  2.6027784 -0.7375545901   16 2021-08-02\n137 -0.6042274  2.4507108 -0.5740803039   17 2021-08-02\n138 -0.5861653  2.6968138 -0.7155234903   18 2021-08-02\n139 -0.5495861  2.7161295 -1.0858622822   19 2021-08-02\n140 -0.5331188  2.6658555 -0.7847178421   20 2021-08-02\n141 -0.4685133  2.7855484 -0.7231518295   21 2021-08-02\n142 -0.4999380  2.6823232 -0.4577592358   22 2021-08-02\n143 -0.3420618  3.4370018 -0.4565179757   23 2021-08-02\n144 -0.4163222  2.4800191 -0.2878019011   24 2021-08-02\n145 -0.4142501  2.5312329 -0.2876957587   25 2021-08-02\n146 -0.4668621  2.5525665 -0.6047624726   26 2021-08-02\n147 -0.4688908  2.5887597 -0.9068635437   27 2021-08-02\n148 -0.5391178  2.6979004 -0.8599549281   28 2021-08-02\n149 -0.5825526  2.6369476 -0.9818120336   29 2021-08-02\n150 -0.5356858  2.5800013 -0.9147740655   30 2021-08-02\n151 -0.5183997 -4.5791339 -0.8599796085   31 2021-08-02\n152 -0.4946069  2.7132480 -0.5206771201   32 2021-08-02\n153 -0.5405204  2.5801862 -0.5744413976   33 2021-08-02\n154 -0.4412976  3.3780884 -0.6361958420   34 2021-08-02\n155 -0.6463959  2.6299954 -0.5624987735   35 2021-08-02\n156 -0.6087992  2.5162695 -0.3103782430   36 2021-08-02\n157 -0.5613192  2.6299762 -0.6498980336   37 2021-08-02\n158 -0.6076666  2.6135721 -0.8562301124   38 2021-08-02\n159 -0.5743061  2.5783366 -0.7183310744   39 2021-08-02\n160 -0.6343673  2.8307034 -0.7612609133   40 2021-08-02\n161 -0.2649441  3.8357068  0.0698481711    1 2021-08-25\n162 -0.2492219  4.1465574  0.1635182319    2 2021-08-25\n163 -0.2891879  3.2880286  0.0540153634    3 2021-08-25\n164 -0.3892729  3.0582468  0.0089946689    4 2021-08-25\n165 -0.2262898  6.1547732 -0.2514779990    5 2021-08-25\n166 -0.3335889  2.7617664 -0.0269560659    6 2021-08-25\n167 -0.2956375  3.0224746  0.0280182172    7 2021-08-25\n168 -0.2686606  3.2196018  0.0010159223    8 2021-08-25\n169 -0.2380668  3.7777926  0.0434079012    9 2021-08-25\n170 -0.2756267  3.6151260  0.2062654531   10 2021-08-25\n171 -0.3828091  3.9623480  0.0860920107   11 2021-08-25\n172 -0.3309504  3.0461159  0.1443419035   12 2021-08-25\n173 -0.2568976  3.6141504 -0.1961771307   13 2021-08-25\n174 -0.3168775  2.5747338 -0.1139408403   14 2021-08-25\n175 -0.2569895  3.2723095 -0.1561919432   15 2021-08-25\n176 -0.3007407  3.0141100  0.1313856963   16 2021-08-25\n177 -0.2164979  3.4557567 -0.0051956405   17 2021-08-25\n178 -0.2199860  3.7560058 -0.0308850317   18 2021-08-25\n179 -0.3006649  1.7972654  0.0416937059   19 2021-08-25\n180 -0.2356067  3.3783881  0.1208025802   20 2021-08-25\n181 -0.2329681  3.2808525 -0.0581929352   21 2021-08-25\n182 -0.2517523  3.9684567  0.0264775382   22 2021-08-25\n183 -0.2099875  3.4266607 -0.0897205516   23 2021-08-25\n184 -0.2887403  2.8435111  0.0007949469   24 2021-08-25\n185 -0.3447009  3.0183728 -0.0764145067   25 2021-08-25\n186 -0.2445589  3.1964585 -0.0502768648   26 2021-08-25\n187 -0.2380703  3.4061204  0.0990268744   27 2021-08-25\n188 -0.3200818  3.0966347  0.0094243580   28 2021-08-25\n189 -0.2282731  3.8013588  0.0401994088   29 2021-08-25\n190 -0.2082698  5.6169288  0.1535082537   30 2021-08-25\n191 -0.2578448  3.2701689  0.0355454843   31 2021-08-25\n192 -0.2064530  4.2204965  0.0133503191   32 2021-08-25\n193 -0.2352597  4.7579951  0.0388698356   33 2021-08-25\n194 -0.2362531  3.0589925 -0.0782489060   34 2021-08-25\n195 -0.2391390  4.4568049  0.0512449521   35 2021-08-25\n196 -0.2518146  4.2942715  0.0458086211   36 2021-08-25\n197 -0.3950171  3.4100838  0.1118462609   37 2021-08-25\n198 -0.2788834  4.0030344  0.0697294939   38 2021-08-25\n199 -0.2455035  4.2586817  0.1443525005   39 2021-08-25\n200 -0.3179484  3.8828494  0.1160957419   40 2021-08-25\n\n\nThen, we will convert the dataframe into a tibble:\n\ncovar.2021 &lt;- tibble(df.2021)\n\nLet’s check what we got:\n\ncovar.2021\n\n# A tibble: 200 × 12\n     blue  green    red redge   nir  ndvi  ndre   cire  sipi mndvi  plot date   \n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;  \n 1 0.0234 0.0596 0.0436 0.144 0.313 0.699 0.352 -0.379  3.64 0.336     1 2021-0…\n 2 0.0228 0.0597 0.0443 0.143 0.300 0.691 0.340 -0.362  3.55 0.383     2 2021-0…\n 3 0.0223 0.0603 0.0430 0.144 0.304 0.699 0.342 -0.367  3.48 0.300     3 2021-0…\n 4 0.0221 0.0629 0.0427 0.149 0.318 0.712 0.348 -0.387  3.43 0.364     4 2021-0…\n 5 0.0231 0.0599 0.0462 0.139 0.285 0.676 0.331 -0.341  3.56 0.444     5 2021-0…\n 6 0.0224 0.0612 0.0415 0.146 0.312 0.720 0.350 -0.377  3.38 0.249     6 2021-0…\n 7 0.0209 0.0587 0.0400 0.141 0.302 0.724 0.352 -0.362  3.41 0.301     7 2021-0…\n 8 0.0213 0.0575 0.0399 0.137 0.296 0.716 0.355 -0.353  3.49 0.341     8 2021-0…\n 9 0.0219 0.0587 0.0409 0.141 0.302 0.712 0.350 -0.364  3.48 0.326     9 2021-0…\n10 0.0226 0.0598 0.0436 0.143 0.299 0.696 0.340 -0.361  3.52 0.398    10 2021-0…\n# ℹ 190 more rows\n\n\nNow, we will need to join the CLS severity data to the spectral covariates data.\nLet’s remind how is structured the CLS data:\n\ncls\n\n# A tibble: 136 × 9\n    Plot    D0    D1    D2    D3    D4    D5    D6  year\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     0 0.25   0.9   4.58  8.75  21      NA  2021\n 2     2     0 0.75   3.68 12.4  20.8   45.4    NA  2021\n 3     3     0 2.92   7.38 18    34.6   65.2    NA  2021\n 4     4     0 0.75   5.45 17.4  42.6   75.5    NA  2021\n 5     5     0 3.38   3.45 15.4  17.2   55.2    NA  2021\n 6     6     0 0.8    1.38  8.8  12.2   15.4    NA  2021\n 7     7     0 0.375  3.05  9.1  13     31.0    NA  2021\n 8     8     0 3.7    8.4  29.4  41.2   71.8    NA  2021\n 9     9     0 2.38   4.28 13.2  15.6   38.1    NA  2021\n10    10     0 2.22   2.28 30.2  17.3   68      NA  2021\n# ℹ 126 more rows\n\n\nNote that the covar.2021 object has a date attribute in a single column and that the cls object has different columns represent dates (i.e. D0 to D5). This means that we need to change the cls object structure to be able to join its attributes to the covar.2021 object. In technical terms, our task is to convert the cls table from wide format into long format.\nWe will use the pivot_longer function from the dplyr package:\n\ncls %&gt;%\n  pivot_longer(\n    cols = starts_with(\"D\"),\n    names_to = \"date\",\n    values_to = \"severity\",\n    values_drop_na = TRUE\n  ) -&gt; long_cls\n\nNow, we will do an additional step, we will replace qualitative date values (D0, D1, …) for actual date values.\n\nchange &lt;- tibble(old = c(\"D0\", \"D1\", \"D2\", \"D3\", \"D4\"),\n                 new = c(\"2021-07-07\", \"2021-07-15\", \"2021-07-20\",\n                          \"2021-08-02\",\"2021-08-25\"))\n\nLet’s check what we got:\n\nchange\n\n# A tibble: 5 × 2\n  old   new       \n  &lt;chr&gt; &lt;chr&gt;     \n1 D0    2021-07-07\n2 D1    2021-07-15\n3 D2    2021-07-20\n4 D3    2021-08-02\n5 D4    2021-08-25\n\n\nNow, let’s do a left join to produce the changes:\n\n(ncls &lt;- left_join(long_cls, change, by = c(date = \"old\")))\n\n# A tibble: 872 × 5\n    Plot  year date  severity new       \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;     \n 1     1  2021 D0        0    2021-07-07\n 2     1  2021 D1        0.25 2021-07-15\n 3     1  2021 D2        0.9  2021-07-20\n 4     1  2021 D3        4.58 2021-08-02\n 5     1  2021 D4        8.75 2021-08-25\n 6     1  2021 D5       21    &lt;NA&gt;      \n 7     2  2021 D0        0    2021-07-07\n 8     2  2021 D1        0.75 2021-07-15\n 9     2  2021 D2        3.68 2021-07-20\n10     2  2021 D3       12.4  2021-08-02\n# ℹ 862 more rows\n\n\nFinally, we can apply a join:\n\nncovar.2021 &lt;- left_join(ncls, covar.2021, by = c(\"Plot\" = \"plot\", \"new\" = \"date\"))\n\n\nncovar.2021\n\n# A tibble: 872 × 15\n    Plot  year date  severity new      blue   green     red  redge    nir   ndvi\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1  2021 D0        0    2021…  0.0234  0.0596  0.0436  0.144  0.313  0.699\n 2     1  2021 D1        0.25 2021…  0.0205  0.0502  0.0297  0.134  0.319  0.788\n 3     1  2021 D2        0.9  2021…  0.0211  0.0532  0.0363  0.162  0.406  0.811\n 4     1  2021 D3        4.58 2021…  0.0246  0.0583  0.0334  0.204  0.467  0.852\n 5     1  2021 D4        8.75 2021…  0.0324  0.0624  0.0707  0.133  0.224  0.526\n 6     1  2021 D5       21    &lt;NA&gt;  NA      NA      NA      NA     NA     NA    \n 7     2  2021 D0        0    2021…  0.0228  0.0597  0.0443  0.143  0.300  0.691\n 8     2  2021 D1        0.75 2021…  0.0186  0.0505  0.0311  0.132  0.306  0.781\n 9     2  2021 D2        3.68 2021…  0.0211  0.0563  0.0369  0.157  0.376  0.788\n10     2  2021 D3       12.4  2021…  0.0223  0.0563  0.0340  0.192  0.409  0.825\n# ℹ 862 more rows\n# ℹ 4 more variables: ndre &lt;dbl&gt;, cire &lt;dbl&gt;, sipi &lt;dbl&gt;, mndvi &lt;dbl&gt;\n\n\n\n\n8.5.10 Change severity percentage to severity categories\nOur final step is to convert percentage severity values into ordinal severity levels.\nFirst, let’s check the severity histogram\n\n#uncomment if you need to the read the file\n#ncovar.2021 &lt;- read_csv(\"./data/ncovar_2021.csv\", show_col_types = FALSE)\n\n\nhist(ncovar.2021$severity)\n\n\n\n\n\n\n\n\nNext code produces four levels of CLS severity:\n\n\n\n\nSeverity [%]\nCategory\n\n\n\n\n0 - 1\n0\n\n\n1 - 10\n1\n\n\n10 - 20\n2\n\n\n20 - 30\n3\n\n\n30 - 40\n4\n\n\n40 - 50\n5\n\n\n50 - 100\n6\n\n\n\n\n\nfcovar.2021 &lt;- ncovar.2021 %&gt;% mutate(category=cut(severity, breaks=c(0,1,  10, 20, 30,40, 50, 100), labels=c(0,1,2,3,4,5,6)))\n\nWhat we got?\n\nsummary(fcovar.2021$category)\n\n   0    1    2    3    4    5    6 NA's \n 188  212  138   77   43   40   78   96 \n\n\nNote that the distribution of values at each disease level is not balanced.\n\n\n8.5.11 Write 2021 csv files\n\nwrite_csv(\n  ncovar.2021,\n  \"data/cercospora_data/ncovar_2021.csv\",\n  na = \"NA\",\n  col_names = TRUE,\n  append = FALSE\n)\n\n\nwrite_csv(\n  fcovar.2021,\n  \"data/cercospora_data/fcovar_2021.csv\",\n  na = \"NA\",\n  col_names = TRUE,\n  append = FALSE\n)\n\nIn this first part we have conducted a very important task: to translate UAV multispectral imagery and CLS severity ground data into a single table storing spectral covariates and severity classes which we will use next for building a prediction model.\n\n\n8.5.12 Machine learning model\nNow we will train & test a machine learning model to estimate qualitative levels of CLS disease severity using the previously created dataset.\n\n8.5.12.1 Software setup\nLet’s start by cleaning up R memory:\n\nrm(list=ls())\n\nThen, we need to install several packages (if they are not installed yet):\n\nlist.of.packages &lt;- c(\"readr\",\"terra\", \"tidyterra\", \"stars\", \"sf\", \"leaflet\", \"leafem\", \"dplyr\", \"ggplot2\", \"tidymodels\", \"xgboost\", \"tune\")\nnew.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\nif(length(new.packages)) install.packages(new.packages)\n\nNow, let’s load all the required packages:\n\nlibrary(readr)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(stars)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidymodels)\nlibrary(xgboost)\nlibrary(tune)\n\nWhat files are in the data folder under the current path:\n\nlist.files(\"data/cercospora_data\")\n\n [1] \"CLS_DS.csv\"          \"D0_2021.csv\"         \"D1_2021.csv\"        \n [4] \"D2_2021.csv\"         \"D3_2021.csv\"         \"D4_2021.csv\"        \n [7] \"fcovar_2021.csv\"     \"multispec_2021_2022\" \"multispec_2023\"     \n[10] \"ncovar_2021.csv\"     \"README.md\"          \n\n\nLet’s remind that fcovar_2021.csv is the file containing the dataset we created previously. We will use the readr package to read the data as a tibble:\n\ncovar21 &lt;- read_csv(\"data/cercospora_data/fcovar_2021.csv\", \n                    col_types = cols(category = col_character())) %&gt;%\n  drop_na()\n\nWhat we got?\n\ncovar21\n\n# A tibble: 504 × 16\n    Plot  year date  severity new          blue  green    red redge   nir  ndvi\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  2021 D1        0.25 2021-07-15 0.0205 0.0502 0.0297 0.134 0.319 0.788\n 2     1  2021 D2        0.9  2021-07-20 0.0211 0.0532 0.0363 0.162 0.406 0.811\n 3     1  2021 D3        4.58 2021-08-02 0.0246 0.0583 0.0334 0.204 0.467 0.852\n 4     1  2021 D4        8.75 2021-08-25 0.0324 0.0624 0.0707 0.133 0.224 0.526\n 5     2  2021 D1        0.75 2021-07-15 0.0186 0.0505 0.0311 0.132 0.306 0.781\n 6     2  2021 D2        3.68 2021-07-20 0.0211 0.0563 0.0369 0.157 0.376 0.788\n 7     2  2021 D3       12.4  2021-08-02 0.0223 0.0563 0.0340 0.192 0.409 0.825\n 8     2  2021 D4       20.8  2021-08-25 0.0322 0.0642 0.0736 0.131 0.213 0.487\n 9     3  2021 D1        2.92 2021-07-15 0.0198 0.0535 0.0329 0.132 0.302 0.764\n10     3  2021 D2        7.38 2021-07-20 0.0218 0.0591 0.0387 0.163 0.388 0.784\n# ℹ 494 more rows\n# ℹ 5 more variables: ndre &lt;dbl&gt;, cire &lt;dbl&gt;, sipi &lt;dbl&gt;, mndvi &lt;dbl&gt;,\n#   category &lt;chr&gt;\n\n\nWe will change data type for the category column from character to factor:\n\ncovar21$category &lt;- as.factor(covar21$category)\n\nLet’s see a summary of the dataset:\n\nsummary(covar21)\n\n      Plot            year          date              severity     \n Min.   : 1.00   Min.   :2021   Length:504         Min.   : 0.025  \n 1st Qu.:10.00   1st Qu.:2021   Class :character   1st Qu.: 0.750  \n Median :20.50   Median :2022   Mode  :character   Median : 4.312  \n Mean   :20.51   Mean   :2022                      Mean   : 9.364  \n 3rd Qu.:30.00   3rd Qu.:2023                      3rd Qu.:13.600  \n Max.   :40.00   Max.   :2023                      Max.   :60.750  \n                                                                   \n      new                  blue             green              red         \n Min.   :2021-07-07   Min.   :0.01718   Min.   :0.04180   Min.   :0.02762  \n 1st Qu.:2021-07-15   1st Qu.:0.02095   1st Qu.:0.05593   1st Qu.:0.03299  \n Median :2021-07-20   Median :0.02252   Median :0.05849   Median :0.03675  \n Mean   :2021-07-29   Mean   :0.02433   Mean   :0.05910   Mean   :0.04343  \n 3rd Qu.:2021-08-02   3rd Qu.:0.02736   3rd Qu.:0.06217   3rd Qu.:0.04706  \n Max.   :2021-08-25   Max.   :0.03781   Max.   :0.07335   Max.   :0.08514  \n                                                                           \n     redge             nir              ndvi             ndre       \n Min.   :0.1147   Min.   :0.1793   Min.   :0.4283   Min.   :0.1995  \n 1st Qu.:0.1382   1st Qu.:0.2817   1st Qu.:0.6740   1st Qu.:0.3118  \n Median :0.1533   Median :0.3467   Median :0.7835   Median :0.3717  \n Mean   :0.1580   Mean   :0.3369   Mean   :0.7294   Mean   :0.3493  \n 3rd Qu.:0.1719   3rd Qu.:0.3982   3rd Qu.:0.8198   3rd Qu.:0.3981  \n Max.   :0.2318   Max.   :0.4795   Max.   :0.8664   Max.   :0.4413  \n                                                                    \n      cire              sipi            mndvi          category\n Min.   :-0.6464   Min.   :-4.579   Min.   :-1.08586   0:153   \n 1st Qu.:-0.4919   1st Qu.: 2.794   1st Qu.:-0.39874   1:176   \n Median :-0.4192   Median : 3.228   Median :-0.08584   2:105   \n Mean   :-0.4153   Mean   : 3.166   Mean   :-0.17603   3: 32   \n 3rd Qu.:-0.3370   3rd Qu.: 3.442   3rd Qu.: 0.04676   4: 17   \n Max.   :-0.2065   Max.   : 6.155   Max.   : 0.58122   5: 13   \n                                                       6:  8   \n\n\nNow, we will filter several columns of this tibble using the dplyr package. First, get the column names:\n\nnames(covar21)\n\n [1] \"Plot\"     \"year\"     \"date\"     \"severity\" \"new\"      \"blue\"    \n [7] \"green\"    \"red\"      \"redge\"    \"nir\"      \"ndvi\"     \"ndre\"    \n[13] \"cire\"     \"sipi\"     \"mndvi\"    \"category\"\n\n\nNow, the selection:\n\ncovar21 %&gt;% select(Plot,ndvi,ndre, cire, sipi, mndvi, category) -&gt; covar\n\nLet’s check what we get:\n\ncovar\n\n# A tibble: 504 × 7\n    Plot  ndvi  ndre   cire  sipi    mndvi category\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   \n 1     1 0.788 0.398 -0.377  3.23  0.0206  0       \n 2     1 0.811 0.422 -0.492  3.29 -0.170   0       \n 3     1 0.852 0.394 -0.609  2.73 -0.933   1       \n 4     1 0.526 0.260 -0.265  3.84  0.0698  1       \n 5     2 0.781 0.392 -0.362  3.28  0.0250  0       \n 6     2 0.788 0.401 -0.457  3.31 -0.0483  1       \n 7     2 0.825 0.362 -0.529  2.63 -0.470   2       \n 8     2 0.487 0.242 -0.249  4.15  0.164   3       \n 9     3 0.764 0.382 -0.358  3.25  0.00614 1       \n10     3 0.784 0.399 -0.476  3.27 -0.0954  1       \n# ℹ 494 more rows\n\n\n\n\n8.5.12.2 CLS classification using machine learning (ML)\nThe overall process to classify the crop disease under study will be conducted using the tidymodels framework which is an extension of the tidyverse suite. It is especially focused towards providing a generalized way to define, run and optimize ML models in R. See https://rstudio-connect.hu.nl/caait/\n\n8.5.12.2.1 Exploratory analysis\nAs a first step in modeling, it’s always a good idea to plot the data.\nLet’s try a boxplot:\n\nplot &lt;- ggplot(covar, aes(x=category, y=mndvi, color = category))+\n     geom_boxplot()+\n     r4pde::theme_r4pde()+\n     theme(legend.position = \"none\" )\n\nWarning: replacing previous import 'car::recode' by 'dplyr::recode' when\nloading 'r4pde'\n\nplot\n\n\n\n\n\n\n\n\nNext, we will do a scatterplot to visualize the relationship between the ndvi and mndvi indices:\n\nggplot(covar) +\n  aes(x = ndvi, y = mndvi, color = category) +\n  geom_point(shape = 16, size = 3) +\n  labs(x = \"ndvi\", y = \"mndvi\", color = \"Severity category\") +\n  r4pde::theme_r4pde()+\n    theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n8.5.12.2.2 Splitting the data\nNext step is to divide the data into a training and a test set. The set.seed() function can be used for reproducibility of the computations that are dependent on random numbers. By default, the training/testing split is 0.75 to 0.25.\n\nset.seed(42)\ndata_split &lt;- initial_split(data = covar)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\nLet’s check the result:\n\ndata_train\n\n# A tibble: 378 × 7\n    Plot  ndvi  ndre   cire  sipi   mndvi category\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n 1    13 0.736 0.380 -0.316  3.49  0.243  0       \n 2    37 0.714 0.357 -0.371  3.53  0.307  0       \n 3     4 0.789 0.390 -0.389  3.15 -0.0687 1       \n 4    39 0.798 0.408 -0.408  3.29 -0.135  1       \n 5    19 0.807 0.413 -0.501  3.29 -0.293  1       \n 6    19 0.595 0.237 -0.301  1.80  0.0417 4       \n 7    37 0.809 0.420 -0.483  3.32 -0.167  1       \n 8    31 0.820 0.385 -0.508  2.86 -0.376  1       \n 9    40 0.562 0.280 -0.318  3.88  0.116  2       \n10    32 0.428 0.214 -0.206  4.22  0.0134 2       \n# ℹ 368 more rows\n\n\n\n\n8.5.12.2.3 Defining the model\nWe will use a gradient boosting machine algorithm which seems a robust algorithm for different applications.\n\nspec_lr &lt;-\nboost_tree(\n  mode = \"classification\",\n  trees=1000,\n  #tree_depth = tune::tune(),\n  #learn_rate = tune::tune(),\n  #loss_reduction = tune::tune()\n  ) %&gt;% \n  set_engine(\"xgboost\", objective = \"multi:softprob\", eval_metric = \"mlogloss\",) \n\n\n\n8.5.12.2.4 Defining the recipe\nThe recipe() function to be used here has two arguments:\n\nA formula. Any variable on the left-hand side of the tilde (~) is considered the model outcome (here, outcome). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (.) to indicate all other variables as predictors.\nThe data. A recipe is associated with the data set used to create the model. This will typically be the training set, so data = data_train here.\n\n\nrecipe_lr &lt;-\n  recipe(category ~ ., data_train) %&gt;%\n  add_role(Plot, new_role = \"id\") |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_corr(all_predictors())\n\n\n\n8.5.12.2.5 Model performance metrics\nNext, we need to specify what we would like to see for determining the performance of the model. Different modelling algorithms have different types of metrics. Because we have a multiclass classification problem (different categories of severity), we will chose the accuracy evaluation metric here.\n\n\n8.5.12.2.6 Combine model specification and recipe into a workflow\nWe will want to use our recipe across several steps as we train and test our model. We will:\n\nProcess the recipe using the training set: This involves any estimation or calculations based on the training set. For our recipe, the training set will be used to determine which predictors should be converted to dummy variables and which predictors will have zero-variance in the training set, and should be slated for removal.\nApply the recipe to the training set: We create the final predictor set on the training set.\nApply the recipe to the test set: We create the final predictor set on the test set. Nothing is recomputed and no information from the test set is used here; the dummy variable and zero-variance results from the training set are applied to the test set.\n\nTo simplify this process, we can use a model workflow, which pairs a model and recipe together. This is a straightforward approach because different recipes are often needed for different models, so when a model and recipe are bundled, it becomes easier to train and test workflows. We’ll use the workflows package from tidymodels to bundle our model with our recipe.\nNow we are ready to setup our complete modelling workflow. This workflow contains the model specification and the recipe.\n\nwf_cls_wilt &lt;-\n  workflow(\n    spec = spec_lr,\n    recipe_lr\n    )\n\nwf_cls_wilt\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_zv()\n• step_corr()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = 1000\n\nEngine-Specific Arguments:\n  objective = multi:softprob\n  eval_metric = mlogloss\n\nComputational engine: xgboost \n\n\n\n\n8.5.12.2.7 Fitting the GBM regression model\nNow we use the workflow previously created to fit the model on our training data. We use the training partition of the data.\n\nfit_lr &lt;- wf_cls_wilt  %&gt;% \n  fit(data = data_train)\n\nLet’s check the output:\n\nfit_lr\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_zv()\n• step_corr()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 9.1 Mb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, \n    subsample = 1), data = x$data, nrounds = 1000, watchlist = x$watchlist, \n    verbose = 0, objective = \"multi:softprob\", eval_metric = \"mlogloss\", \n    nthread = 1, num_class = 7L)\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", objective = \"multi:softprob\", eval_metric = \"mlogloss\", nthread = \"1\", num_class = \"7\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 5 \nniter: 1000\nnfeatures : 5 \nevaluation_log:\n     iter training_mlogloss\n    &lt;num&gt;             &lt;num&gt;\n        1         1.5109977\n        2         1.2670614\n---                        \n      999         0.4083580\n     1000         0.4083564\n\n\nNow, we will use the fitted model to estimate CLS severity in the training data:\n\nrf_training_pred &lt;- \n  predict(fit_lr, data_train) %&gt;% \n  bind_cols(predict(fit_lr, data_train) %&gt;% \n  # Add the true outcome data back in\n  bind_cols(data_train %&gt;% \n              select(category)))\n\nNew names:\n• `.pred_class` -&gt; `.pred_class...1`\n• `.pred_class` -&gt; `.pred_class...2`\n\n\nWhat we got?\n\nnames(rf_training_pred) &lt;- c (\".pred_class_1\", \".pred_class_2\", \"category\")\n\nLet’s estimate the training accuracy:\n\nrf_training_pred %&gt;%                # training set predictions\n  accuracy(truth = category, .pred_class_1) -&gt; acc_train\nacc_train\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.751\n\n\nThe accuracy of the model on the training data is 0.75 which is above 0.5 (mere chance). This basically means that the model was able to learn predictive patterns from the training data. To see if the model is able to generalise what it learned when exposed to new data, we evaluate the model on our hold-out (or so-called test data). We created a test dataset when splitting the data at the start of the modelling.\n\n\n8.5.12.2.8 Evaluating the model on the test data\nNow, we will use the fitted model to estimate health status in the testing data:\n\nlr_testing_pred &lt;- \n  predict(fit_lr, data_test) %&gt;% \n  bind_cols(predict(fit_lr, data_test, type = \"prob\")) %&gt;% \n  bind_cols(data_test %&gt;% select(category))\n\nWhat we got:\n\nlr_testing_pred\n\n# A tibble: 126 × 9\n   .pred_class  .pred_0 .pred_1 .pred_2 .pred_3 .pred_4 .pred_5 .pred_6 category\n   &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n 1 2            2.55e-5 1.11e-3 9.99e-1 1.00e-5 2.64e-5 4.22e-6 3.48e-6 2       \n 2 3            2.12e-5 3.86e-4 4.99e-1 4.99e-1 8.53e-4 9.82e-4 9.26e-5 3       \n 3 1            2.82e-3 9.97e-1 4.03e-6 1.35e-6 6.22e-6 4.66e-6 9.53e-7 0       \n 4 3            4.20e-5 2.63e-3 1.06e-3 9.96e-1 1.46e-4 2.71e-4 8.61e-6 2       \n 5 1            7.37e-5 9.35e-1 2.20e-2 4.25e-2 5.26e-4 3.03e-4 1.01e-5 2       \n 6 1            5.23e-2 9.48e-1 1.83e-5 3.82e-6 1.31e-5 7.59e-6 4.16e-6 0       \n 7 1            9.68e-2 9.03e-1 1.71e-4 4.72e-5 4.16e-5 3.29e-5 1.84e-5 1       \n 8 1            1.18e-5 9.99e-1 3.36e-5 6.69e-4 1.75e-5 3.35e-6 1.03e-6 1       \n 9 0            9.96e-1 1.79e-3 1.32e-3 2.37e-4 1.46e-4 1.80e-5 9.87e-6 1       \n10 1            1.58e-3 9.98e-1 4.67e-6 8.22e-7 2.83e-6 1.66e-6 8.95e-7 1       \n# ℹ 116 more rows\n\n\nLet’s compute the testing accuracy:\n\nlr_testing_pred %&gt;%                   # test set predictions\n  accuracy(category, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.492\n\n\nThe resulting accuracy is much lower than the accuracy on the training data. This means that we would need to figure out how to improve our classification model.\n\n## Let's plot the AUC-ROC \nlr_testing_pred %&gt;% \n  conf_mat(truth = category, .pred_class) -&gt; cm \ncm\n\n          Truth\nPrediction  0  1  2  3  4  5  6\n         0 25 11  0  0  0  0  0\n         1 15 22  8  3  1  1  0\n         2  0  9 11  6  1  0  2\n         3  0  1  3  4  0  1  0\n         4  0  0  0  0  0  0  0\n         5  0  0  1  0  1  0  0\n         6  0  0  0  0  0  0  0\n\n\nNote what are the disease categories more difficult to discriminate.\nIn this section we trained and tested a gradient boosting model (GBM) using three spectral indices as predictor variables (i.e. NDVI, NDRE, CIRE, SIPI, and MNDVI) obtained from imagery and field data. Accuracy metrics show that both data & model need improvement.\nSome ideas for further experiments: - Include data collected in 2022 and 2023 - Refine CLS severity categories (see (Del Ponte et al. 2019)) - Compute other vegetation indices as advised in relevant literature - Compute other covariates (e.g. image texture metrics)\nThis section illustrated how to use VIs derived from UAV-based multispectral imagery and ground data to develop an classification model for estimating CLS in table beet. The results showed that a gradient boosting machine model is unable to estimate severity from several VIs with a good accuracy. We can explore the use of additional covariates to improve accuracy of the initial model.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Remote sensing</span>"
    ]
  },
  {
    "objectID": "data-remote-sensing.html#summary",
    "href": "data-remote-sensing.html#summary",
    "title": "8  Remote sensing",
    "section": "8.6 Summary",
    "text": "8.6 Summary\nWe have completed a journey that hopefully may serve as foundation to apply remote sensing data and techniques to infer information useful for monitoring crop diseases.\nOur journey consisted of three stages. In the first section we reviewed basic remote sensing concepts and provided a summary of applications of remote sensing of crop diseases. In the second section we navigated a case study on identification of banana Fusarium wilt from multispectral UAV imagery. In the third section we moved to a case study dealing with estimation of cercospora leaf spot disease severity on table beet. For the two case studies we used datasets publicly available.\nNow, it is time to conclude with several reflections about: (i) what we learned in this chapter; (ii) what are the potential and limitations of remote sensing for identification & monitoring crop diseases; and (iii) what are main challenges to overcome to advance the use of remote sensing techniques for identification and quantification of crop diseases.\n\n8.6.1 Lesson learned in this journey\nFirst of all, we started to become familiar with concepts and techniques that can guide us in future studies focused on the use of remote sensing for crop disease monitoring. Although they are only the tip of the remote sensing iceberg, which we must know in depth, they represent part of the foundation necessary to be successful in any research related to the topic of this chapter.\nAlong the way, we learned to use some R libraries that allow us to: (i) read, visualize and process multispectral images and geospatial data; (ii) merge raster and vector data and extract spectral covariates related to plant biophysical, biochemical, and functional traits; and (iii) apply machine learning algorithms to infer information useful for plant disease identification and quantification.\nThe results obtained in our hands-on activities suggest that the application of machine learning algorithms to learning from data is driven by an iterative process: starting from an initial workflow, users need to iteratively modify their workflow, based on previous results, to improve performance.\nWe have learned that, in order to use data properly, research data needs to be fully described and documented. Otherwise, users may become frustrated or lost in guessing, cleaning, and pre-processing tasks. We have also learned that if the code is well documented, it is feasible to reuse it in future studies.\n\n\n8.6.2 Potential and limitations of remote sensing for crop disease monitoring\nA variety of remote sensing (RS) devices can be used to obtain data on the spectral response of plants and crops to diseases and insect damage. Processing and analyzing such data allows to estimate information useful for monitoring crops and analysing different stages of plant health. As individual devices have some limitations, multimodal remote sensing data & techniques (i.e. reflectance, fluorescence, thermal, microwave, lidar, multiangular sensing) are increasingly being used in research.\nIn the multimodal setting, data from different platforms & instruments are used together to obtain a better outcome. Multimodal methods allow for the integration of spectral responses from different & complementary electromagnetic radiation ranges, allowing to quantify different plant responses (e.g. stomatal closure, increased respiration, decreased chlorophyll, altered pigments, modified biochemistry, photosynthetic inhibition, altered growth, modified leaf angle, water content), and analyzing them to obtain a comprehensive understanding of disease progression, including eventual recovery. By using multimodal methods, it is possible to obtain a more accurate and detailed analysis of crop conditions, leading to advanced knowledge, improved decision-making and ultimately, better crop health management.\n\n\n8.6.3 Main challenges to overcome\nBuilding more resilient agricultural systems requires concurrent and near-real-time and reliable crop information for effective decision making. Using remote sensing data & techniques for monitoring crop health can become a relevant method to derive crop information at leaf, canopy, field, and farm scales.\nIn spite of the increasing literature reporting the application of remote sensing for monitoring crop diseases, it seems that there is a lack of open, transparent and reproducible data and methods to ensure a robust evaluation of performance, reliability and generalization of machine learning and deep learning models, a necessary pre-requisite to safely advance the applicability of remote sensing for crop monitoring in the field.\nTo fully realize the potential of remote sensing for estimating crop health status, there is a need of reliable ground truth data to enable both data calibration (i.e. linking in-situ to remote values) and accuracy assessment of estimations. Since field measurements require a lot of work and time, there is always a risk that the sampling is not correct (e.g. not enough samples, incorrect sample location, or the sampled area does not match the sensor footprint).\nBoth collaborative research and crowdsourcing could become a solution to overcome the restrictions of ground-truth data accessibility as well as the lack of open, transparent, and reusable remote sensing data & workflows. It is anticipated that joint multidisciplinary research as well as user participation in the complete process of crop monitoring from remote sensing could improve the reliability of crop health information.\n\n\n\n\nDel Ponte, E. M., Mahlein, A.-K., and Bock, C. H. 2024. Plant disease quantification. In Elsevier, pp. 211–225. https://doi.org/10.1016/b978-0-12-822429-8.00006-6.\n\n\nDel Ponte, E. M., Nelson, S. C., and Pethybridge, S. J. 2019. Evaluation of App-Embedded Disease Scales for Aiding Visual Severity Estimation of Cercospora Leaf Spot of Table Beet. Plant Disease 103:1347–1356. https://doi.org/10.1094/pdis-10-18-1718-re.\n\n\nHank, T. B., Berger, K., Bach, H., Clevers, J. G. P. W., Gitelson, A., Zarco-Tejada, P., and Mauser, W. 2018. Spaceborne Imaging Spectroscopy for Sustainable Agriculture: Contributions and Challenges. Surveys in Geophysics 40:515–551. https://doi.org/10.1007/s10712-018-9492-0.\n\n\nHuichun YE, Senzheng CHEN, Anting GUO, Chaojia NIE, and Jingjing WANG. 2022. A dataset of UAV multispectral images for banana Fusarium wilt survey. https://doi.org/10.57760/SCIENCEDB.07000.\n\n\nJones, H. G., and Vaughan, R. A. 2010. Remote sensing of vegetation: Principles, techniques, and applications. Oxford, United Kingdom: Oxford University Press.\n\n\nKouadio, L., El Jarroudi, M., Belabess, Z., Laasli, S.-E., Roni, M. Z. K., Amine, I. D. I., Mokhtari, N., Mokrini, F., Junk, J., and Lahlali, R. 2023. A Review on UAV-Based Applications for Plant Disease Detection and Monitoring. Remote Sensing 15:4273. https://doi.org/10.3390/rs15174273.\n\n\nLowe, A., Harrison, N., and French, A. P. 2017. Hyperspectral image analysis techniques for the detection and classification of the early onset of plant disease and stress. Plant Methods 13. https://doi.org/10.1186/s13007-017-0233-z.\n\n\nOerke, E.-C. 2020. Remote Sensing of Diseases. Annual Review of Phytopathology 58:225–252. https://doi.org/10.1146/annurev-phyto-010820-012832.\n\n\nPegg, K. G., Coates, L. M., O’Neill, W. T., and Turner, D. W. 2019. The epidemiology of fusarium wilt of banana. Frontiers in Plant Science 10. https://doi.org/10.3389/fpls.2019.01395.\n\n\nSaif, M. S., Chancia, R., Pethybridge, S., Murphy, S. P., Hassanzadeh, A., and Aardt, J. van. 2023. Forecasting Table Beet Root Yield Using Spectral and Textural Features from Hyperspectral UAS Imagery. Remote Sensing 15:794. https://doi.org/10.3390/rs15030794.\n\n\nSaif, M. S., Chancia, R., Sharma, P., Murphy, S., Raqueno, N., Bauch, T., Pethybridge, S., and Aardt, J. van. 2024. Data for: Estimation of cercospora leaf spot disease severity in table beets from UAS multispectral images. https://doi.org/10.17632/V9B7RWRWX9.1.\n\n\nSkaracis, G. N., Pavli, O. I., and Biancardi, E. 2010. Cercospora Leaf Spot Disease of Sugar Beet. Sugar Tech 12:220–228. https://doi.org/10.1007/s12355-010-0055-z.\n\n\nTan, W., Li, K., Liu, D., and Xing, W. 2023. Cercospora leaf spot disease of sugar beet. Plant Signaling & Behavior 18. https://doi.org/10.1080/15592324.2023.2214765.\n\n\nYe, H., Huang, W., Huang, S., Cui, B., Dong, Y., Guo, A., Ren, Y., and Jin, Y. 2020. Recognition of Banana Fusarium Wilt Based on UAV Remote Sensing. Remote Sensing 12:938. https://doi.org/10.3390/rs12060938.",
    "crumbs": [
      "Epidemic data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Remote sensing</span>"
    ]
  },
  {
    "objectID": "temporal-dpc.html",
    "href": "temporal-dpc.html",
    "title": "9  Disease progress curves",
    "section": "",
    "text": "9.1 How epidemics occur\nBefore knowing how epidemics develop in time, it is important to understand how an epidemic occur. An epidemic begins when the primary inoculum (a variable number of propagules able to infect the plant) that is surviving somewhere establishes an intimate contact with individuals of the host population - this process is called infection. These inocula are usually surviving externally to the plant host and need to disperse (move), passively or by means of a vector, to reach the plant. It can also be that a growing host encounter a localized (static) source of inoculum.\nOnce the infection is established, the pathogen colonizes the plant tissues and disease symptoms are noticed. When this happens, the incubation period can be measured in time units. A successful colonization will lead to reproduction of the pathogen inside and/or external to the crop, and so the latent period is completed, and can also be measure in time units. Finally, the infectious period takes place and continues until the pathogen is not capable of producing the secondary inoculum on the infected site.\nflowchart\n  A[Infection] --&gt; B[Colonization]\n  B --&gt; C[Reproduction]\n  C -. New inoculum .-&gt; D[Dispersal]\n  E[Survival] -- Primary inoculum --&gt; D\n  D  --&gt; A\n  D  -.-&gt; A\n  C --&gt; E \n\n\n\n\nFigure 9.1: Five main processes of the disease cycle\nEpidemiologists are generally interested in determining the length of the incubation, latent, and infectious periods as influenced by factors related to the host, pathogen, or environment. This is relevant because the longer it takes for the completion of the incubation and latent periods, the lower the potential number of repeated cycles. In summary, a single “infection cycle” represents all events that occur from infection to dispersal, and this occurs only once for many diseases, while for others there may be multiple cycles, which are defined as an “infection chain.”\nCode\nlibrary(tidyverse)\nlibrary(r4pde)\nperiods &lt;- tibble::tribble(\n  ~period, ~length, ~color, ~order,\n  \"Incubation\", 10, 0, 1,\n  \"Latent\" , 15, 0, 2,\n  \"Infectious\", 25, 15, 3\n)\n\np &lt;- periods |&gt; \n  ggplot(aes(reorder(period, order), length, fill = period))+\n  geom_col()+\n  geom_col(aes(period, color), color = \"white\", fill = \"white\")+\n  coord_flip()+\n  theme_void()+\n  theme(legend.position = \"none\")+\n  annotate(geom = \"text\", x = 0.5, y = 15, label = \"----- Time ---&gt;\")+\n  annotate(geom = \"text\", x = 1, y = 5, label = \"Incubation\", color = \"white\")+\n  annotate(geom = \"text\", x = 2, y = 8, label = \"Latent\", color = \"white\")+\n  annotate(geom = \"text\", x = 3, y = 20, label = \"Infectious\", color = \"white\")+\n  annotate(geom = \"text\", x = 1, y = 10.5, label = \"Visible symptoms\", angle = 90, size = 1.7)+\n  annotate(geom = \"text\", x = 2, y = 15.5, label = \"Reproduction starts\", angle = 90, size =1.7)+\n  annotate(geom = \"text\", x = 3, y = 25.5, label = \"Reproduction ends\", angle = 90, size =1.7)+\n  scale_fill_manual(values = c(\"darkgreen\",  \"brown\", \"darkorange\"))+\n  geom_segment(mapping=aes(x=0.6, y=0, xend=0.6, yend=10), arrow=arrow(ends='both'), size=1, color = \"black\")+ \n  geom_segment(mapping=aes(x=1.6, y=0, xend=1.6, yend=15), arrow=arrow(ends='both'), size=1, color = \"black\")  +\n   geom_segment(mapping=aes(x=2.6, y=15, xend=2.6, yend=25), arrow=arrow(ends='both'), size=1, color = \"black\") \n  library(png)\n  library(cowplot)\n  incubation &lt;- readPNG(\"imgs/incubation3.png\", native = TRUE)\n  latent &lt;- readPNG(\"imgs/latent3.png\", native = TRUE)\n  p2 &lt;- p + draw_image(incubation , x = 0.5, y = 13, scale = 5)+\n    draw_image(latent , x = 1.5, y = 20, scale = 5)\n  ggsave(\"imgs/periods.png\", width =6, height =2, bg = \"white\")",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Disease progress curves</span>"
    ]
  },
  {
    "objectID": "temporal-dpc.html#how-epidemics-occur",
    "href": "temporal-dpc.html#how-epidemics-occur",
    "title": "9  Disease progress curves",
    "section": "",
    "text": "Figure 9.2: Three time-related epidemiological periods and their relations with stages of the disease cycle including colonization (symptoms) and reproduction (sporulation in the case of fungi). Drawings of apple scab symptoms and signs adapted from Agrios (2005)",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Disease progress curves</span>"
    ]
  },
  {
    "objectID": "temporal-dpc.html#disease-curves",
    "href": "temporal-dpc.html#disease-curves",
    "title": "9  Disease progress curves",
    "section": "9.2 Disease curves",
    "text": "9.2 Disease curves\nA key understanding of the epidemics relates to the knowledge of rates and patterns. Epidemics can be viewed as dynamic systems that change their state as time goes. The first and simplest way to characterize such changes in time is to produce a graphical plot called disease progress curve (DPC). This curve can be obtained as long as the intensity of the disease (y) in the host population is assessed sequentially in time (t).\nA DPC summarizes the interaction of the three main components of the disease triangle occurring during the epidemic. The curves can vary greatly in shape according to variations in each of the components, in particular due to management practices that alter the course of the epidemics and for which the goal is to stop disease increase. We can create a data frame in R for a single DPC and make a plot using ggplot. By convention we use t for time and y for disease intensity, expressed in percentage (0 to 100%).\nFirstly, let’s load the essential R packages and set up the environment.\n\nlibrary(tidyverse) # essential packages \ntheme_set(theme_r4pde()) # set global theme\n\nThere are several ways to create a data frame in R. I like to use the tribble function as below. The entered data will be assigned to a dataframe called dpc.\n\ndpc &lt;- \n  tribble(\n   ~t,  ~y, \n   0,  0.1, \n   7,  1, \n  14,  9, \n  21,  25, \n  28,  80, \n  35, 98, \n  42, 99, \n  49, 99.9\n  )\n\nNow the plot\n\ndpc1 &lt;- dpc |&gt;\n  ggplot(aes(t, y)) +\n  theme_r4pde()+\n  geom_line(size = 1)+\n  geom_point(size = 3, shape = 16)+\n  labs(x = \"Assessment time (days)\",\n       y = \"Disease intensity (%)\")\n\nggsave(\"imgs/dpc1.png\", dpc1)\n\n\n\n\n\n\n\nFigure 9.3: A typical disease progress curve for an epidemic that reaches the maximum value",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Disease progress curves</span>"
    ]
  },
  {
    "objectID": "temporal-dpc.html#epidemic-classification",
    "href": "temporal-dpc.html#epidemic-classification",
    "title": "9  Disease progress curves",
    "section": "9.3 Epidemic classification",
    "text": "9.3 Epidemic classification\nVanderplank analysed the shapes of great number of epidemic curves and classified the epidemics into two basic types: monocyclic or polycyclic (Vanderplank 1963). In monocyclic epidemics, inoculum capable of infecting the crop is not produced during the epidemics. These epidemics are initiated and maintained only by the primary inoculum. There is no secondary infection and hence no further spread of newly produced inoculum among the host individuals. Tipically, the progress curves for monocyclic epidemics have a saturation type shape. An example of a monocyclic epidemic is the disease known as wheat smut caused by Tilletia caries. In this disease, although two types of spores are produced during a single cycle (teliospores and basidiospores), no additional infection cycles occur within the same growing season.\nConversely, when the secondary inoculum produced during the epidemics is capable of infecting the host during the same crop cycle, a polycyclic epidemic is established. The number of repeated cycles just depends on how long it takes to complete a single infection cycle. These epidemics most commonly present a sigmoid shape Figure 9.4. An example of a polycyclic epidemic is soybean rust as it undergoes repeated infection cycles with uredospores on soybean.\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_bw(base_size = 16))\n\nlibrary(epifitter)\npolyc &lt;- sim_logistic(N = 50, dt = 5, \n                      y0 = 0.01, r = 0.2, \n                      K = 0.8, n = 1, \n                      alpha =0)\n\np &lt;- polyc |&gt; \n  ggplot(aes(time, y))+\n  geom_point(aes(time, y), size =19, shape =1)+\n  geom_line()+\n  ylim(0,1)+\n  theme_r4pde()+\n  labs(x = \"Time\", y = \"Disease intensity\")\n\n\nmonoc &lt;- sim_monomolecular(N = 50, dt = 5, \n                           y0 = 0.01, r = 0.1,\n                           K = 0.8, n = 1, \n                           alpha =0)\nlibrary(ggforce)\nm &lt;- monoc |&gt; \n  ggplot(aes(time, y))+\n  geom_point(aes(x = 25, y = 0.5), size =90, shape = 1)+\n   geom_line()+\n theme_r4pde()+\n  ylim(0,1)+\n  labs(x = \"Time\", y = \"Disease intensity\")\n\nlibrary(patchwork)\ncycles &lt;- m | p\nggsave(\"imgs/cycles.png\", bg = \"white\", width = 8, height =4)\n\n\n\n\n\n\n\n\nFigure 9.4: Hypothetical curves for monocyclic (left) and polycyclic (right) epidemics. Each circle represents a single infection cycle.",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Disease progress curves</span>"
    ]
  },
  {
    "objectID": "temporal-dpc.html#curve-descriptors-and-audpc",
    "href": "temporal-dpc.html#curve-descriptors-and-audpc",
    "title": "9  Disease progress curves",
    "section": "9.4 Curve descriptors and AUDPC",
    "text": "9.4 Curve descriptors and AUDPC\nThe depiction and analysis of disease progress curves can provide useful information for gaining understanding of the underlying epidemic process. The curves are extensively used to evaluate how disease control measures affect epidemics. When characterizing DPCs, a researcher may be interested in describing and comparing epidemics that result from different treatments, or simply in their variations as affected by changes in environment, host or pathogen.\nThe precision and complexity of the analysis of progress curve data depends on the objective of the study. In general, the goal is to synthesize similarities and differences among epidemics based on common descriptors of the disease progress curves. For example, the simple appraisal of the disease intensity at any time during the course of the epidemic should be sufficient for certain situations. Furthermore, a few quantitative and qualitative descriptors can be extracted including:\n\nEpidemic duration\nMaximum disease\nCurve shape\nArea under the area under the disease progress curve (AUDPC).\n\nLet’s visualize the AUDPC in the same plot that we produced above.\n\ndpc2 &lt;- dpc |&gt;\n  ggplot(aes(t, y)) +\n  labs(x = \"Assessment time (days)\",\n       y = \"Disease intensity (%)\")+\n    geom_area(fill = \"#339966\")+\n    geom_line(linewidth = 1)+\n  theme_r4pde()+\n  geom_point(size = 3, shape = 16)+\n  scale_x_continuous(breaks = c(0, 7, 14, 21, 28, 35, 42))\nggsave(\"imgs/dpc2.png\")\n\n\n\n\n\n\n\nFigure 9.5: Representation of the area under the disease progress curve\n\n\n\nThe AUDPC summarizes the “total measure of disease stress” and is largely used to compare epidemics (Jeger and Viljanen-Rollinson 2001). The most common approach to calculate AUDPC is the trapezoidal method, which splits the disease progress curves into a series of rectangles, calculating the area of each of them and then summing the areas. Let’s extend the plot code to show those rectangles using the annotate function.\n\n\nCode\ndpc3 &lt;- dpc |&gt;\n  ggplot(aes(t, y)) +\n  theme_r4pde()+\n  labs(x = \"Assessment time (days)\",\n       y = \"Disease intensity (%)\")+\n  annotate(\"rect\", xmin = dpc$t[1], xmax = dpc$t[2], \n           ymin = 0, ymax = (dpc$y[1]+ dpc$y[2])/2, \n           color = \"white\", fill = \"#339966\")+\n   annotate(\"rect\", xmin = dpc$t[2], xmax = dpc$t[3], \n            ymin = 0, ymax = (dpc$y[2]+ dpc$y[3])/2, \n            color = \"white\", fill = \"#339966\")+\n   annotate(\"rect\", xmin = dpc$t[3], xmax = dpc$t[4], \n            ymin = 0, ymax = (dpc$y[3]+ dpc$y[4])/2,\n            color = \"white\", fill = \"#339966\")+\n   annotate(\"rect\", xmin = dpc$t[4], xmax = dpc$t[5], \n            ymin = 0, ymax = (dpc$y[4]+ dpc$y[5])/2, \n            color = \"white\", fill = \"#339966\")+\n   annotate(\"rect\", xmin = dpc$t[5], xmax = dpc$t[6], \n            ymin = 0, ymax = (dpc$y[5]+ dpc$y[6])/2, \n            color = \"white\",fill = \"#339966\")+\n   annotate(\"rect\", xmin = dpc$t[6], xmax = dpc$t[7], \n            ymin = 0, ymax = (dpc$y[6]+ dpc$y[7])/2, \n            color = \"white\", fill = \"#339966\")+\n  annotate(\"rect\", xmin = dpc$t[7], xmax = dpc$t[8], \n            ymin = 0, ymax = (dpc$y[7]+ dpc$y[8])/2, \n            color = \"white\", fill = \"#339966\")+\n  geom_line(linewidth = 1)+\n  geom_point(size = 3, shape = 16)+\n  annotate(geom = \"text\", x = 36.5, y = 50,\n           label = \"AUDPC = 2534\" , size = 4)+\n  scale_x_continuous(breaks = c(0, 7, 14, 21, 28, 35, 42, 49))\nggsave(\"imgs/dpc3.png\")\n\n\n\n\n\n\n\n\nFigure 9.6: Representation of the area under the disease progress curve calculated using the trapezoidal method\n\n\n\nIn R, we can obtain the AUDPC for the DPC we created earlier using the AUDPC function offered by the epifitter package. Because we are using the percent data, we need to set the argument y_proportion = FALSE. The function returns the absolute AUDPC. If one is interested in relative AUDPC, the argument type should be set to \"relative\". There is also the alternative to AUDPC, the area under the disease progress stairs (AUDPS) (Simko and Piepho 2012).\n\nlibrary(epifitter)\nAUDPC(dpc$t, dpc$y, \n      y_proportion = FALSE)\n\n[1] 2534\n\n# The relative AUDPC \nAUDPC(dpc$t, dpc$y, \n      y_proportion = FALSE, \n      type = \"relative\")\n\n[1] 0.5171429\n\n# To calculate AUDPS, the alternative to AUDPC\nAUDPS(dpc$t, dpc$y, \n      y_proportion = FALSE)\n\n[1] 2884\n\n\n\n\n\n\nJeger, M. J., and Viljanen-Rollinson, S. L. H. 2001. The use of the area under the disease-progress curve (AUDPC) to assess quantitative disease resistance in crop cultivars. Theoretical and Applied Genetics 102:32–40. https://doi.org/10.1007/s001220051615.\n\n\nSimko, I., and Piepho, H.-P. 2012. The Area Under the Disease Progress Stairs: Calculation, Advantage, and Application. Phytopathology® 102:381–389. https://doi.org/10.1094/phyto-07-11-0216.\n\n\nVanderplank, J. 1963. Plant disease epidemics and control. Elsevier. https://doi.org/10.1016/c2013-0-11642-x.",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Disease progress curves</span>"
    ]
  },
  {
    "objectID": "temporal-models.html",
    "href": "temporal-models.html",
    "title": "10  Population models",
    "section": "",
    "text": "10.1 Non-flexible models\nMathematical models can be fitted to the DPC data to express epidemic progress in terms of rates and absolute/relative quantities. The latter can be accomplished using population dynamics (or growth-curve) models for which the estimated parameters are usually meaningful biologically and appropriately describe epidemics that do not decrease in disease intensity. By fitting an appropriate model to the progress curve data, another set of parameters is available to the researcher when attempting to represent, understand or compare epidemics.\nThe family of models that describe the growth of epidemics, hence population dynamics model, are known as deterministic models of continuous time (Madden et al. 2007). These models are usually fitted to DPC data to obtain two or more biologically meaningful parameters. Here, these models and their formulations are shown using R scripts to simulate the theoretical curves for each model.\nThese population dynamics models require at least two parameters, hence they are known as non-flexible, as opposed to the flexible ones for which there are at least one additional (third) parameter.\nFollowing the convention proposed by (Madden et al. 2007) in their book “The study of plant disease epidemics”:\nNow we can proceed and learn which non-flexible models exist and for which situation they are more appropriate.",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Population models</span>"
    ]
  },
  {
    "objectID": "temporal-models.html#non-flexible-models",
    "href": "temporal-models.html#non-flexible-models",
    "title": "10  Population models",
    "section": "",
    "text": "time is represented by \\(t\\)\ndisease intensity by \\(y\\)\nthe rate of change in \\(y\\) between two time units is represented by \\(\\frac{dy}{dt}\\)\n\n\n\n10.1.1 Exponential\nThe differential equation for the exponential model is given by\n\\(\\frac{dy}{dt} = r_E.y\\),\nwhere \\(r_E\\) is the apparent infection rate (subscript E for this model) (sensu Vanderplank) and \\(y\\) is the disease intensity. Biologically, this formulation suggests that diseased plants, or \\(y\\), and \\(r_E\\) at each time contribute to disease increase. The value of \\(\\frac{dy}{dt}\\) is minimal when \\(y = 0\\) and increases exponentially with the increase in \\(y\\).\nThe integral for the exponential model is given by\n\\(y = y_0 e^{r_Et}\\),\nwhere \\(y0\\) is and \\(r\\) are obtained via estimation. Let’s simulate two curves by varying \\(r\\) while fixing \\(y0\\) and varying the latter while fixing \\(r_E\\). We produce the two plots in ggplot and add the predicted curve using the `stat_function`. But first, we need to define values for the two model parameters. Further modifications to these values will be handled directly in the simulation (e.g. doubling infection rate, reducing initial inoculum by half, etc.).\n\nlibrary(tidyverse) # essential packages \nlibrary(cowplot)\nlibrary(r4pde)\ntheme_set(theme_r4pde()) # set global theme\n\n\ny0 &lt;- 0.001 \nr &lt;- 0.06 \ntmax &lt;- 60 # maximum duration t of the epidemics\ndat &lt;- data.frame(t = seq(1:tmax), y = seq(0:1)) # define the axes\n\nIn the plot below, note that the infection rate in one curve was doubled (\\(r\\) = 0.12)\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) y0 * exp(r * t), linetype = 1) +\n  stat_function(fun = function(t) y0 * exp(r * 2 * t), linetype = 2) +\n  ylim(0, 1) +\n  theme_r4pde()+\n  labs(x = \"Time\")\n\n\n\n\n\n\n\nFigure 10.1: Exponential curves with two rates of infection (0.06 and 0.12) and the same initial inoculum (0.001)\n\n\n\n\n\nNow the inoculum was increased five times while using the same doubled rate.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) y0 * exp(r * 2 * t), linetype = 1) +\n  stat_function(fun = function(t) y0 * 5 * exp(r * 2 * t), linetype = 2) +\n  ylim(0, 1) +\n  theme_r4pde()+\n  labs(x = \"Time\")\n\n\n\n\n\n\n\nFigure 10.2: Exponential curves with the same rate of infection (0.12) and single and five times the initial inoculum (0.001)\n\n\n\n\n\n\n\n10.1.2 Monomolecular\nThe differential of the monomolecular model is given by\n\\(\\frac{dy}{dt} = r_M (1-y)\\)\nwhere now the \\(r_M\\) is the rate parameter of the monomolecular model and \\((1-y)\\) is the proportion of non-infected (healthy) individuals or host tissue. Note that \\(\\frac{dy}{dt}\\) is maximum when \\(y = 0\\) and decreases when \\(y\\) approaches 1. Its decline is due to decrease in the proportion of individuals or healthy sites with the increase in \\(y\\). Any inoculum capable of infecting the host will more likely land on infected individuals or sites.\nThe integral of the monomolecular model is given by\n\\(\\frac{dy}{dt} = 1 - (1-y)e^{-r_Mt}\\)\nThis model commonly describes the temporal patterns of the monocyclic epidemics. In those, the inoculum produced during the course of the epidemics do not contribute new infections. Therefore, different from the exponential model, disease intensity \\(y\\) does not affect the epidemics and so the absolute rate is proportional to \\((1-y)\\).\nLet’s simulate two monomolecular curve with different rate parameters where one is one third of the other.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-r * t))) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-(r / 3) * t))) +\n  labs(x = \"Time\") +\n theme_r4pde()+\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"r = 0.06\") +\n  annotate(geom = \"text\", x = 50, y = 0.55, label = \"r = 0.02\")\n\n\n\n\n\n\n\nFigure 10.3: Monomolecular curves with two rates of infection (0.06 and 0.02) and the same initial inoculum (0.001)\n\n\n\n\n\nNow inoculum was increased 100 times with the reduced rate.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-r / 2 * t))) +\n  stat_function(fun = function(t) 1 - ((1 - (y0 * 100)) * exp(-r / 2 * t))) +\n  theme_r4pde()+\n  labs(x = \"Time\") +\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"y0 = 0.1\") +\n  annotate(geom = \"text\", x = 45, y = 0.65, label = \"y0 = 0.001\")\n\n\n\n\n\n\n\nFigure 10.4: Monomolecular curves with one rate (0.06) and the initial inoculum increased 100 times\n\n\n\n\n\n\n\n10.1.3 Logistic\nThe logistic model is a more elaborated version of the two previous models as it incorporates the features of them both. Its differential is given by\n\\(\\frac{dy}{dt} = r_L. y . (1 - y)\\),\nwhere \\(r_L\\) is the infection rate of the logistic model, \\(y\\) is the proportion of diseased individuals or host tissue and \\((1-y)\\) is the proportion of non-affected individuals or host area.\nBiologically, \\(y\\) in its differential equation implies that \\(\\frac{dy}{dt}\\) increases with the increase in \\(y\\) (as in the exponential) because more disease means more inoculum. However, \\((1-y)\\) leads to a decrease in \\(\\frac{dy}{dt}\\) when \\(y\\) approaches the maximum \\(y=1\\), because the proportion of healthy individuals or host area decreases (as in the monomolecular). Therefore, \\(\\frac{dy}{dt}\\) is minimal at the onset of the epidemics, reaches a maximum when \\(y/2\\) and declines until \\(y=1\\).\nThe integral is given by\n\\(y = \\frac{1}{1 + (1-y_0).e^{-r.t}}\\),\nwhere \\(r_L\\) is the apparent infection rate of the logistic model and \\(y0\\) is the disease intensity at \\(t=0\\). This model provides a good fit to polycyclic epidemics.\nLet’s check two curves where in one the infection rate is double while keeping the same initial inoculum.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 2 * t))\n  ) +\n  stat_function(fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 4 * t))) +\n  labs(x = \"Time\") +\ntheme_r4pde()+\n  annotate(geom = \"text\", x = 41, y = 0.77, label = \"r = 0.18\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"r = 0.024\")\n\n\n\n\n\n\n\nFigure 10.5: Logistic curves with two rates of infection (0.18 and 0.024) and the same initial inoculum (0.001)\n\n\n\n\n\nNow the inoculum is reduced 10 times for a same infection rate.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) 1 / (1 + ((1 - (y0 / 10)) / (y0 / 10)) * exp(-r * 3 * t))\n  ) +\n  stat_function(fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 3 * t))) +\n  labs(x = \"Time\") +\n theme_r4pde()+\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"y0 = 0.001\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"y0 = 0.0001\")\n\n\n\n\n\n\n\nFigure 10.6: Logistic curves with a single rate of infection (0.24) and two initial inoculum (0.001 and 0.0001)\n\n\n\n\n\n\n\n10.1.4 Gompertz\nThe Gompertz model is similar to the logistic and also provides a very good fit to several polycyclic diseases. The differential equation is given by\n\\(\\frac{dy}{dt} = r_G.[ln(1) - ln(y)]\\)\nDifferently from the logistic, the variable representing the non-infected individuals or host area is \\(-ln(y)\\). The integral equation is given by\n\\(y = e^{(ln(y0)).{e^{-r_G.t)}}}\\),\nwhere \\(r_G\\) is the apparent infection rate for the Gompertz models and \\(y_0\\) is the disease intensity at \\(t = 0\\).\nLet’s check curves for two rates.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) exp(log(y0) * exp(-r/2 * t))\n  ) +\n  stat_function(fun = function(t) exp(log(y0) * exp(-r*2 * t))) +\n  labs(x = \"Time\") +\n theme_r4pde()+\n  annotate(geom = \"text\", x = 41, y = 0.77, label = \"r = 0.12\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"r = 0.03\")\n\n\n\n\n\n\n\nFigure 10.7: Gompertz curves with two rates of infection (0.12 and 0.03) and the same initial inoculum (0.001)\n\n\n\n\n\nAnd those when inoculum was reduced one thousand times.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) exp(log(y0) * exp(-r*2 * t))\n  ) +\n  stat_function(fun = function(t) exp(log(y0/1000) * exp(-r*2 * t))) +\n  labs(x = \"Time\") +\n theme_r4pde()+\n  annotate(geom = \"text\", x = 15, y = 0.77, label = \"y0 = 0.001\") +\n  annotate(geom = \"text\", x = 25, y = 0.10, label = \"y0 = 0.00001\")\n\n\n\n\n\n\n\nFigure 10.8: Gompertz curves with a single rate of infection (0.12) and two levels of initial inoculum (0.001 and 0.00001)",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Population models</span>"
    ]
  },
  {
    "objectID": "temporal-models.html#interactive-application",
    "href": "temporal-models.html#interactive-application",
    "title": "10  Population models",
    "section": "10.2 Interactive application",
    "text": "10.2 Interactive application\nA shiny app was developed to demonstrate these four models interactively. Click on the image below to get access to the app.\n\n\n\n\n\n\nFigure 10.9: Screenshot of the application to visualize the population dynamics models by varying the model's parameters\n\n\n\n\n\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. Temporal analysis i: Quantifying and comparing epidemics. In The American Phytopathological Society, pp. 63–116. https://doi.org/10.1094/9780890545058.004.",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Population models</span>"
    ]
  },
  {
    "objectID": "temporal-fitting.html",
    "href": "temporal-fitting.html",
    "title": "11  Model fitting",
    "section": "",
    "text": "11.1 Linear regression: single epidemics\nIn model fitting for temporal analysis, the objective is to determine which previously-reviewed epidemiological (population dynamics) models best fit the data from actual epidemics. Doing so allows us to obtain two key parameters: the initial inoculum and the apparent infection rate.\nThere are essentially two methods for achieving this: linear regression and non-linear regression modeling. We’ll begin with linear regression, which is computationally simpler. I’ll illustrate the procedure using both built-in R functions and custom functions from the epifitter package (Alves and Del Ponte 2021). Epifitter offers a set of user-friendly functions that can fit and rank the best models for a given epidemic.\nTo exemplify, we’ll continue examining a previously shown curve that represents the incidence of the tobacco etch virus, a disease affecting peppers, over time. This dataset is featured in Chapter 3 of the book, “Study of Plant Disease Epidemics” (Madden et al. 2007). While the book presents SAS code for certain analyses, we offer an alternative code that accomplishes similar analyses, even if it doesn’t replicate the book’s results exactly.\nlibrary(tidyverse)\nlibrary(r4pde)\ntheme_set(theme_r4pde())\ndpc &lt;- \n  tribble(\n   ~t,  ~y, \n   0,  0.1, \n   7,  1, \n  14,  9, \n  21,  25, \n  28,  80, \n  35, 98, \n  42, 99, \n  49, 99.9\n  )\ndpc |&gt; \n  ggplot(aes(t, y))+\n  geom_point(size =3)+\n  geom_line()+\n  theme_r4pde()+\n  labs(x = \"Time\", y = \"Disease intensity (%)\")\n\n\n\n\n\n\n\nFigure 11.1: Disease progress curves for one tobacco etch epidemics in pepper. Reproduced from Madden et al. (2007) page 94\nTo start, we’ll need to transform the disease intensity (in proportion scale) data according to each of the models we aim to fit. In this instance, we’ll look at the four models discussed in the previous chapter: exponential, monomolecular, logistic, and Gompertz. We can use the mutate() function of dplyr package. The transformed y will be referred to as y* (or y2 in the code) followed by the letter E, M, L or G, for each model (exponential, monomolecular, etc) respectively.\ndpc1 &lt;- dpc |&gt; \n  mutate(y = y/100) |&gt; # transform to proportion\n  mutate(exponential = log(y),\n         monomolecular = log(1 / (1 - y)),\n         logistic = log(y / (1 - y)),\n         gompertz = -log(-log(y)))\nknitr::kable(round(dpc1, 4)) \n\n\n\n\nt\ny\nexponential\nmonomolecular\nlogistic\ngompertz\n\n\n\n\n0\n0.001\n-6.9078\n0.0010\n-6.9068\n-1.9326\n\n\n7\n0.010\n-4.6052\n0.0101\n-4.5951\n-1.5272\n\n\n14\n0.090\n-2.4079\n0.0943\n-2.3136\n-0.8788\n\n\n21\n0.250\n-1.3863\n0.2877\n-1.0986\n-0.3266\n\n\n28\n0.800\n-0.2231\n1.6094\n1.3863\n1.4999\n\n\n35\n0.980\n-0.0202\n3.9120\n3.8918\n3.9019\n\n\n42\n0.990\n-0.0101\n4.6052\n4.5951\n4.6001\n\n\n49\n0.999\n-0.0010\n6.9078\n6.9068\n6.9073\nNow we can plot the curves using the transformed values regressed against time. The curve that appears most linear, closely coinciding with the regression fit line, is a strong candidate for the best-fitting model. To accomplish this, we’ll first reshape the dataframe into long format, and then generate plots for each of the four models.\ndpc2 &lt;- dpc1 |&gt; \n  pivot_longer(3:6, names_to = \"model\", values_to = \"y2\") \n\n\ndpc2 |&gt; \n  ggplot(aes(t, y2))+\n  geom_point()+\n  geom_smooth(method = \"lm\", color = \"black\", se = F)+\n  facet_wrap(~ model)+\n  theme_r4pde()+\n  labs(x = \"Time\", y = \"Transformed value (y*)\",\n       color = \"Model\")+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 11.2: Curves of the transformed data for each epidemiological against time. The goal is to check which of the models provides the best fit based on the straight line\nFor this particular curve, it’s readily apparent that the logistic model offers the best fit to the data, as evidenced by the data points being closely aligned with the regression line, compared to the other models. However, to make a more nuanced decision between the logistic and Gompertz models—which are both typically used for sigmoid curves—we can rely on additional statistical measures.\nSpecifically, we can fit a regression model for each and examine key metrics such as the R-squared value and the residual standard error. To further validate the model’s accuracy, we can use Lin’s Concordance Correlation Coefficient to assess how closely the model’s predictions match the actual (transformed) data points.\nFor this exercise, let’s focus on the logistic and Gompertz models. We’ll start by fitting the logistic model and then move on to analyzing the summary of the regression model.\nlogistic &lt;- dpc2 |&gt; \n  filter(model == \"logistic\")\n\nm_logistic &lt;- lm(y2 ~ t, data = logistic)\n\n\n# R-squared\nsummary(m_logistic)$r.squared\n\n[1] 0.9923659\n\n# RSE \nsummary(m_logistic)$sigma\n\n[1] 0.4523616\n\n# calculate the Lin's CCC\nlibrary(epiR)\nccc_logistic &lt;- epi.ccc(logistic$y2, predict(m_logistic))\nccc_logistic$rho.c[1]\n\n        est\n1 0.9961683\nWe repeat the procedure for the Gompertz model.\ngompertz &lt;- dpc2 |&gt; \n  filter(model == \"gompertz\")\n\nm_gompertz &lt;- lm(y2 ~ t, data = gompertz)\n\n# R-squared\nsummary(m_gompertz)$r.squared\n\n[1] 0.9431066\n\n# RSE \nsummary(m_gompertz)$sigma\n\n[1] 0.8407922\n\n# calculate the Lin's CCC\nlibrary(epiR)\nccc_gompertz &lt;- epi.ccc(gompertz$y2, predict(m_gompertz))\nccc_gompertz$rho.c[1]\n\n        est\n1 0.9707204\nNext, let’s extract the two parameters of interest from each fitted model and incorporate them into the integral form of the respective models. To do this, we’ll need to back-transform the intercept, which represents the initial inoculum. This can be accomplished using specific equations, which we’ll outline next.\nrL &lt;- m_logistic$coefficients[2]\nrL\n\n        t \n0.2784814 \n\ny02 &lt;- m_logistic$coefficients[1]\ny0L = 1 / (1 + exp(-y02))\ny0L\n\n(Intercept) \n0.001372758 \n\nrG &lt;-m_gompertz$coefficients[2]\nrG\n\n        t \n0.1848378 \n\ny03 &lt;- m_gompertz$coefficients[1]\ny0G &lt;- exp(-exp(-y03))\ny0G\n\n (Intercept) \n1.968829e-09\nNow the plot:\nlogistic |&gt;\n  ggplot(aes(t, y)) +\n  geom_point(size = 2)+\n  stat_function(\n    linetype = 2,\n    fun = function(t) 1 / (1 + ((1 - y0L) / y0L) * exp(-rL * t)))+\nstat_function(\n    linetype = 1,\n    fun = function(t) exp(log(y0G) * exp(-rG * t))\n  )+\n  theme_r4pde()+\n  labs(x = \"Time\", y = \"Disease intensity\")\n\n\n\n\n\n\n\nFigure 11.3: Disease progress curve and the fit of the logistic (dashed line) and the Gompertz (solid line) based on parameters estimated using linear regression\nIn this case, it’s clear that the logistic model (the solid line above) emerges as the best fit based on our statistical evaluation. The approach for model selection outlined here is straightforward and manageable when dealing with a single epidemic and comparing only two models. However, real-world scenarios often require analyzing multiple curves and fitting various models to each, making manual comparison impractical for selecting a single best-fitting model. To streamline this task, it’s advisable to automate the process using custom functions designed to simplify the coding work involved.\nThat’s where the epifitter package comes into play! This package offers a range of custom functions designed to automate the model fitting and selection process, making it much more efficient to analyze multiple curves across different epidemics. By using epifitter, one can expedite the statistical evaluation needed to identify the best-fitting models.",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model fitting</span>"
    ]
  },
  {
    "objectID": "temporal-fitting.html#linear-regression-single-epidemics",
    "href": "temporal-fitting.html#linear-regression-single-epidemics",
    "title": "11  Model fitting",
    "section": "",
    "text": "Model\nTransformation\nBack-transformation\n\n\n\n\nExponential\nlog(y)\nexp(y*E)\n\n\nMonomolecular\nlog(1 / (1 - y))\n1 - exp(-y*M)\n\n\nLogistic\nlog(y / (1 - y))\n1 / (1 + exp(-y*L))\n\n\nGompertz\n-log(-log(y))\nexp(-exp(-y*G))",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model fitting</span>"
    ]
  },
  {
    "objectID": "temporal-fitting.html#non-linear-regression",
    "href": "temporal-fitting.html#non-linear-regression",
    "title": "11  Model fitting",
    "section": "11.2 Non linear regression",
    "text": "11.2 Non linear regression\nAlternatively, one can fit a nonlinear model to the data for each combination of curve and model using the nlsLM function in R of the minpack.lm package.\n\nlibrary(minpack.lm)\nfit_logistic &lt;- nlsLM(y/100 ~ 1 / (1+(1/y0-1)*exp(-r*t)), \n           start = list(y0 = 0.01, r = 0.3), \n           data = dpc)\n\nsummary(fit_logistic)\n\n\nFormula: y/100 ~ 1/(1 + (1/y0 - 1) * exp(-r * t))\n\nParameters:\n    Estimate Std. Error t value Pr(&gt;|t|)    \ny0 0.0003435  0.0002065   1.663    0.147    \nr  0.3321352  0.0249015  13.338  1.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02463 on 6 degrees of freedom\n\nNumber of iterations to convergence: 15 \nAchieved convergence tolerance: 1.49e-08\n\nfit_gompertz &lt;- nlsLM(y/100 ~ exp(log(y0/1)*exp(-r*t)), \n                    start = list(y0 = 0.01, r = 0.1), \n                    data = dpc)\nsummary(fit_gompertz)\n\n\nFormula: y/100 ~ exp(log(y0/1) * exp(-r * t))\n\nParameters:\n    Estimate Std. Error t value Pr(&gt;|t|)   \ny0 2.472e-14  5.493e-13   0.045   0.9656   \nr  1.621e-01  3.013e-02   5.380   0.0017 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06813 on 6 degrees of freedom\n\nNumber of iterations till stop: 50 \nAchieved convergence tolerance: 1.49e-08\nReason stopped: Number of iterations has reached `maxiter' == 50.\n\n\nWe can see that the model coefficients are not the same as those estimated using linear regression. Among other reasons, nls() often uses iterative techniques to estimate parameters, such as the Levenberg-Marquardt algorithm, which may provide different estimates than algebraic methods used in linear regression. While both methods aim to fit a model to data, they do so in ways that have distinct assumptions, strengths, and weaknesses, and this can result in different estimated parameters.\nBoth approaches—nonlinear least squares and linear regression on transformed data—have their own merits and limitations. The choice between the two often depends on various factors like the nature of the data, the underlying assumptions, and the specific requirements of the analysis. For an epidemiologist, the choice might come down to preference, familiarity with the techniques, or specific aims of the analysis.\nIn summary, both methods are valid tools in the toolkit of an epidemiologist or any researcher working on curve fitting and model selection. Understanding the nuances of each can help in making an informed choice tailored to the needs of a particular study.",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model fitting</span>"
    ]
  },
  {
    "objectID": "temporal-fitting.html#epifitter---multiple-epidemics",
    "href": "temporal-fitting.html#epifitter---multiple-epidemics",
    "title": "11  Model fitting",
    "section": "11.3 epifitter - multiple epidemics",
    "text": "11.3 epifitter - multiple epidemics\nWe will now examine three disease progress curves (DPCs) representing the incidence of the tobacco etch virus, a disease affecting peppers. Incidence evaluations were conducted at 7-day intervals up to 49 days. The relevant data can be found in Chapter 4, page 93, of the book “Study of Plant Disease Epidemics” (Madden et al. 2007). To get started, let’s input the data manually and create a data frame. The first column will represent the assessment time, while the remaining columns will correspond to the treatments, referred to as ‘groups’ in the book, ranging from 1 to 3.",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model fitting</span>"
    ]
  },
  {
    "objectID": "temporal-fitting.html#entering-data",
    "href": "temporal-fitting.html#entering-data",
    "title": "11  Model fitting",
    "section": "11.4 Entering data",
    "text": "11.4 Entering data\n\nlibrary(tidyverse) # essential packages \ntheme_set(theme_bw(base_size = 16)) # set global theme\n\n\npepper &lt;- \n  tribble(\n   ~t,  ~`1`,  ~`2`,  ~`3`,\n   0,  0.08, 0.001, 0.001,\n   7,  0.13,  0.01, 0.001,\n  14,  0.78,  0.09,  0.01,\n  21,  0.92,  0.25,  0.05,\n  28,  0.99,   0.8,  0.18,\n  35, 0.995,  0.98,  0.34,\n  42, 0.999,  0.99,  0.48,\n  49, 0.999, 0.999,  0.74\n  )",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model fitting</span>"
    ]
  },
  {
    "objectID": "temporal-fitting.html#visualize-the-dpcs",
    "href": "temporal-fitting.html#visualize-the-dpcs",
    "title": "11  Model fitting",
    "section": "11.5 Visualize the DPCs",
    "text": "11.5 Visualize the DPCs\nBefore proceeding with model selection and fitting, let’s visualize the three epidemics. The code below reproduces quite exactly the top plot of Fig. 4.15 (Madden et al. (2007) page 94). The appraisal of the curves might give us a hint on which models are the best candidates.\nBecause the data was entered in the wide format (each DPC is in a different column) we need to reshape it to the long format. The pivot_longer() function will do the job of reshaping from wide to long format so we can finally use the ggplot() function to produce the plot.\n\npepper |&gt; \n  pivot_longer(2:4, names_to =\"treat\", values_to = \"inc\") |&gt; \n  ggplot (aes(t, inc, \n              linetype = treat, \n              shape = treat, \n              group = treat))+\n  scale_color_grey()+\n  theme_grey()+\n  geom_line(linewidth = 1)+\n  geom_point(size =3, shape = 16)+\n  annotate(geom = \"text\", x = 15, y = 0.84, label = \"1\")+\n  annotate(geom = \"text\", x = 23, y = 0.6, label = \"2\")+\n  annotate(geom = \"text\", x = 32, y = 0.33, label = \"3\")+\n  labs(y = \"Disease incidence (y)\",\n       x = \"Time (days)\")+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 11.4: Disease progress curves for three tobacco etch epidemics in pepper. Reproduced from Madden et al. (2007) page 94\n\n\n\n\n\nMost of the three curves show a sigmoid shape with the exception of group 3 that resembles an exponential growth, not reaching the maximum value, and thus suggesting an incomplete epidemic. We can easily eliminate the monomolecular and exponential models and decide on the other two non-flexible models: logistic or Gompertz. To do that, let’s proceed to model fitting and evaluate the statistics for supporting a final decision. There are two modeling approaches for model fitting in epifitter: the linear or nonlinear parameter-estimation methods.\n\n11.5.1 epifitter: linear regression\nAmong the several options offered by epifitter we start with the simplest one, which is to fit a model to a single epidemics using the linear regression approach. For such, the fit_lin() requires two arguments: time (time) and disease intensity (y) each one as a vector stored or not in a dataframe.\nSince we have three epidemics, fit_lin() will be use three times. The function produces a list object with six elements. Let’s first look at the Stats dataframe of each of the three lists named epi1 to epi3.\n\nlibrary(epifitter)\nepi1 &lt;- fit_lin(time = pepper$t,  \n                y = pepper$`1` )\nknitr::kable(epi1$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nGompertz\n0.9848\n0.9700\n0.5911\n\n\nMonomolecular\n0.9838\n0.9681\n0.5432\n\n\nLogistic\n0.9782\n0.9572\n0.8236\n\n\nExponential\n0.7839\n0.6447\n0.6705\n\n\n\n\n\n\nepi2 &lt;- fit_lin(time = pepper$t,  \n  y = pepper$`2` )\nknitr::kable(epi2$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nLogistic\n0.9962\n0.9924\n0.4524\n\n\nGompertz\n0.9707\n0.9431\n0.8408\n\n\nMonomolecular\n0.9248\n0.8601\n1.0684\n\n\nExponential\n0.8971\n0.8134\n1.2016\n\n\n\n\n\n\nepi3 &lt;- fit_lin(time = pepper$t,  \n  y = pepper$`3` )\nknitr::kable(epi3$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nLogistic\n0.9829\n0.9665\n0.6045\n\n\nGompertz\n0.9825\n0.9656\n0.2263\n\n\nExponential\n0.9636\n0.9297\n0.7706\n\n\nMonomolecular\n0.8592\n0.7531\n0.2534\n\n\n\n\n\nThe statistics of the model fit confirms our initial guess that the predictions by the logistic or the Gompertz are closer to the observations than predictions by the other models. There is a slight difference between them based on these statistics. However, to pick one of the models, it is important to inspect the curves with the observed and predicted values to check which model is best for all curves. For such, we can use the plot_fit() function from epifitter to explore visually the fit of the four models to each curve.\n\nplot_fit(epi1)+\n  ylim(0,1)+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nknitr::kable(epi1$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbest_model\nmodel\nr\nr_se\nr_ci_lwr\nr_ci_upr\nv0\nv0_se\nr_squared\nRSE\nCCC\ny0\ny0_ci_lwr\ny0_ci_upr\n\n\n\n\n1\nGompertz\n0.1815713\n0.0130299\n0.1496882\n0.2134544\n-1.2050364\n0.3815570\n0.9700273\n0.5911056\n0.9847857\n0.0355477\n0.0002059\n0.2693349\n\n\n2\nMonomolecular\n0.1616413\n0.0119739\n0.1323423\n0.1909404\n-0.4625249\n0.3506326\n0.9681251\n0.5431977\n0.9838044\n-0.5880787\n-2.7452636\n0.3266178\n\n\n3\nLogistic\n0.2104047\n0.0181544\n0.1659824\n0.2548270\n-2.2715851\n0.5316185\n0.9572410\n0.8235798\n0.9781534\n0.0935038\n0.0273207\n0.2747287\n\n\n4\nExponential\n0.0487634\n0.0147802\n0.0125974\n0.0849293\n-1.8090602\n0.4328113\n0.6446531\n0.6705085\n0.7839381\n0.1638080\n0.0568061\n0.4723623\n\n\n\n\nplot_fit(epi2)+\n  ylim(0,1)+\n  scale_color_grey()+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nknitr::kable(epi2$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbest_model\nmodel\nr\nr_se\nr_ci_lwr\nr_ci_upr\nv0\nv0_se\nr_squared\nRSE\nCCC\ny0\ny0_ci_lwr\ny0_ci_upr\n\n\n\n\n1\nLogistic\n0.2784814\n0.0099716\n0.2540818\n0.3028809\n-6.589560\n0.2919981\n0.9923659\n0.4523616\n0.9961683\n0.0013728\n0.0006724\n0.0028007\n\n\n2\nGompertz\n0.1848378\n0.0185339\n0.1394871\n0.2301886\n-2.998021\n0.5427290\n0.9431066\n0.8407922\n0.9707204\n0.0000000\n0.0000000\n0.0049309\n\n\n3\nMonomolecular\n0.1430234\n0.0235503\n0.0853979\n0.2006489\n-1.325645\n0.6896255\n0.8600832\n1.0683633\n0.9247793\n-2.7646136\n-19.3503499\n0.3035837\n\n\n4\nExponential\n0.1354579\n0.0264869\n0.0706469\n0.2002689\n-5.263915\n0.7756171\n0.8134015\n1.2015809\n0.8971003\n0.0051750\n0.0007757\n0.0345258\n\n\n\n\nplot_fit(epi3)+\n  ylim(0,1)+\n  scale_color_grey()+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nknitr::kable(epi3$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbest_model\nmodel\nr\nr_se\nr_ci_lwr\nr_ci_upr\nv0\nv0_se\nr_squared\nRSE\nCCC\ny0\ny0_ci_lwr\ny0_ci_upr\n\n\n\n\n1\nLogistic\n0.1752146\n0.0133257\n0.1426077\n0.2078215\n-7.1136060\n0.3902187\n0.9664590\n0.6045243\n0.9829434\n0.0008133\n0.0003132\n0.0021104\n\n\n2\nGompertz\n0.0647145\n0.0049874\n0.0525107\n0.0769182\n-2.2849079\n0.1460470\n0.9655894\n0.2262550\n0.9824935\n0.0000541\n0.0000008\n0.0010358\n\n\n3\nExponential\n0.1513189\n0.0169860\n0.1097556\n0.1928822\n-6.8629493\n0.4974031\n0.9297097\n0.7705736\n0.9635747\n0.0010458\n0.0003097\n0.0035322\n\n\n4\nMonomolecular\n0.0238957\n0.0055853\n0.0102291\n0.0375624\n-0.2506567\n0.1635537\n0.7531307\n0.2533763\n0.8591837\n-0.2848689\n-0.9171854\n0.1389001\n\n\n\n\n\n\n\n11.5.2 epifitter: non linear regression\n\nepi11 &lt;- fit_nlin(time = pepper$t,  \n                y = pepper$`1` )\nknitr::kable(epi11$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nGompertz\n0.9963\n0.9956\n0.0381\n\n\nLogistic\n0.9958\n0.9939\n0.0403\n\n\nMonomolecular\n0.9337\n0.8883\n0.1478\n\n\nExponential\n0.7161\n0.5903\n0.2770\n\n\n\n\nepi22 &lt;- fit_nlin(time = pepper$t,  \n                y = pepper$`2` )\nknitr::kable(epi22$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nLogistic\n0.9988\n0.9981\n0.0246\n\n\nGompertz\n0.9904\n0.9857\n0.0683\n\n\nMonomolecular\n0.8697\n0.8020\n0.2329\n\n\nExponential\n0.8587\n0.7862\n0.2413\n\n\n\n\nepi33 &lt;- fit_nlin(time = pepper$t,  \n                y = pepper$`3` )\nknitr::kable(epi33$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nLogistic\n0.9957\n0.9922\n0.0270\n\n\nGompertz\n0.9946\n0.9894\n0.0306\n\n\nExponential\n0.9880\n0.9813\n0.0445\n\n\nMonomolecular\n0.8607\n0.7699\n0.1426\n\n\n\n\n\nAnd now we can produce the plot of the fitted curves together with the original incidence dat. The stats_all dataframe shows everything we need regarding the statistics and the values of the parameteres.\n\nplot_fit(epi11)+\n  scale_color_grey()+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n\nknitr::kable(epi11$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ny0\ny0_se\nr\nr_se\ndf\nCCC\nr_squared\nRSE\ny0_ci_lwr\ny0_ci_upr\nr_ci_lwr\nr_ci_upr\nbest_model\n\n\n\n\nGompertz\n0.0000004\n0.0000017\n0.2865037\n0.0313160\n6\n0.9962708\n0.9956357\n0.0380871\n-0.0000039\n0.0000046\n0.2098762\n0.3631312\n1\n\n\nLogistic\n0.0092495\n0.0060330\n0.4181351\n0.0539931\n6\n0.9957910\n0.9938810\n0.0403179\n-0.0055126\n0.0240117\n0.2860189\n0.5502514\n2\n\n\nMonomolecular\n-0.0264717\n0.1405690\n0.0836206\n0.0212473\n6\n0.9337210\n0.8883223\n0.1477596\n-0.3704318\n0.3174883\n0.0316303\n0.1356109\n3\n\n\nExponential\n0.4159606\n0.1376766\n0.0215063\n0.0088658\n6\n0.7160769\n0.5903409\n0.2770420\n0.0790782\n0.7528430\n-0.0001874\n0.0432000\n4\n\n\n\n\nplot_fit(epi22)+\n  scale_color_grey()+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n\nknitr::kable(epi22$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ny0\ny0_se\nr\nr_se\ndf\nCCC\nr_squared\nRSE\ny0_ci_lwr\ny0_ci_upr\nr_ci_lwr\nr_ci_upr\nbest_model\n\n\n\n\nLogistic\n0.0003435\n0.0002065\n0.3321341\n0.0249014\n6\n0.9988253\n0.9980693\n0.0246349\n-0.0001618\n0.0008488\n0.2712025\n0.3930657\n1\n\n\nGompertz\n0.0000000\n0.0000000\n0.1618740\n0.0301525\n6\n0.9904450\n0.9856896\n0.0682506\n0.0000000\n0.0000000\n0.0880936\n0.2356545\n2\n\n\nMonomolecular\n-0.1971530\n0.2005248\n0.0442060\n0.0131684\n6\n0.8696734\n0.8020353\n0.2328814\n-0.6878195\n0.2935134\n0.0119840\n0.0764280\n3\n\n\nExponential\n0.1612234\n0.0848398\n0.0410936\n0.0125521\n6\n0.8587176\n0.7862042\n0.2412526\n-0.0463722\n0.3688189\n0.0103797\n0.0718076\n4\n\n\n\n\nplot_fit(epi33)+\n  scale_color_grey()+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n\nknitr::kable(epi33$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ny0\ny0_se\nr\nr_se\ndf\nCCC\nr_squared\nRSE\ny0_ci_lwr\ny0_ci_upr\nr_ci_lwr\nr_ci_upr\nbest_model\n\n\n\n\nLogistic\n0.0056634\n0.0019662\n0.1249931\n0.0086711\n6\n0.9957129\n0.9922359\n0.0270311\n0.0008522\n0.0104746\n0.1037758\n0.1462105\n1\n\n\nGompertz\n0.0000002\n0.0000008\n0.0759837\n0.0061025\n6\n0.9945740\n0.9894187\n0.0305883\n-0.0000017\n0.0000022\n0.0610515\n0.0909159\n2\n\n\nExponential\n0.0225267\n0.0072184\n0.0718824\n0.0070479\n6\n0.9880169\n0.9812854\n0.0445057\n0.0048638\n0.0401896\n0.0546367\n0.0891281\n3\n\n\nMonomolecular\n-0.1371485\n0.1060292\n0.0169603\n0.0042530\n6\n0.8606954\n0.7698769\n0.1426044\n-0.3965925\n0.1222955\n0.0065536\n0.0273669\n4\n\n\n\n\n\nFor multiple epidemics, we can use another handy function that allows us to simultaneously fit the models to multiple DPC data. Different from fit_lin(), fit_multi() requires the data to be structured in the long format where there is a column specifying each of the epidemics.\nLet’s then create a new data set called pepper2 using the data transposing functions of the tidyr package.\n\npepper2 &lt;- pepper |&gt; \n  pivot_longer(2:4, names_to =\"treat\", values_to = \"inc\")\n\nNow we fit the models to all DPCs. Note that the name of the variable indicating the DPC code needs to be informed in strata_cols argument. To use the nonlinear regression approach we set nlin argument to TRUE.\n\nepi_all &lt;- fit_multi(\n  time_col = \"t\",\n  intensity_col = \"inc\",\n  data = pepper2,\n  strata_cols = \"treat\",\n  nlin = FALSE\n)\n\nNow let’s select the statistics of model fitting. Again, Epifitter ranks the models based on the CCC (the higher the better) but it is important to check the RSE as well - the lower the better. In fact, the RSE is more important when the goal is prediction.\n\nepi_all$Parameters |&gt; \n  select(treat, model, best_model, RSE, CCC)\n\n   treat         model best_model       RSE       CCC\n1      1      Gompertz          1 0.5911056 0.9847857\n2      1 Monomolecular          2 0.5431977 0.9838044\n3      1      Logistic          3 0.8235798 0.9781534\n4      1   Exponential          4 0.6705085 0.7839381\n5      2      Logistic          1 0.4523616 0.9961683\n6      2      Gompertz          2 0.8407922 0.9707204\n7      2 Monomolecular          3 1.0683633 0.9247793\n8      2   Exponential          4 1.2015809 0.8971003\n9      3      Logistic          1 0.6045243 0.9829434\n10     3      Gompertz          2 0.2262550 0.9824935\n11     3   Exponential          3 0.7705736 0.9635747\n12     3 Monomolecular          4 0.2533763 0.8591837\n\n\nThe code below calculates the frequency that each model was the best. This would facilitate in the case of many epidemics to analyse.\n\nfreq_best &lt;- epi_all$Parameters %&gt;% \n    filter(best_model == 1) %&gt;% \n    group_by(treat, model) %&gt;% \n    summarise(first = n()) %&gt;%\n  ungroup() |&gt; \n  count(model) \nfreq_best \n\n# A tibble: 2 × 2\n  model        n\n  &lt;chr&gt;    &lt;int&gt;\n1 Gompertz     1\n2 Logistic     2\n\n\nWe can see that the Logistic model was the best model in two out of three epidemics.\nTo be more certain about our decision, let’s advance to the final step which is to produce the plots with the observed and predicted values for each assessment time by calling the Data dataframe of the `epi_all list.\n\nepi_all$Data |&gt;\n filter(model %in% c(\"Gompertz\", \"Logistic\")) |&gt; \n  ggplot(aes(time, predicted, shape = treat)) +\n  geom_point(aes(time, y)) +\n  geom_line() +\n  facet_wrap(~ model) +\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"bottom\")+\n coord_cartesian(ylim = c(0, 1)) + # set the max to 0.6\n  labs(\n    shape = \"Epidemic\",\n    y = \"Disease incidence\",\n    x = \"Time (days after emergence)\"\n  )\n\n\n\n\n\n\n\nFigure 11.5: Observed (dots) and fitted (line) values for three tobacco etch epidemics in pepper\n\n\n\n\n\nOverall, the logistic model seems a better fit for all the curves. Let’s produce a plot with the prediction error versus time.\n\nepi_all$Data |&gt;\n filter(model %in% c(\"Gompertz\", \"Logistic\")) |&gt; \n  ggplot(aes(time, predicted -y, shape = treat)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype =2)+\n  facet_wrap(~ model) +\n coord_cartesian(ylim = c(-0.4, 0.4)) + # set the max to 0.6\n  labs(\n    y = \"Prediction error\",\n    x = \"Time (days after emergence)\",\n    shape = \"Epidemic\"\n  )\n\n\n\n\n\n\n\nFigure 11.6: Prediction error (dotted lines) by two models fitted to the progress curves of three tobacco etch epidemics in pepper\n\n\n\n\n\nThe plots above confirms the logistic model as good fit overall because the errors for all epidemics combined are more scattered around the non-error line.\nWe can then now extract the parameters of interest of the chosen model. These data are stored in the Parameters data frame of the epi_all list. Let’s filter the Logistic model and apply a selection of the parameters of interest.\n\n  epi_all$Parameters |&gt;\n    filter(model == \"Logistic\") |&gt;\n    select(treat, y0, y0_ci_lwr, y0_ci_upr, r, r_ci_lwr, r_ci_upr \n)\n\n  treat           y0    y0_ci_lwr   y0_ci_upr         r  r_ci_lwr  r_ci_upr\n1     1 0.0935037690 0.0273207272 0.274728744 0.2104047 0.1659824 0.2548270\n2     2 0.0013727579 0.0006723537 0.002800742 0.2784814 0.2540818 0.3028809\n3     3 0.0008132926 0.0003131745 0.002110379 0.1752146 0.1426077 0.2078215\n\n\nWe can produce a plot for visual inference on the differences in the parameters.\n\np1 &lt;- epi_all$Parameters |&gt;\n  filter(model == \"Logistic\") |&gt;\n  ggplot(aes(treat, r)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Epidemic\",\n    y = \"r\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np2 &lt;- epi_all$Parameters |&gt;\n  filter(model == \"Logistic\") |&gt;\n  ggplot(aes(treat, 1 - exp(-y0))) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Epidemic\",\n    y = \"y0\"\n  )\n\nlibrary(patchwork)\np1 | p2\n\n\n\n\n\n\n\nFigure 11.7: Estimated infection rates (left) and initial inoculum (right) by a logistic model fitted to the progress curves of three epidemics of tobacco etch on pepper\n\n\n\n\n\nWe can compare the rate parameter (slopes) from two separate linear regression models using a t-test. This is sometimes referred to as a “test of parallelism” in the context of comparing slopes. The t-statistic for comparing two slopes with their respective standard errors can be calculated as:\n\\(t = \\frac{\\beta_1 - \\beta_2}{\\sqrt{SE_{\\beta_1}^2 + SE_{\\beta_2}^2}}\\)\nThis t-statistic follows a t-distribution with ( df = n_1 + n_2 - 4 ) degrees of freedom, where ( n_1 ) and ( n_2 ) are the sample sizes of the two groups. In our case, ( n_1 = n_2 = 8 ), so ( df = 8 + 8 - 4 = 12 ).\nHere’s how to perform the t-test for comparing curve 1 and 2.\n\n# Given slopes and standard errors from curve 1 and 2\nbeta1 &lt;- 0.2104 \nbeta2 &lt;- 0.2784 \nSE_beta1 &lt;- 0.01815 \nSE_beta2 &lt;- 0.00997\n\n# Sample sizes for both treatments (n1 and n2)\nn1 &lt;- 8\nn2 &lt;- 8\n\n# Calculate the t-statistic\nt_statistic &lt;- abs(beta1 - beta2) / sqrt(SE_beta1^2 + SE_beta2^2)\n\n# Degrees of freedom\ndf &lt;- n1 + n2 - 4\n\n# Calculate the p-value\np_value &lt;- 2 * (1 - pt(abs(t_statistic), df))\n\n# Print the results\nprint(paste(\"t-statistic:\", round(t_statistic, 4)))\n\n[1] \"t-statistic: 3.2837\"\n\nprint(paste(\"Degrees of freedom:\", df))\n\n[1] \"Degrees of freedom: 12\"\n\nprint(paste(\"p-value:\", round(p_value, 4)))\n\n[1] \"p-value: 0.0065\"\n\n\nThe pt() function in R gives the cumulative distribution function of the t-distribution. The 2 * (1 - pt(abs(t_statistic), df)) line calculates the two-tailed p-value. This will tell us if the slopes are significantly different at your chosen alpha level (commonly 0.05).",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model fitting</span>"
    ]
  },
  {
    "objectID": "temporal-fitting.html#designed-experiments",
    "href": "temporal-fitting.html#designed-experiments",
    "title": "11  Model fitting",
    "section": "11.6 Designed experiments",
    "text": "11.6 Designed experiments\nIn the following section, we’ll focus on disease data collected over time from the same plot unit, also known as repeated measures. This data comes from a designed experiment aimed at evaluating and comparing the effects of different treatments.\nSpecifically, we’ll use a dataset of progress curves found on page 98 of “Study of Plant Disease Epidemics” (Madden et al. 2007). These curves depict the incidence of soybean plants showing symptoms of bud blight, which is caused by the tobacco streak virus. Four different treatments, corresponding to different planting dates, were evaluated using a randomized complete block design with four replicates. Each curve has four time-based assessments.\nThe data for this study is stored in a CSV file, which we’ll load into our environment using the read_csv() function. Once loaded, we’ll store the data in a dataframe named budblight.\n\n11.6.1 Loading data\n\nbudblight &lt;- read_csv(\"https://raw.githubusercontent.com/emdelponte/epidemiology-R/main/data/bud-blight-soybean.csv\")\n\nLet’s have a look at the first six rows of the dataset and check the data type for each column. There is an additional column representing the replicates, called block.\n\nbudblight\n\n# A tibble: 64 × 4\n   treat  time block     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 PD1      30     1  0.1 \n 2 PD1      30     2  0.3 \n 3 PD1      30     3  0.1 \n 4 PD1      30     4  0.1 \n 5 PD1      40     1  0.3 \n 6 PD1      40     2  0.38\n 7 PD1      40     3  0.36\n 8 PD1      40     4  0.37\n 9 PD1      50     1  0.57\n10 PD1      50     2  0.52\n# ℹ 54 more rows\n\n\n\n\n11.6.2 Visualizing the DPCs\nLet’s have a look at the curves and produce a combo plot figure similar to Fig. 4.17 of the book, but without the line of the predicted values.\n\np3 &lt;- budblight |&gt;\n  ggplot(aes(\n    time, y,\n    group = block,\n    shape = factor(block)\n  )) +\n  geom_point(size = 1.5) +\n  ylim(0, 0.6) +\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")+\n  facet_wrap(~treat, ncol =1)+\n  labs(y = \"Disease incidence\",\n       x = \"Time (days after emergence)\")\n\np4 &lt;- budblight |&gt;\n  ggplot(aes(\n    time, log(1 / (1 - y)),\n    group = block,\n    shape = factor(block)\n  )) +\n  geom_point(size = 2) +\n  facet_wrap(~treat, ncol = 1) +\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")+\n  labs(y = \"Transformed incidence\", x = \"Time (days after emergence)\")\nlibrary(patchwork)\np3 | p4\n\n\n\n\n\n\n\nFigure 11.8: Disease progress curves for the incidence of budblight of soybean in Brazil for four planting dates\n\n\n\n\n\n\n\n11.6.3 Model fitting\nRemember that the first step in model selection is the visual appraisal of the curve data linearized with the model transformation. In the case the curves represent complete epidemics (close to 100%) appraisal of the absolute rate (difference in y between two times) over time is also helpful.\nFor the treatments above, it looks like the curves are typical of a monocyclic disease (the case of soybean bud blight), for which the monomolecular is usually a good fit, but other models are also possible as well. For this exercise, we will use both the linear and the nonlinear estimation method.\n\n11.6.3.1 Linear regression\nFor convenience, we use the fit_multi() to handle multiple epidemics. The function returns a list object where a series of statistics are provided to aid in model selection and parameter estimation. We need to provide the names of columns (arguments): assessment time (time_col), disease incidence (intensity_col), and treatment (strata_cols).\n\nlin1 &lt;- fit_multi(\n  time_col = \"time\",\n  intensity_col = \"y\",\n  data = budblight,\n  strata_cols = \"treat\",\n  nlin = FALSE\n)\n\nLet’s look at how well the four models fitted the data. Epifitter suggests the best fitted model (1 to 4, where 1 is best) for each treatment. Let’s have a look at the statistics of model fitting.\n\nlin1$Parameters |&gt; \nselect(treat, best_model, model, CCC, RSE)\n\n   treat best_model         model       CCC        RSE\n1    PD1          1 Monomolecular 0.9348429 0.09805661\n2    PD1          2      Gompertz 0.9040182 0.22226189\n3    PD1          3      Logistic 0.8711178 0.44751963\n4    PD1          4   Exponential 0.8278055 0.36124036\n5    PD2          1 Monomolecular 0.9547434 0.07003116\n6    PD2          2      Gompertz 0.9307192 0.17938711\n7    PD2          3      Logistic 0.9062012 0.38773023\n8    PD2          4   Exponential 0.8796705 0.32676216\n9    PD3          1 Monomolecular 0.9393356 0.06832499\n10   PD3          2      Gompertz 0.9288436 0.17156394\n11   PD3          3      Logistic 0.9085414 0.39051075\n12   PD3          4   Exponential 0.8896173 0.33884790\n13   PD4          1      Gompertz 0.9234736 0.17474422\n14   PD4          2 Monomolecular 0.8945962 0.06486949\n15   PD4          3      Logistic 0.8911344 0.52412586\n16   PD4          4   Exponential 0.8739618 0.49769642\n\n\nAnd now we extract values for each parameter estimated from the fit of the monomolecular model.\n\nlin1$Parameters |&gt;\nfilter(model == \"Monomolecular\") |&gt;\nselect(treat, y0, r)\n\n  treat         y0          r\n1   PD1 -0.5727700 0.02197351\n2   PD2 -0.5220593 0.01902952\n3   PD3 -0.4491365 0.01590586\n4   PD4 -0.3619898 0.01118047\n\n\nNow we visualize the fit of the monomolecular model (using filter function - see below) to the data together with the observed data and then reproduce the right plots in Fig. 4.17 from the book.\n\nlin1$Data |&gt;\n  filter(model == \"Monomolecular\") |&gt;\n  ggplot(aes(time, predicted)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(aes(time, y)) +\n  geom_line(linewidth = 0.5) +\n  facet_wrap(~treat) +\n  coord_cartesian(ylim = c(0, 0.6)) + # set the max to 0.6\n  labs(\n    y = \"Disease incidence\",\n    x = \"Time (days after emergence)\"\n  )\n\n\n\n\n\n\n\nFigure 11.9: Observed (dot) and fitted values by a monomolecular model (line) to the data on the incidence of budblight of soybean in Brazil for four planting dates\n\n\n\n\n\nNow we can plot the means and respective 95% confidence interval of the apparent infection rate (\\(r\\)) and initial inoculum (\\(y_0\\)) for visual inference.\n\np5 &lt;- lin1$Parameters |&gt;\n  filter(model == \"Monomolecular\") |&gt;\n  ggplot(aes(treat, r)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Epidemic\",\n    y = \"Infection rate (r)\"\n  )\n\np6 &lt;- lin1$Parameters |&gt;\n  filter(model == \"Monomolecular\") |&gt;\n  ggplot(aes(treat, y0)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"Initial inoculum (y0)\"\n  )\np5 | p6\n\n\n\n\n\n\n\nFigure 11.10: Estimates of the infection rate (left) and initial inoculum (right) from the fit of a monomolecular model to the data on the incidence of budblight of soybean in Brazil for four planting dates\n\n\n\n\n\n\n\n11.6.3.2 Non-linear regression\nTo estimate the parameters using the non-linear approach, we repeat the same arguments in the fit_multi function, but include an additional argument nlin set to TRUE.\n\nnlin1 &lt;- fit_multi(\n  time_col = \"time\",\n  intensity_col = \"y\",\n  data = budblight,\n  strata_cols = \"treat\",\n  nlin = TRUE\n)\n\nLet’s check statistics of model fit.\n\nnlin1$Parameters |&gt;\nselect(treat, model, CCC, RSE, best_model)\n\n   treat         model       CCC        RSE best_model\n1    PD1 Monomolecular 0.9382991 0.06133704          1\n2    PD1      Gompertz 0.9172407 0.06986307          2\n3    PD1      Logistic 0.8957351 0.07700720          3\n4    PD1   Exponential 0.8544194 0.08799512          4\n5    PD2 Monomolecular 0.9667886 0.04209339          1\n6    PD2      Gompertz 0.9348370 0.05726761          2\n7    PD2      Logistic 0.9077857 0.06657793          3\n8    PD2   Exponential 0.8702365 0.07667322          4\n9    PD3 Monomolecular 0.9570853 0.04269129          1\n10   PD3      Gompertz 0.9261609 0.05443852          2\n11   PD3      Logistic 0.8997106 0.06203037          3\n12   PD3   Exponential 0.8703443 0.06891021          4\n13   PD4 Monomolecular 0.9178226 0.04595409          1\n14   PD4      Gompertz 0.9085579 0.04791331          2\n15   PD4      Logistic 0.8940731 0.05083336          3\n16   PD4   Exponential 0.8842437 0.05267415          4\n\n\nAnd now we obtain the two parameters of interest. Note that the values are not the sames as those estimated using linear regression, but they are similar and highly correlated.\n\nnlin1$Parameters |&gt;\nfilter(model == \"Monomolecular\") |&gt;\nselect(treat, y0, r)\n\n  treat         y0          r\n1   PD1 -0.7072562 0.02381573\n2   PD2 -0.6335713 0.02064629\n3   PD3 -0.5048763 0.01674209\n4   PD4 -0.3501234 0.01094368\n\n\n\np7 &lt;- nlin1$Parameters |&gt;\n  filter(model == \"Monomolecular\") |&gt;\n  ggplot(aes(treat, r)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Epidemic\",\n    y = \"Infection rate (r)\"\n  )\n\np8 &lt;- nlin1$Parameters |&gt;\n  filter(model == \"Monomolecular\") |&gt;\n  ggplot(aes(treat, y0)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Epidemic\",\n    y = \"Initial inoculum (y0)\"\n  )\n\np7 | p8\n\n\n\n\n\n\n\nFigure 11.11: Estimates of the infection rate (left) and initial inoculum (right) from the fit of a monomolecular model to the data on the incidence of budblight of soybean in Brazil for four planting dates\n\n\n\n\n\n\n\n\n\nAlves, K. S., and Del Ponte, E. M. 2021. Analysis and simulation of plant disease progress curves in R: introducing the epifitter package. Phytopathology Research 3. https://doi.org/10.1186/s42483-021-00098-7.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. Temporal analysis i: Quantifying and comparing epidemics. In The American Phytopathological Society, pp. 63–116. https://doi.org/10.1094/9780890545058.004.",
    "crumbs": [
      "Temporal analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Model fitting</span>"
    ]
  },
  {
    "objectID": "spatial-gradients.html",
    "href": "spatial-gradients.html",
    "title": "12  Spatial gradients",
    "section": "",
    "text": "12.1 Introduction\nThe assessment of disease in terms of its spatial distribution, particularly considering changes in its intensity as it spreads over distance, is defined as the “disease gradient.” It’s the dispersal, or migration, of the pathogen through various means—such as wind, vectors, rain, movement of infected material, or even human mediation—that encourages the spread of plant diseases within a field or across continents, thereby creating these disease gradients.\nThere exist two distinct types of gradients: the inoculum gradient, in which the availability of a host is not necessarily a prerequisite, and the disease gradient, where all three elements of the disease triangle are essential.\nIn the ensuing chapters, we shall explore examples of actual disease gradients measured in the field, each exhibiting its own unique pattern.\nOur first example, from Mundt’s 1999 study (Mundt et al. 1999), sought to measure the dispersal potential of the pathogenic bacteria, Xanthomonas oryzae pv. oryzae, which is responsible for leaf blight in rice. This study was conducted using experimental plots in the Philippines during the wet seasons of 1994 and 1995.\nThe data were made available in this tutorial. We enter the data manually and then produce two plots, one for each year.\nlibrary(tidyverse)\nlibrary(r4pde)\ntheme_set(theme_r4pde())\n\nxo &lt;- \ntibble::tribble(\n    ~d,   ~y4,   ~y5,\n     0, 3.083, 7.185,\n  0.22, 0.521,  0.38,\n  0.44, 0.083, 0.157,\n  0.66, 0.021, 0.028\n  )\n\ng1 &lt;- xo |&gt; \n  ggplot(aes(d, y4))+\n theme_r4pde()+\n  geom_point(size = 2)+\n  geom_line()+\n  ylim(0,8)+\n  labs(y = \"Number of new lesions\",\n       x = \"Distance (m)\",\n       title = \"1994 wet season\")\n\ng2 &lt;- xo |&gt; \n  ggplot(aes(d, y5))+\n  theme_r4pde()+\n  geom_point(size = 2)+\n  geom_line()+\n  ylim(0,8)+\n  labs(y = \"Number of new lesions\",\n       x = \"Distance (m)\",\n       title = \"1995 wet season\")\n\nlibrary(patchwork)\n(g1 | g2) +  plot_annotation(\n    caption = \"Source: Mundt et al. (1999)\")\n\n\n\n\n\n\n\nFigure 12.1: Primary gradients of bacerial blight of rice in two wet seasons in the Philippines\nThe second example of a disease gradient pertains to stripe rust, caused by Puccinia striiformis f. sp. tritici, on wheat. This data was collected during a field experiment conducted at Hermiston in 2002, as reported in Sackett’s 2005 study (Sackett and Mundt 2005). Later, the data was made publicly available in 2015, courtesy of Mikaberidze (Mikaberidze et al. 2015). For our discussion, we’ll manually input the data in a tibble format.\nThis tibble contains five columns. The first and second columns represent distances from the source of infection, denoted in feet and meters, respectively. The remaining three columns consist of measures of stripe rust severity, each from a separate replicated plot. These measurements offer us a quantifiable view of the disease gradient of stripe rust in the field, thereby shedding light on the infection’s spatial distribution and intensity.\nhermiston &lt;- \n  tibble::tribble(\n  ~dist_f, ~dist_m,  ~`1`,  ~`2`,  ~`3`,\n  0,        0,    65,    65,    39,\n  5,      1.5,    35,    44,   7.5,\n  10,       3,  21.5,  14.5,  1.75,\n  20,     6.1,     8,  0.75,   0.2,\n  40,    12.2,     1,  0.08, 0.025,\n  60,    18.3,  0.25, 0.026, 0.015,\n  80,    24.4, 0.035, 0.015, 0.009,\n  100,   30.5,  0.01, 0.003, 0.008,\n  120,   36.6, 0.008, 0.016,  0.01,\n  140,   42.7, 0.003, 0.003,  0.01,\n  160,   48.8, 0.001, 0.006, 0.006,\n  180,   54.9, 0.001, 0.002, 0.002,\n  200,     61, 0.001, 0.003, 0.004,\n  220,   67.1, 0.001, 0.003, 0.002,\n  240,   73.2, 0.001, 0.001,     0,\n  260,   79.2, 0.001, 0.002,     0,\n  280,   85.3, 0.001, 0.001,     0,\n  300,   91.4, 0.001, 0.001, 0.001\n  )\nlibrary(tidyverse)\nlibrary(ggthemes)\n\nhermiston |&gt; \n  pivot_longer(3:5, names_to = \"replicate\", values_to = \"severity\") |&gt; \n  ggplot(aes(dist_m, severity, color = replicate))+\n  theme_r4pde()+\n  theme(legend.position = \"bottom\")+\n  geom_point(size = 2)+\n  geom_line(size = 1)+\n  scale_color_grey()+\n  labs(x = \"Distance from the source (m)\",\n       y = \"Stripe rust severity (%)\",\n       color = \"Replicate\",\n       caption = \"source: Sackett et al. (2005)\")\n\n\n\n\n\n\n\nFigure 12.2: Primary gradients of stripe rust of wheat on a replicated experiment\nAs evidenced by the examples presented above, disease gradients, assuming a single source of inoculum, typically display a pattern wherein the disease’s intensity diminishes more steeply within shorter proximities to the source. Conversely, the decrease is less steep at greater distances, eventually reaching a point of either zero or a low background level with only occasional diseased plants.\nThe unique shapes of these gradients are largely influenced by mechanisms associated with the dispersal of the inoculum, which are contingent not only on the pathogen’s biological characteristics but also heavily upon environmental factors that can impact the pathogen’s dispersion.\nFrom this, we can categorize the resulting gradients into two types: primary and secondary. The primary gradient is generated solely from the initial source of the infection. On the other hand, the secondary gradient arises from the movement of inoculum that has been produced by plants previously infected due to the primary gradient. These secondary infections then spread to other plants situated at increasing distances from the initial source.\nAs the disease proliferates over time, it’s expected that a combination of both primary and secondary gradients will manifest. This interplay between the two gradient types contributes to the overall spread and severity of the disease within a given population and environment.\nAs an example of primary and secondary gradients, let’s visualize the gradients of Septoria leaf spot, caused by Septoria lycopersici, on tomato (Parker et al. 1997). The gradients were measured during two times, thus enabling a comparison of primary and secondary dispersal/disease gradients. More details of the study and experimental approach were provided in this tutorial. The data is entered below as a tribble and the plot produced using ggplot2.\nseptoria &lt;- \ntibble::tribble(\n ~d, ~date1, ~date4,\n 60,     75,    87,\n 120,    40,    78,\n 180,    30,    68,\n 240,    20,    62,\n 300,    15,    50,\n 360,    12,    27,\n 420,    10,    32,\n 480,    12,    12,\n 540,     8,    13,\n 600,     5,     5,\n 660,     4,     4\n                )\n\nseptoria |&gt; \n  pivot_longer(2:3, names_to = \"date\", \n               values_to = \"defoliation\") |&gt; \n  ggplot(aes(d, defoliation, color = date))+\n  theme_r4pde()+\n  geom_point()+\n  geom_line()+\n  scale_color_grey()+\n  annotate(geom = \"text\", x = 200, y = 12, \n           label = \"Primary gradient\", hjust = \"left\")+\n  annotate(geom = \"text\", x = 200, y = 72, \n           label = \"Secondary gradient\", hjust = \"left\")+\n  labs(x = \"Distance from focus (m)\",\n       y = \"Percent defoliation\",\n       color = \"Date\",\n       caption = \"Parker et al. (1997)\")\n\n\n\n\n\n\n\nFigure 12.3: Primary and secondary gradients of defoliation due to Septoria leaf spot on tomato\nWhen studying disease gradients, researchers need to make sure that there is a well-defined single source of inoculum. In gradients, this is called a focus (where foci are deemed the plural), from where the inoculum originates. Three types of foci can be defined: point, line or area sources. While the point source can be a plant or group of plants at any position in the plot or field (center or corner), line and area sources are usually defined as one or more rows of diseased plants at one side of the plot or field.\nCode\nlibrary(ggplot2)\n\nline &lt;- ggplot(data.frame(c(1:10),c(1:10)))+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 0, ymax = 10, fill = \"gray92\")+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 9.7, ymax = 10, color = \"black\", fill = \"#339966\")+\n  annotate(\"segment\", size = 2, x = 1, xend = 1, y = 9.5, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 3, xend = 3, y = 9.5, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 5, y = 9.5, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 7, xend = 7, y = 9.5, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 9, xend = 9, y = 9.5, yend = 2, arrow = arrow())+\n  ylim(0,10)+\n  xlim(0,10)+\n  coord_fixed()+\n  theme_void()+\n  labs(title = \"      Side line\")\n\narea &lt;- ggplot(data.frame(c(1:10),c(1:10)))+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 0, ymax = 10, fill = \"gray92\")+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 8.2, ymax = 10, color = \"black\", fill = \"#339966\")+\n  annotate(\"segment\", size = 2, x = 1, xend = 1, y = 8, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 3, xend = 3, y = 8, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 5, y = 8, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 7, xend = 7, y = 8, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 9, xend = 9, y = 8, yend = 2, arrow = arrow())+\n  ylim(0,10)+\n  xlim(0,10)+\n  coord_fixed()+\n  theme_void()+\n  labs(title = \"      Side area\")\n\npoint_central &lt;- ggplot(data.frame(c(1:10),c(1:10)))+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 0, ymax = 10, fill = \"gray92\")+\n  annotate(\"segment\", size = 2, x = 5, xend = 10, y = 5, yend = 10, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 10, y = 5, yend = 5, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 10, y = 5, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 0, y = 5, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 0, y = 5, yend = 5, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 0, y = 5, yend = 10, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 5, y = 5, yend = 10, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 5, y = 5, yend = 0, arrow = arrow())+\n   annotate(\"rect\", xmin = 5.5, xmax = 4.5, ymin = 4.5, ymax = 5.5, color = \"black\", fill = \"#339966\" )+\n  ylim(0,10)+\n  xlim(0,10)+\n  coord_fixed()+\n  theme_void()+\n  labs(title = \"    Central point/area\")\n\npoint_corner &lt;- ggplot(data.frame(c(1:10),c(1:10)))+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 0, ymax = 10, fill = \"gray92\")+\n  annotate(\"segment\", size = 2, x = 0, xend = 10, y = 10, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 6.6, y = 10, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 3.3, y = 10, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 0, y = 10, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 10, y = 10, yend = 3.3, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 10, y = 10, yend = 6.6, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 10, y = 10, yend = 10, arrow = arrow())+\n  annotate(\"rect\", xmin = 0, xmax = 1, ymin = 9, ymax = 10, color = \"black\", fill = \"#339966\")+\n  ylim(0,10)+\n  xlim(0,10)+\n  coord_fixed()+\n  theme_void()+\n  labs(title = \"    Corner point/area\")\n\nlibrary(patchwork)\np_gradients &lt;- (line | area)/\n(point_central | point_corner)\n\nggsave(\"imgs/gradients.png\", width =9, height =9, bg = \"white\")",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spatial gradients</span>"
    ]
  },
  {
    "objectID": "spatial-gradients.html#introduction",
    "href": "spatial-gradients.html#introduction",
    "title": "12  Spatial gradients",
    "section": "",
    "text": "Figure 12.4: Example of location and size of inoculum sources for the study of disease gradients\n\n\n\n\n\n\n\nMikaberidze, A., Mundt, C. C., and Bonhoeffer, S. 2015. Data from: Invasiveness of plant pathogens depends on the spatial scale of host distribution. https://doi.org/10.5061/DRYAD.F2J8S.\n\n\nMundt, C. C., Ahmed, H. U., Finckh, M. R., Nieva, L. P., and Alfonso, R. F. 1999. Primary Disease Gradients of Bacterial Blight of Rice. Phytopathology® 89:64–67. https://doi.org/10.1094/phyto.1999.89.1.64.\n\n\nParker, S. K., Nutter, F. W., and Gleason, M. L. 1997. Directional Spread of Septoria Leaf Spot in Tomato Rows. Plant Disease 81:272–276. https://doi.org/10.1094/pdis.1997.81.3.272.\n\n\nSackett, K. E., and Mundt, C. C. 2005. Primary Disease Gradients of Wheat Stripe Rust in Large Field Plots. Phytopathology® 95:983–991. https://doi.org/10.1094/phyto-95-0983.",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spatial gradients</span>"
    ]
  },
  {
    "objectID": "spatial-models.html",
    "href": "spatial-models.html",
    "title": "13  Gradient models",
    "section": "",
    "text": "13.1 Exponential model\nSimilar to the disease progress curves, models can be fitted empirically to observed disease gradient curves and provide insights into the mechanisms of inoculum dispersal and deposition, the source of inoculum, and the physical processes underlying dispersal.\nWhen modeling disease gradients, the distance is represented by \\(x\\), a continuous variable which can be expressed by various units (cm, m, km, etc). The gradient models, similar to the population dynamics models (disease progress) are of the deterministic type. The difference is that, for disease progress curves, disease intensity tends to increase with increasing time, while in disease gradients the disease intensity tends to decrease with increasing distance from the source of inoculum. Two models are most commonly fitted to data on disease gradients. More details about these models can be obtained it this tutorial.\nThe exponential model is also known as Kiyosawa & Shiyomi model. The differential of the exponential model is given by\n\\(\\frac{dy}{dx}\\) = \\(-b_{E}.y\\) ,\nwhere \\(b_{E}\\) is the exponential form of the rate of decline and \\(y\\) is the disease intensity. This model suggests that \\(y\\) (any disease intensity) is greater close to the source of inoculum, or at the distance zero. The integral form of the model is given by\n\\(y = a . e^{-b.x}\\) ,\nwhere \\(a\\) is the disease intensity at the distance zero and \\(b\\) is the rate of decline, in this case negative because disease intensity decreases with the increase of the distance from inoculum source. Let’s make a plot for two disease gradients of varying parameters for this model.\nFirst we need to load essential packages for programming, customizing the outputs and defining a global ggplot theme.\nlibrary(tidyverse)\nlibrary(r4pde)\ntheme_set(theme_r4pde()) # set global theme\nSet the parameters for the exponential model with two rates and the same inoculum level at the source:\na1 &lt;- 0.2 # y at distance zero for gradient 1\na2 &lt;- 0.2 # y at distance zero for gradient 2\nb1 &lt;- 0.1 # decline rate for gradient 1\nb2 &lt;- 0.05 # decline rate for gradient 2\nmax1 &lt;- 80 # maximum distance for gradient 1\nmax2 &lt;- 80 # maximum distance for gradient 2\ndat &lt;- data.frame(x = seq(1:max1), y = seq(0:a1))\nThe following code allows to visualize the model predictions.\ndat |&gt;\n  ggplot(aes(x, y)) +\n  theme_r4pde()+\n  stat_function(fun = function(x) a1 * exp(-b1 * x), linetype = 1) +\n  stat_function(fun = function(x) a2 * exp(-b2 * x), linetype = 2) +\n  ylim(0, a1) +\n  annotate(\"text\", x = 20, y = 0.04, label = \"b = 0.1\") +\n  annotate(\"text\", x = 20, y = 0.10, label = \"b = 0.05\") +\n  labs(x = \"Distance (m)\", y = \"Disease incidence (proportion)\"\n  )\n\n\n\n\n\n\n\nFigure 13.1: Exponential curves describing plant disease gradients",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gradient models</span>"
    ]
  },
  {
    "objectID": "spatial-models.html#power-law-model",
    "href": "spatial-models.html#power-law-model",
    "title": "13  Gradient models",
    "section": "13.2 Power law model",
    "text": "13.2 Power law model\nAlso known as the modified Gregory’s model (Gregory was a pioneer in the use this model to describe plant disease gradients). In the power law model, \\(Y\\) is proportional to the power of the distance, and is given by:\n\\(Y = a_{P}.x - b_{P}\\)\nwhere \\(a_{P}\\) and \\(b_{P}\\) are the two parameters of the power law model. They differ from the exponential because as closer to \\(x\\) is to zero, \\(Y\\) is indefinitely large (not meaningful biologically). However, the model can still be useful because it produces realistic values at any distance \\(x\\) away from the source. The values of the \\(a_{P}\\) parameter should be interpreted in accord to the scale of \\(x\\), whether in centimeters or meters. If the distance between the source and the first measure away from the source is 0.5m, it is so more appropriate to record the distance in cm than in m or km.\nOnce \\(y\\) at the distance zero from the source is undefined when using the power law model, this is usually modified by the addition of a positive constant \\(C\\) in \\(x\\):\n\\(Y = a_{P}.(x + C) - b_{P}\\)\nFor this reason, the model is named as the modified power law. Here, the constant \\(C\\) is of the same unit of \\(x\\). At the distance zero, the positive constant is a term that express the size of the inoculum source. In other words, the \\(a\\) parameter is a theoretical value of \\(Y\\) at the distance \\(1-C\\) from the center of the inoculum source.\nLet’s plot two gradients with two rate parameters for the modified power law model:\n\nC &lt;- 0.5\na1 &lt;- 0.2 # y at zero distance for gradient 1\na2 &lt;- 0.2 # y at zero distance for gradient 2\nb1 &lt;- 0.5 # decline rate for gradient 1\nb2 &lt;- 0.7 # decline rate for gradient 2\nmax1 &lt;- 80 # maximum distance for gradient 1\nmax2 &lt;- 80 # maximum distance for gradient 2\ndat2 &lt;- data.frame(x = seq(1:max1), y = seq(0:a1))\n\n\ndat2 |&gt;\n  ggplot(aes(x, y)) +\n  theme_r4pde()+\n  stat_function(fun = function(x) a1 * ((x + C)^-b1), linetype = 1) +\n  stat_function(fun = function(x) a2 * ((x + C)^-b2), linetype = 2) +\n  ylim(0, a1 - 0.02) +\n  annotate(\"text\", x = 20, y = 0.03, label = \"b = 0.1\") +\n  annotate(\"text\", x = 20, y = 0.06, label = \"b = 0.05\") +\n  labs(x = \"Distance (m)\", y = \"Disease incidence\")\n\n\n\n\n\n\n\nFigure 13.2: Power law (modified) curves describing plant disease gradients\n\n\n\n\n\nThe differential equation of the power law model is given by:\n\\(\\frac{dy}{dx}\\) = \\(\\frac{-b_{P}.Y}{x - C}\\)\nSimilar to the exponential model, \\(\\frac{dy}{dx}\\) is proportional to \\(Y\\), meaning that the gradient is steeper (more negative) at the highest disease intensity value, usually closer to the source.",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gradient models</span>"
    ]
  },
  {
    "objectID": "spatial-models.html#linearization-of-the-models",
    "href": "spatial-models.html#linearization-of-the-models",
    "title": "13  Gradient models",
    "section": "13.3 Linearization of the models",
    "text": "13.3 Linearization of the models\n\n13.3.1 Transformations of y\nThe gradient models, again similar to the temporal disease models, are non linear in their parameters. The model is intrinsically linear if transformations are applied (according to the model) in both sides of the equations. The linear model in its generic state is given by\n\\(y* = a* + bx\\) ,\nwhere the asterisk in \\(a\\) indicated that one of the transformations was applied in \\(y\\) that produced the linear model. Note that \\(a*\\) is the transformed version of the initial disease intensity, which needs to be returned to the original scale according to the respective back-transformation. Follows the linearized form of the two most common gradient models.\n\\(ln(y) = ln(a_{E}) - b_{E}. x\\)\n\\(ln(y) = ln(a_{P}) - b_{E}. ln(x+C)\\)\n\n\n13.3.2 Plot for the linearized form of models\nLet’s visualize the linearization of the exponential model with two different slopes (gradient 1 and 2). Note that the transformation used was \\(ln(y)\\).\n\nC &lt;- 0.5\na1 &lt;- 0.2 # y at zero distance for gradient 1\na2 &lt;- 0.2 # y at zero distance for gradient 2\nb1 &lt;- 0.5 # decline rate for gradient 1\nb2 &lt;- 0.7 # decline rate for gradient 2\nmax1 &lt;- 80 # maximum distance for gradient 1\nmax2 &lt;- 80 # maximum distance for gradient 2\ndat2 &lt;- data.frame(x = seq(1:max1), y = seq(0:a1))\n\ndat2 |&gt;\n  ggplot(aes(x, y)) +\n  theme_r4pde()+\n  stat_function(fun = function(x) log(a1) - (b1 * x), linetype = 1) +\n  stat_function(fun = function(x) log(a2) - (b2 * x), linetype = 2) +\n  labs(x = \"log of distance (m)\", y = \"log of disease incidence\"\n  )\n\n\n\n\n\n\n\nFigure 13.3: Linearization of the exponential model describing plant disease gradients\n\n\n\n\n\nFollows the linearization of the modified power law model. Note that the transformation used was \\(ln(y)\\) and \\(ln(x+C)\\) .\n\nC &lt;- 0.5\na1 &lt;- 0.2 # y at zero distance for gradient 1\na2 &lt;- 0.2 # y at zero distance for gradient 2\nb1 &lt;- 0.5 # decline rate for gradient 1\nb2 &lt;- 0.7 # decline rate for gradient 2\nmax1 &lt;- log(80) # maximum distance for gradient 1\nmax2 &lt;- log(80) # maximum distance for gradient 2\ndat2 &lt;- data.frame(x = seq(1:max1), y = seq(0:a1))\n\ndat2 |&gt;\n  ggplot(aes(x, y)) +\n  theme_r4pde()+\n  stat_function(fun = function(x) log(a1) - (b1 * log(x + C)), linetype = 1) +\n  stat_function(fun = function(x) log(a2) - (b2 * log(x + C)), linetype = 2) +\n  labs(\n    title = \"Modified Power Law\",\n    subtitle = \"\",\n    x = \"log of distance (m)\",\n    y = \"log of disease incidence\"\n  )\n\n\n\n\n\n\n\nFigure 13.4: Linearization of the modified power law curves describing plant disease gradients",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gradient models</span>"
    ]
  },
  {
    "objectID": "spatial-models.html#interactive-application",
    "href": "spatial-models.html#interactive-application",
    "title": "13  Gradient models",
    "section": "13.4 Interactive application",
    "text": "13.4 Interactive application\nA shiny app was developed to demonstrate these two models interactively. Click on the image below to get access to the app.\n\n\n\n\n\n\nFigure 13.5: Screenshot of the application to visualize the spatial disease gradient models by varying the model’s parameters",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gradient models</span>"
    ]
  },
  {
    "objectID": "spatial-fitting.html",
    "href": "spatial-fitting.html",
    "title": "14  Fitting gradient models",
    "section": "",
    "text": "14.1 Dataset\nThe hypothetical data describe the gradient curve for the number of lesions counted at varying distances (in meters) from the source. Let’s create two vectors, one for the distances \\(x\\) and the other for the lesion count \\(Y\\), and then a data frame by combining the two vectors.\n# create the two vectors\nx &lt;- c(0.8, 1.6, 2.4, 3.2, 4, 7.2, 12, 15.2, 21.6, 28.8)\nY &lt;- c(184.9, 113.3, 113.3, 64.1, 25, 8, 4.3, 2.5, 1, 0.8)\ngrad1 &lt;- data.frame(x, Y) # create the dataframe\nknitr::kable(grad1) # show the gradient\n\n\n\n\nx\nY\n\n\n\n\n0.8\n184.9\n\n\n1.6\n113.3\n\n\n2.4\n113.3\n\n\n3.2\n64.1\n\n\n4.0\n25.0\n\n\n7.2\n8.0\n\n\n12.0\n4.3\n\n\n15.2\n2.5\n\n\n21.6\n1.0\n\n\n28.8\n0.8",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting gradient models</span>"
    ]
  },
  {
    "objectID": "spatial-fitting.html#visualize-the-gradient-curve",
    "href": "spatial-fitting.html#visualize-the-gradient-curve",
    "title": "14  Fitting gradient models",
    "section": "14.2 Visualize the gradient curve",
    "text": "14.2 Visualize the gradient curve\nThe gradient can be visualized using ggplot function.\n\ngrad1 |&gt; \n  ggplot(aes(x, Y))+\n  theme_r4pde(font_size = 16)+\n  geom_point(size = 2)+\n  geom_line()+\n  labs(y = \"Lesion count\",\n       x = \"Distance (m)\")\n\n\n\n\n\n\n\nFigure 14.1: Hypothetical gradient of lesion count over distances from the inoculum source",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting gradient models</span>"
    ]
  },
  {
    "objectID": "spatial-fitting.html#linear-regression",
    "href": "spatial-fitting.html#linear-regression",
    "title": "14  Fitting gradient models",
    "section": "14.3 Linear regression",
    "text": "14.3 Linear regression\nOne method to determine the best-fitting model for gradient data is through linear regression. Depending on the chosen model, the transformed \\(Y\\) variable is regressed against the distance (which could be either in its original form or transformed). By doing this, we can derive the model’s parameters and evaluate its fit using various statistics. Two primary ways to appraise the model’s fit are by visually inspecting the regression line and examining the coefficient of determination (often denoted as \\(R^2\\) ). Now, let’s proceed to fit each of the three models discussed in the previous chapter.\n\n14.3.1 Exponential model\nIn this model, the log of \\(Y\\) is taken and regressed against the (untransformed) distance from the focus. Let’s fit the model and examine the summary output of model fit.\n\nreg_exp &lt;- lm(log(Y) ~ x, data = grad1)\njtools::summ(reg_exp)\n\nMODEL INFO:\nObservations: 10\nDependent Variable: log(Y)\nType: OLS linear regression \n\nMODEL FIT:\nF(1,8) = 57.39, p = 0.00\nR² = 0.88\nAdj. R² = 0.86 \n\nStandard errors: OLS\n------------------------------------------------\n                     Est.   S.E.   t val.      p\n----------------- ------- ------ -------- ------\n(Intercept)          4.58   0.35    13.00   0.00\nx                   -0.20   0.03    -7.58   0.00\n------------------------------------------------\n\n\nThe intercept \\(a\\) represents the natural logarithm (log) of the response variable when the predictor is at a distance of zero. The negative slope \\(-b\\) indicates the rate at which the response decreases as the predictor increases — this is the decline rate of the gradient. The adjusted R-squared value of 0.86 suggests that approximately 86% of the variability in the response variable can be explained by the predictor in the model. While this seems to indicate a good fit, it is essential to compare this coefficient with those from other models to determine its relative goodness of fit. Furthermore, visually inspecting a regression plot is crucial. By doing this, we can check for any patterns or residuals around the predicted line, which can provide insights into the model’s assumptions and potential areas of improvement\n\ngrad1 |&gt; \n  ggplot(aes(x, log(Y)))+\n  theme_r4pde(font_size = 16)+\n  geom_point(size = 2)+\n  geom_line()+\n  geom_abline(slope = coef(reg_exp)[[2]], intercept = coef(reg_exp)[[1]],\n              linewidth = 1, linetype = 2)+\n labs(y = \"Log of Lesion count\",\n       x = \"Distance (m)\")\n\n\n\n\n\n\n\nFigure 14.2: Fit of the exponential model to the log of lesion count over distances from the inoculum source\n\n\n\n\n\nFrom the aforementioned plot, it’s evident that the exponential model might not be the optimal choice. This inference is drawn from the noticeable patterns or residuals surrounding the regression fit line, suggesting that the model may not capture all the underlying structures in the data.\n\n\n14.3.2 Power law model\nFor the power law model, we employ a log-log transformation: the natural logarithm (log) of \\(Y\\) is regressed against the log of \\(X\\). Following this transformation, we apply the regression procedure to determine the model’s parameters. Additionally, we extract the relevant statistics to evaluate the model’s fit to the data\n\nreg_p &lt;- lm(log(Y) ~ log(x), data = grad1)\njtools::summ(reg_p)\n\nMODEL INFO:\nObservations: 10\nDependent Variable: log(Y)\nType: OLS linear regression \n\nMODEL FIT:\nF(1,8) = 203.26, p = 0.00\nR² = 0.96\nAdj. R² = 0.96 \n\nStandard errors: OLS\n------------------------------------------------\n                     Est.   S.E.   t val.      p\n----------------- ------- ------ -------- ------\n(Intercept)          5.56   0.25    22.66   0.00\nlog(x)              -1.70   0.12   -14.26   0.00\n------------------------------------------------\n\n\nThe plot presented below underscores the superiority of the power law model in comparison to the exponential model. One of the key indicators of this superior fit is the higher coefficient of determination, \\(R^2\\) for the power law model. A higher \\(R^2\\) value suggests that the model can explain a greater proportion of the variance in the dependent variable, making it a better fit for the data at hand.\n\ngrad1 |&gt; \n  ggplot(aes(log(x), log(Y)))+\n  theme_r4pde(font_size = 16)+\n  geom_point(size = 2)+\n  geom_line()+\n  geom_abline(slope = coef(reg_p)[[2]], intercept = coef(reg_p)[[1]],\n              linewidth = 1, linetype = 2)+\n labs(y = \"Log of Lesion count\",\n       x = \"Log of distance\")\n\n\n\n\n\n\n\nFigure 14.3: Fit of the power law model to the log of lesion count over log of the distance from the inoculum source\n\n\n\n\n\n\n\n14.3.3 Modified power law model\nIn the modified power model, a constant is added to \\(x\\).\n\nreg_pm &lt;- lm(log(Y) ~ log(x + 0.4), data = grad1)\njtools::summ(reg_pm)\n\nMODEL INFO:\nObservations: 10\nDependent Variable: log(Y)\nType: OLS linear regression \n\nMODEL FIT:\nF(1,8) = 302.16, p = 0.00\nR² = 0.97\nAdj. R² = 0.97 \n\nStandard errors: OLS\n-------------------------------------------------\n                      Est.   S.E.   t val.      p\n------------------ ------- ------ -------- ------\n(Intercept)           6.10   0.23    26.73   0.00\nlog(x + 0.4)         -1.88   0.11   -17.38   0.00\n-------------------------------------------------\n\n\n\ngrad1 |&gt; \n  ggplot(aes(log(x+0.4), log(Y)))+\n  theme_r4pde(font_size = 16)+\n  geom_point(size = 2)+\n  geom_line()+\n  geom_abline(slope = coef(reg_pm)[[2]], intercept = coef(reg_pm)[[1]],\n              linewidth = 1, linetype = 2)+\n labs(y = \"Log of Lesion count\",\n       x = \"Log of distance + 0.4 (m)\")\n\n\n\n\n\n\n\nFigure 14.4: Fit of the modified power law model to the log of lesion count over log + 0.4 of the distances from the inoculum source\n\n\n\n\n\nAmong the models tested, the modified power law emerges as the most suitable choice based on its highest coefficient of determination, \\(R^2\\) . This conclusion is not only supported by the statistical metrics but also visibly evident when we examine the graphs of the fitted models.To further illustrate this, we’ll generate a gradient plot. On this plot, we’ll overlay the data with the best-fitting model — the modified power law. Remember, to accurately represent the data, we’ll need to back-transform the parameter \\(a\\) before plotting.\n\ngrad1 |&gt; \n  ggplot(aes(x, Y))+\n  theme_r4pde(font_size = 16)+\n  geom_point(size = 2)+\n  geom_line()+\n  stat_function(fun = function(x) intercept = exp(coef(reg_pm)[[1]]) * ((x + 0.4)^coef(reg_pm)[[2]]), linetype = 2) +\n  labs(y = \"Lesion count\",\n       x = \"Distance (m)\")",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting gradient models</span>"
    ]
  },
  {
    "objectID": "spatial-fitting.html#fit_gradients",
    "href": "spatial-fitting.html#fit_gradients",
    "title": "14  Fitting gradient models",
    "section": "14.4 fit_gradients",
    "text": "14.4 fit_gradients\nThe fit_gradients() function of the {r4pde} package is designed to take in a dataset consisting of two variables: distance (x) and some measure of the phenomenon (Y). Using this data, the function fits each of the three models and evaluates their performance by calculating the R-squared value for each fit. The higher the R-squared value, the better that particular model explains the variation in the data. Once the models are fit, the function returns a series of outputs:\n\nA table that summarizes the parameters and fit statistics of each model.\nDiagnostic plots that show how well each model fits the data in its transformed space.\nPlots that juxtapose the original, untransformed data against the fits from each of the three models.\n\nA notable feature is the addition of a constant (C) that can be adjusted in the modified power model. This provides flexibility in tweaking the model to better fit the data if necessary. By providing a comparative analysis of three gradient models, it enables users to quickly identify which model best represents the spatial patterns in their data.\nHere is how to use the function with our grad1 dataset. Then we show the table and two plots as outputs.\n\nlibrary(r4pde)\ntheme_set(theme_r4pde(font_size = 16))\n\nfit1 &lt;- fit_gradients(grad1, C = 0.4)\n\nknitr::kable(fit1$results_table) # display the table with coefficients and stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na.(Intercept)\nse_a\nsig_a\nb.x\nse_b\nsig_b\na_back.(Intercept)\nR2\n\n\n\n\nExponential\n4.577\n0.352\n**\n-0.201\n0.027\n**\n97.222\n0.878\n\n\nPower\n5.564\n0.246\n**\n-1.698\n0.119\n**\n260.864\n0.962\n\n\nModified_Power\n6.101\n0.228\n**\n-1.884\n0.108\n**\n446.304\n0.974\n\n\n\n\nlibrary(patchwork) # to place plots side by side\n(fit1$plot_power  |\n  fit1$plot_power_original)+\n  labs(title = \"\")\n\n\n\n\n\n\n\n\nEach plot can be further customized for publication purposes.\n\n fit1$plot_power_original +\n  labs(x = \"Distance from the focus (m)\",\n       y = \"Lesion count\",\n       title = \"\")",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting gradient models</span>"
    ]
  },
  {
    "objectID": "spatial-patterns.html",
    "href": "spatial-patterns.html",
    "title": "15  Spatial patterns",
    "section": "",
    "text": "15.1 Definitions\nA spatial disease pattern can be defined as the arrangement of diseased entities relative to each other and to the architecture of the host crop (Madden et al. 2007). Such arrangement is the realization of the underlying dispersal of the pathogen, from one or several sources within and/or outside the area of interest, under the influence of physical, biological and environmental factors.\nThe study of spatial patterns is conducted at a specific time or multiple times during the epidemic. When assessed multiple times, both spatial and temporal processes can be characterized. Because epidemics change over time, it is expected that spatial patterns are not constant but change over time as well. Usually, plant pathologists are interested in determining spatial patterns at one or various spatial scales, depending on the objective of the study. The scale of interest may be a leaf or root, plant, field, municipality, state, country or even intercontinental area. The diseased units observed may vary from lesions on a single leaf to diseased fields in a large production region.\nThe patterns can be classified into two main types that occur naturally: random or aggregated. The random pattern originates because the chances for the units (leaf, plant, crop) to be infected are equal and low, and are largely independent from each other. In aggregated spatial patterns, such chances are unequal and there is dependency among the units. For example, a healthy unit close to a diseased unit is at higher risk than more distant units.\nLet’s simulate in R two vectors (x,y) for the positions of diseased units that follow a random or an aggregated pattern. For the random pattern, we use runif, a function which generates random deviates from the uniform distribution.\nset.seed(123)          # for reproducibility\nx &lt;- runif(50, 0, 30)  # x vector\ny &lt;- runif(50, 0, 30)  # y vector\ndat &lt;- data.frame(x,y) # dataframe for plotting\nNow, the plot to visualize the random pattern.\nlibrary(tidyverse) \nlibrary(r4pde)\ntheme_set(theme_r4pde())\n\npr &lt;- dat |&gt; # R base pipe operator\n  ggplot(aes(x, y))+\n  theme_r4pde(font_size = 12)+\n  geom_point(size =3, \n             color = \"darkred\")+\n  ylim(0,30)+\n  xlim(0,30)+\n  coord_fixed()+\n  labs(x = \"Distance x\", y = \"Distance y\", \n       title = \"Random\")\npr\n\n\n\n\n\n\n\nFigure 15.1: Random pattern of a plant disease epidemic\nNow, we can generate new x and y vectors using rnbinom function which allows generating values for the negative binomial distribution (which should give rise to aggregated patterns) with parameters size and prob. Let’s simulate 50 values with mean 12 and size 20 as dispersal parameter.\nx &lt;- rnbinom(n = 50, mu = 12, size = 20)\ny &lt;- rnbinom(n = 50, mu = 5, size = 20)\ndat2 &lt;- data.frame(x, y)\nThis should give us an aggregated pattern.\npag &lt;- dat2 |&gt;\n  ggplot(aes(x, y))+\n  theme_r4pde(font_size = 12)+\n  geom_point(size = 3, color = \"darkred\")+\n  ylim(0,30)+\n  xlim(0,30)+\n  coord_fixed()+\n  labs(x = \"Distance x\", y = \"Distance y\", \n       title = \"Aggregated\")\npag\n\n\n\n\n\n\n\nFigure 15.2: Aggregated pattern of a plant disease epidemic\nA rare pattern found in nature is the regular pattern, but it may be generated artificially by the man when conducting experimentation. Follows a code to produce the regular pattern.\nx &lt;- rep(c(0,5,10,15,20, 25, 30, 35, 40, 45), 5) \ny &lt;- rep(c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45), each = 10)\ndat3 &lt;- data.frame(x, y)\n\npreg &lt;- dat3 |&gt;\n  ggplot(aes(x, y))+\n  theme_r4pde(font_size = 12)+\n  geom_point(size = 3, color = \"darkred\")+\n  ylim(0,30)+\n  xlim(0,30)+\n  coord_fixed()+\n  labs(x = \"Distance x\", y = \"Distance y\", \n       title = \"Regular\")\npreg\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 15.3: Regular pattern of a plant disease epidemic\nlibrary(patchwork)\npreg + pr + pag\nggsave(\"imgs/spatial.png\", width = 10, height = 4)\n\n\n\n\n\n\n\nFigure 15.4: Patterns of a plant disease epidemic",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Spatial patterns</span>"
    ]
  },
  {
    "objectID": "spatial-patterns.html#spatiotemporal",
    "href": "spatial-patterns.html#spatiotemporal",
    "title": "15  Spatial patterns",
    "section": "15.2 Spatiotemporal",
    "text": "15.2 Spatiotemporal\nThe location of diseased plants can be assessed over time and so we can appraise both the progress and pattern of the epidemics. Let’s visualize spatial data collected from actual epidemics monitored (plant is diseased or not diseased) during six times during the epidemics. The data is available in the epiphy R package. Let’s use only one variety and one irrigation type.\n\nlibrary(epiphy)\ntswv_1928 &lt;- tomato_tswv$field_1928\n\ntswv_1928 |&gt;\n  filter(variety == \"Burwood-Prize\"&\n         irrigation == \"trenches\") |&gt; \n  ggplot(aes(x, y, fill= factor(i)))+\n  geom_tile(color = \"black\")+\n  coord_fixed()+\n  scale_fill_manual(values = c(\"grey70\", \"darkred\"))+\n  labs(fill = \"Status\", title = \"\")+\n  theme_void()+\n  theme(legend.position = \"bottom\")+\n  facet_wrap(~ t, nrow =1)\n\n\n\n\n\n\n\nFigure 15.5: Spatial patterns of tomato spotted wilt virus at six assessment times\n\n\n\n\n\nIn this other example, the severity of gummy stem blight (Didymella bryoniae) of watermelon (Café-Filho et al. 2010) was recorded in a 0-4 ordinal scale over time (days after planting) and space, in a naturally-infected rain-fed commercial field, to evaluate the effect of the distance of initial inoculum on the intensity of the disease. The dataset is included in the {r4pde} package that accompanies the book.\n\nlibrary(r4pde)\ndf &lt;- DidymellaWatermelon\n\ndf |&gt; \n  ggplot(aes(NS_col, EW_row, fill = severity))+\n  coord_fixed()+\n  geom_tile (color = \"white\")+\n  theme_void()+\n  theme(legend.position = c(0.9,0.25))+\n scale_fill_gradient(low = \"grey70\", high = \"darkred\")+\n  facet_wrap(~ dap, ncol = 4)\n\n\n\n\n\n\n\nFigure 15.6: Spatial patterns of the severity (0-4 ordinal scale) gummy stem blight of watermelon during seven times after the day of planting (Café-Filho et al. 2010)\n\n\n\n\n\nWe can see that the disease spread from the initial focus detected at 50 days after planting, taking the entire field 37 days later with various levels of severity.",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Spatial patterns</span>"
    ]
  },
  {
    "objectID": "spatial-patterns.html#simulating-spatial-patterns",
    "href": "spatial-patterns.html#simulating-spatial-patterns",
    "title": "15  Spatial patterns",
    "section": "15.3 Simulating spatial patterns",
    "text": "15.3 Simulating spatial patterns\nTwo Shiny apps have been developed to allow simulating various spatial disease patterns. The first generates a disease- or pathogen-only data where the units are located in a scatter plot where the user can define the number of cells of the grid as well as the number of points to be plotted and the realized pattern: random or aggregated.\n\n\n\n\n\n\nFigure 15.7: Screenshot of a Shiny app to simulate disease-only data in a grid\n\n\n\nThe second app generates an artificial plantation with presence-absence data in a 2D map. The user can define the number of rows and number of plants per row and the realized pattern: random or aggregated. The latter pattern can start from the center or border of the plantation. The app calculates the number of foci and the final incidence (proportion of diseased plants).\n\n\n\n\n\n\nFigure 15.8: Screenshot of a Shiny app to simulate a presence-absence data in a 2D map\n\n\n\n\n\n\n\nCafé-Filho, A. C., Santos, G. R., and Laranjeira, F. F. 2010. Temporal and spatial dynamics of watermelon gummy stem blight epidemics. European Journal of Plant Pathology 128:473–482. https://doi.org/10.1007/s10658-010-9674-1.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. Spatial aspects of epidemicsIII: Patterns of plant disease. In The American Phytopathological Society, pp. 235–278. https://doi.org/10.1094/9780890545058.009.",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Spatial patterns</span>"
    ]
  },
  {
    "objectID": "spatial-tests.html",
    "href": "spatial-tests.html",
    "title": "16  Tests for patterns",
    "section": "",
    "text": "16.1 Intensively mapped\nA range of techniques, most based on statistical tests, can be used to detect deviations from randomness in space. The choice of the methods depends on the question, the nature of the data and scale of observation. Usually, more than one test is applied for the same or different scales of interest depending on how the data are collected.\nThe several exploratory or inferential methods can be classified based on the spatial scale and type of data (binary, count, etc.) collected, but mainly if the spatial location of the unit is known (mapped) or not known (sampled). Following Madden et al. (2007b), two major groups can be formed. The first group uses intensively mapped data for which the location [x,y] of the sampling unit is known. Examples of data include binary data (plant is infected or not infected) in planting row, point pattern (spatial arrangements of points in a 2-D space) and quadrat data (grids are superimposed on point pattern data). The second group is the sparsely sampled data in the form of count or proportion (incidence) data for which the location is not known or, if known, not taken into account in the analysis.",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Tests for patterns</span>"
    ]
  },
  {
    "objectID": "spatial-tests.html#intensively-mapped",
    "href": "spatial-tests.html#intensively-mapped",
    "title": "16  Tests for patterns",
    "section": "",
    "text": "16.1.1 Binary data\nIn this situation the individual plants are mapped, meaning that their relative positions to one another are known. It is the case when a census is used to map presence/absence data. The status of each unit (usually a plant) is noted as a binary variable. The plant is either diseased (D or 1) or non-diseased or healthy (H or 0). Several statistical tests can be used to detect a deviation from randomness. The most commonly used tests are runs, doublets and join count.\n\n16.1.1.1 Runs test\nA run is defined as a succession of one or more diseased (D) or healthy (H) plants, which are followed and preceded by a plant of the other disease status or no plant at all (Madden 1982). There would be few runs if there is an aggregation of diseased or healthy plants and a large number of runs for a random mixing of diseased and healthy plants.\nLet’s create a vector of binary (0 = non-diseased; 1 = diseased) data representing a crop row with 32 plants and assign it to y. For plotting purposes, we make a data frame for more complete information.\n\nlibrary(tidyverse) \n\n\ny1 &lt;- c(1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,0,0)\nx1 &lt;- c(1:32) # position of each plant\nz1 &lt;- 1\nrow1 &lt;- data.frame(x1, y1, z1) # create a dataframe\n\nWe can then visualize the series using ggplot and count the number of runs as 8, aided by the different colors used to identify a run.\n\nlibrary(grid)\nruns2 &lt;- row1 |&gt;\n  ggplot(aes(x1, z1, label = x1, color = factor(y1))) +\n  geom_point(shape = 15, size = 10) +\n  scale_x_continuous(breaks = max(z1)) +\n  scale_color_manual(values = c(\"gray70\", \"darkred\")) +\n  geom_text(vjust = 0, nudge_y = 0.5) +\n  coord_fixed() +\n  ylim(0, 2.5) +\n  theme_void() +\n   theme(legend.position = \"top\")+\n  labs(color = \"Status\")\nggsave(\"imgs/runs2.png\", runs2, width = 12, height = 2, bg = \"white\")\n\n\n\n\n\n\n\nFigure 16.2: Sequence of diseased (dark red) or non-diseased (gray) units (plants). The numbers represent the position of the unit\n\n\n\nWe can obtain the number of runs and related statistics using the oruns_test() function of the {r4pde}.\n\nlibrary(r4pde)\noruns_test(row1$y1)\n\nOrdinary Runs Test of Data Sequence:\n -------------------------------------\n Total Number of Runs (U): 8\n Expected Number of Runs (EU): 16.75\n Standard Deviation of Runs (sU): 2.74\n Z-score: -3.20\n P-value: 0.0007\n\n Interpretation:\n Based on the Z-score, the sequence exhibits 'aggregation or clustering'.\n\n\n\n\n16.1.1.2 Join count\nIn a joint count statistics, two adjacent plants may be classified by the type of join that links them: D-D, H-H or H-D. The number of joins of the specified type in the orientation(s) of interest is then counted. The question is whether the observed join-count is large (or small) relative to that expected for a random pattern. The join-count statistics provides a basic measure of spatial autocorrelation. The expected number of join counts can defined under randomness and the corresponding standard errors can be estimated after constants are calculated based on the number of rows and columns of the matrix.\nLet’s use the join_count() function of the {r4pde} package to perform a join count test. The formulations in this function apply only for a rectangular array with no missing values, with the “rook” definition of proximity, and they were all presented in the book The Study of Plant Disease Epidemics, page 261 (Madden et al. 2007a).\nLet’s create a series of binary data from left to right and top to bottom. The data is displayed in Fig. 9.13 in page 260 of the book (Madden et al. 2007a). In the example, there are 5 rows and 5 columns. This will be informed later to compose the matrix which is the data format for analysis.\n\nm1 &lt;- c(1,0,1,1,0,\n       1,1,0,0,0,\n       1,0,1,0,0,\n       1,0,0,1,0,\n       0,1,0,1,1)\nmatrix1 &lt;- matrix(m1, 5, 5, byrow = TRUE)\n\nWe can visualize the two-dimensional array by converting to a raster.\n\n# Convert to raster \nmapS2 &lt;- terra::rast(matrix(matrix1, 5 , 5, byrow = TRUE))\n# Convert to data frame\nmapS3 &lt;- terra::as.data.frame(mapS2, xy = TRUE)\nmapS3 |&gt;\n  ggplot(aes(x, y, label = lyr.1, fill = factor(lyr.1))) +\n  geom_tile(color = \"white\", linewidth = 0.5) +\n  theme_void() +\n  coord_fixed()+\n  labs(fill = \"Status\") +\n  scale_fill_manual(values = c(\"gray70\", \"darkred\"))+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nFigure 16.3: Visualization of a matrix of presence/absence data representing a disease spatial pattern\n\n\n\n\n\nThe function join_count() calculates spatial statistics for a matrix based on the specifications and calculations shown in the book (Madden et al. 2007a). It identifies patterns of aggregation for values in a binary matrix based on join count statistics. The results determine whether the observed spatial arrangement is aggregated or non-aggregated (random) based on a standard normal distribution test statistic Z-score applied separately for HD or DD sequences. For HD, Z-score lower than - 1.64 (more negative) is taken as a basis for rejection of hypothesis of randomness (P = 0.05). For DD sequences, Z-score greater than 1.64 indicates aggregation.\n\nlibrary(r4pde)\njoin_count(matrix1)\n\nJoin Count Analysis of Spatial Patterns of Plant Diseases:\n ----------------------------------------------------------\n 1) 'HD' Sequences:\n    - Observed Count: 23\n    - Expected Count : 19.97\n    - Standard Deviation: 3.17\n    - Z-score: 0.80\n The pattern for 'HD' sequences is 'not aggregated'.\n\n 2) 'DD' Sequences:\n    - Observed Count: 7\n    - Expected Count: 9.22\n    - Standard Deviation: 4.23\n    - Z-score: -0.41\n The pattern for 'DD' sequences is 'not aggregated'.\n ----------------------------------------------------------\n\n\nIn the example above, the observed count of HD sequence was larger than the expected count, so the Z-score suggests non aggregation, or randomness. The DD sequence observed count was lower than expected count, confirming non aggregation. Let’s repeat the procedure using the second array of data shown in the book chapter, for which the result is different. In this case, there is evidence of aggregation of diseased plants, because the observed DD is greater than expected and observed HD is lower than expected. The Z-score for HD is less than -1.64 (P&lt; 0.05) and the Z-score for DD is greater than 1.64 (P &lt; 0.05).\n\nm2 &lt;- c(1,1,1,0,0,\n       1,1,1,0,0,\n       1,1,1,0,0,\n       1,1,1,0,0,\n       0,0,0,0,0)\nmatrix2 &lt;- matrix(m2, 5, 5, byrow = TRUE)\n\njoin_count(matrix2)\n\nJoin Count Analysis of Spatial Patterns of Plant Diseases:\n ----------------------------------------------------------\n 1) 'HD' Sequences:\n    - Observed Count: 7\n    - Expected Count : 19.97\n    - Standard Deviation: 3.17\n    - Z-score: -4.24\n The pattern for 'HD' sequences is 'aggregated'.\n\n 2) 'DD' Sequences:\n    - Observed Count: 17\n    - Expected Count: 9.22\n    - Standard Deviation: 4.23\n    - Z-score: 1.96\n The pattern for 'DD' sequences is 'aggregated'.\n ----------------------------------------------------------\n\n\nWe can apply these tests for a real example epidemic data provided by the {r4pde} R package. Let’s work with a portion of the intensively mapped data on the occurrence of gummy stem blight (GSB) of watermelon (Café-Filho et al. 2010).\n\nlibrary(r4pde)\ngsb &lt;- DidymellaWatermelon\ngsb \n\n# A tibble: 1,344 × 4\n     dap NS_col EW_row severity\n   &lt;int&gt;  &lt;int&gt;  &lt;int&gt;    &lt;int&gt;\n 1    50      1      1        0\n 2    50      1      2        0\n 3    50      1      3        0\n 4    50      1      4        0\n 5    50      1      5        0\n 6    50      1      6        0\n 7    50      1      7        0\n 8    50      1      8        0\n 9    50      1      9        0\n10    50      1     10        0\n# ℹ 1,334 more rows\n\n\nThe inspection of the data frame shows four variables where dap is the day after planting, NS_col is the north-south direction, EW_row is the east-west direction and severity is the severity in ordinal score (0-4). Let’s produce a map for the 65 and 74 dap, but first we need to create the incidence variable based on severity.\n\ngsb2 &lt;- gsb |&gt;\n  filter(dap %in% c(65, 74)) |&gt; \n  mutate(incidence = case_when(severity &gt; 0 ~ 1,\n                               TRUE ~ 0))\n\ngsb2 |&gt; \n  ggplot(aes(NS_col, EW_row, fill = factor(incidence)))+\n  geom_tile(color = \"white\") +\n  coord_fixed() +\n  scale_fill_manual(values = c(\"gray70\", \"darkred\")) +\n  facet_wrap( ~ dap) +\n  labs(fill = \"Status\")+\n  theme_void()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nFigure 16.4: Incidence maps for gummy stem blight of watermelon\n\n\n\n\n\nNow w eshould check the number of rows (y) and columns (x) for further preparing the matrix for the join count statistics. The m65 object should be in the matrix format before running the join count test.\n\nmax(gsb2$NS_col)\n\n[1] 12\n\nmax(gsb2$EW_row)\n\n[1] 16\n\nm65 &lt;- gsb2 |&gt; \n  filter(dap == 65) |&gt; \n  pull(incidence) |&gt; \n  matrix(12, 16, byrow = TRUE)\n\njoin_count(m65)\n\nJoin Count Analysis of Spatial Patterns of Plant Diseases:\n ----------------------------------------------------------\n 1) 'HD' Sequences:\n    - Observed Count: 104\n    - Expected Count : 108.47\n    - Standard Deviation: 13.89\n    - Z-score: -0.36\n The pattern for 'HD' sequences is 'not aggregated'.\n\n 2) 'DD' Sequences:\n    - Observed Count: 15\n    - Expected Count: 12.52\n    - Standard Deviation: 4.76\n    - Z-score: 0.63\n The pattern for 'DD' sequences is 'not aggregated'.\n ----------------------------------------------------------\n\n\nWe can apply the join count test for 74 dap. The result shows that the pattern is now aggregated, differing from 64 dap.\n\n# Pull the binary sequence of time 2\nm74 &lt;- gsb2 |&gt; \n  filter(dap == 74) |&gt; \n  pull(incidence) |&gt; \n  matrix(12, 16, byrow = TRUE)\n\njoin_count(m74)\n\nJoin Count Analysis of Spatial Patterns of Plant Diseases:\n ----------------------------------------------------------\n 1) 'HD' Sequences:\n    - Observed Count: 110\n    - Expected Count : 155.67\n    - Standard Deviation: 11.91\n    - Z-score: -3.88\n The pattern for 'HD' sequences is 'aggregated'.\n\n 2) 'DD' Sequences:\n    - Observed Count: 58\n    - Expected Count: 37.12\n    - Standard Deviation: 8.85\n    - Z-score: 2.42\n The pattern for 'DD' sequences is 'aggregated'.\n ----------------------------------------------------------\n\n\n\n\n16.1.1.3 Foci analysis\nThe Analysis of Foci Structure and Dynamics (AFSD), introduced by (Nelson 1996) and further expanded by (Laranjeira et al. 1998), was used in several studies on citrus diseases in Brazil. In this analysis, the data come from incidence maps where both the diseased and no-diseased trees are mapped in the 2D plane (Jesus Junior and Bassanezi 2004; Laranjeira et al. 2004).\nHere is an example of an incidence map with four foci (adapted from (Laranjeira et al. 1998)). The data is organized in the wide format where the first column x is the index for the row and each column is the position of the plant within the row. The 0 and 1 represent the non-diseased and diseased plant, respectively.\n\nfoci &lt;- tibble::tribble(\n           ~x, ~`1`, ~`2`, ~`3`, ~`4`, ~`5`, ~`6`, ~`7`, ~`8`, ~`9`,\n           1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           2,   1,   1,   1,   0,   0,   0,   0,   1,   0,\n           3,   1,   1,   1,   0,   0,   0,   1,   1,   1,\n           4,   0,   1,   1,   0,   0,   0,   0,   1,   0,\n           5,   0,   1,   1,   0,   0,   0,   0,   0,   0,\n           6,   0,   0,   0,   1,   0,   0,   0,   0,   0,\n           7,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           8,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           9,   0,   0,   0,   0,   0,   1,   0,   1,   0,\n          10,   0,   0,   0,   0,   0,   0,   1,   0,   0,\n          11,   0,   1,   0,   0,   0,   1,   0,   1,   0,\n          12,   0,   0,   0,   0,   0,   0,   0,   0,   0\n          )\n\nSince the data frame is in the wide format, we need to reshape it to the long format using pivot_longer function of the tidyr package before plotting using ggplot2 package.\n\nlibrary(tidyr)\n\nfoci2 &lt;- foci |&gt; \n  pivot_longer(2:10, names_to = \"y\", values_to = \"i\")\nfoci2\n\n# A tibble: 108 × 3\n       x y         i\n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n 1     1 1         0\n 2     1 2         0\n 3     1 3         0\n 4     1 4         0\n 5     1 5         0\n 6     1 6         0\n 7     1 7         0\n 8     1 8         0\n 9     1 9         0\n10     2 1         1\n# ℹ 98 more rows\n\n\nNow we can make the plot.\n\nlibrary(ggplot2)\nfoci2 |&gt; \n  ggplot(aes(x, y, fill = factor(i)))+\n  geom_tile(color = \"black\")+\n  scale_fill_manual(values = c(\"grey70\", \"darkred\"))+\n  theme_void()+\n  coord_fixed()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 16.5: Examples of foci of plant diseases - see text for description\n\n\n\n\n\nIn the above plot, the upper left focus is composed of four diseased plants with a pattern of vertical and horizontal proximity to the central unit (or the Rook’s case). The upper right focus, also with four diseased plants denotes a pattern of longitudinal proximity to the central unit (or the Bishop’s case). The lower left focus is composed of 11 diseased plants with 4 rows and 6 columns occupied by the focus; the shape index of the focus (SIF) is 1.25 and the compactness index of the focus (CIF) is 0.55. The lower right is a single-unit focus.\nIn this analysis, several statistics can be summarized, both at the single focus and averaging across all foci in the area, including:\n\nNumber of foci (NF) and number of single focus (NSF)\nTo compare maps with different number of plants, NF and NSF can be normalized to 1000 plants as NF1000 and NSF1000\nNumber of plants in each focus i (NPFi)\nMaximum number of rows of the focus i (rfi) and maximum number of columns of the focus i (cfi)\nMean shape index of foci (meanSIF = [∑(fri / cfi)]/NF), where SIF values equal to 1.0 indicate isodiametrical foci; values greater than 1.0 indicate foci with greater length in the direction between the planting rows and values less than 1 indicate foci with greater length in the direction of the planting row.\nMean compactness index of foci (meanCIF = [∑(NPFi/rfi*cfi)]/NF), where CIF values close to 1.0 indicate a more compact foci, that is, greater aggregation and proximity among all the plants belonging to the focus\n\nWe can obtain the above-mentioned foci statistics using the AFSD function of the r4pde package. Let’s calculate for the foci2 dataset already loaded, but first we need to check whether all variables are numeric or integer.\n\nstr(foci2) # y was not numeric\n\ntibble [108 × 3] (S3: tbl_df/tbl/data.frame)\n $ x: num [1:108] 1 1 1 1 1 1 1 1 1 2 ...\n $ y: chr [1:108] \"1\" \"2\" \"3\" \"4\" ...\n $ i: num [1:108] 0 0 0 0 0 0 0 0 0 1 ...\n\nfoci2$y &lt;- as.integer(foci2$y) # transform to numeric\n\nlibrary(r4pde)\nresult_foci &lt;- AFSD(foci2)\n\nThe AFSD function returns a list of three data frames. The first is a summary statistics of this analysis, together with the disease incidence (DIS_INC), for the data frame in analysis.\n\nknitr::kable(result_foci[[1]])\n\n\n\n\nstats\nvalue\n\n\n\n\nNF\n4.0000000\n\n\nNF1000\n37.0370370\n\n\nNSF\n1.0000000\n\n\nNSF1000\n9.2592593\n\n\nDIS_INC\n0.2037037\n\n\nmean_SIF\n1.0625000\n\n\nmean_CIF\n0.6652778\n\n\n\n\n\nThe second object in the list is a data frame with statistics at the focus level, including the number of rows and columns occupied by each focus as well as the two indices for each focus: shape and compactness.\n\nknitr::kable(result_foci[[2]])\n\n\n\n\nfocus_id\nsize\nrows\ncols\nSIF\nCIF\n\n\n\n\n1\n11\n4\n5\n0.8\n0.5500000\n\n\n2\n5\n3\n3\n1.0\n0.5555556\n\n\n3\n5\n3\n3\n1.0\n0.5555556\n\n\n4\n1\n1\n1\n1.0\n1.0000000\n\n\n\n\n\nThe third object is the original data frame amended with the id for each focus which can be plotted and labelled (the focus ID) using the plot_AFSD() function.\n\nfoci_data &lt;- result_foci[[3]]\nfoci_data\n\n# A tibble: 22 × 4\n       x     y     i focus_id\n   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     2     1     1        1\n 2     2     2     1        1\n 3     2     3     1        1\n 4     2     8     1        2\n 5     3     1     1        1\n 6     3     2     1        1\n 7     3     3     1        1\n 8     3     7     1        2\n 9     3     8     1        2\n10     3     9     1        2\n# ℹ 12 more rows\n\n\nThe plot shows the ID for each focus.\n\nplot_AFSD(foci_data)+\n  theme_void()\n\n\n\n\n\n\n\n\nWe will now analyse the gummy stem blight dataset again focusing on 65 and 74 dap. We first need to prepare the data for the analysis by getting the x, y and i vectors in the dataframe.\n\nlibrary(r4pde)\ngsb65 &lt;- DidymellaWatermelon |&gt; \n  filter(dap %in% c(65)) |&gt; \n  mutate(incidence = case_when(severity &gt; 0 ~ 1,\n                               TRUE ~ 0)) |&gt; \n  select(EW_row, NS_col, incidence) |&gt; \n  rename(x = EW_row, y = NS_col, i = incidence)\n\nNow we can run the AFSD function and obtain the statistics.\n\nresult_df1 &lt;- AFSD(gsb65)\n\nknitr::kable(result_df1[[1]])\n\n\n\n\nstats\nvalue\n\n\n\n\nNF\n12.0000000\n\n\nNF1000\n62.5000000\n\n\nNSF\n2.0000000\n\n\nNSF1000\n10.4166667\n\n\nDIS_INC\n0.1875000\n\n\nmean_SIF\n1.0722222\n\n\nmean_CIF\n0.7162037\n\n\n\n\n\nThis analysis is usually applied to multiple maps and the statistics are visually related to the incidence in the area in a scatter plot. Let’s calculate the statistics for the two dap . We can split it by dap before applying the function. We can do it using the map function of the purrr package.\n\nlibrary(purrr)\ngsb65_74 &lt;- DidymellaWatermelon |&gt; \n  filter(dap %in% c(65, 74)) |&gt; \n  mutate(incidence = case_when(severity &gt; 0 ~ 1,\n                               TRUE ~ 0)) |&gt; \n  select(dap, EW_row, NS_col, incidence) |&gt; \n  rename(dap = dap, x = EW_row, y = NS_col, i = incidence)\n\n# Split the dataframe by 'time'\ndf_split &lt;- split(gsb65_74, gsb65_74$dap)\n\n# Apply the AFSD function to each split dataframe\nresults &lt;- map(df_split, AFSD)\n\nWe can check the summary results for time 2 and time 3.\n\ntime65 &lt;- data.frame(results[[1]][1])\nknitr::kable(time65)\n\n\n\n\nstats\nvalue\n\n\n\n\nNF\n12.0000000\n\n\nNF1000\n62.5000000\n\n\nNSF\n2.0000000\n\n\nNSF1000\n10.4166667\n\n\nDIS_INC\n0.1875000\n\n\nmean_SIF\n1.0722222\n\n\nmean_CIF\n0.7162037\n\n\n\n\ntime74 &lt;- data.frame(results[[2]][1])\nknitr::kable(time74)\n\n\n\n\nstats\nvalue\n\n\n\n\nNF\n10.0000000\n\n\nNF1000\n52.0833333\n\n\nNSF\n2.0000000\n\n\nNSF1000\n10.4166667\n\n\nDIS_INC\n0.3229167\n\n\nmean_SIF\n1.1338095\n\n\nmean_CIF\n0.7525022\n\n\n\n\n# Plot the results to see the two foci in time 1\nplot_AFSD(results[[1]][[3]])+\n  theme_void()+\n  coord_fixed()\n\n\n\n\n\n\n\n# Plot time 2\nplot_AFSD(results[[2]][[3]])+\n  theme_void()+\n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\n16.1.2 Point pattern analysis\nPoint pattern analysis involves the study of the spatial arrangement of points in a two-dimensional space. In its simplest form, one can visualize this as a scatter plot on a map, where each point represents an event, object, or entity in space. For example, the points might represent the locations of diseased plants in a population.\nThe easiest way to visualize a 2-D point pattern is to produce a map of the locations, which is simply a scatter plot but with the provision that the axes are equally scaled. However, while the visualization can provide a basic understanding of the spatial distribution, the real power of point pattern analysis lies in the quantitative methods that allow one to analyze the distribution in a more detailed and systematic way. These methods help to identify whether the points are randomly distributed, clustered (points are closer together than expected by chance), or regularly spaced (points are more evenly spaced than expected by chance). This analysis can provide insights into underlying processes that might explain the observed patterns.\nLet’s work with two simulated datasets that were originally generated to produced a random or an aggregated (clustered) pattern.\n\nlibrary(r4pde)\nrand &lt;- SpatialRandom\naggr &lt;- SpatialAggregated\n\nLet’s produce 2-D map for each data frame.\n\nprand &lt;- rand |&gt; \n  ggplot(aes(x, y))+\n  geom_point()+\n  scale_color_manual(values = c(\"black\", NA))+\n  coord_fixed()+\n  coord_flip()+\n   theme_r4pde(font_size = 12)+\n  theme(legend.position = \"none\")+\n  labs (title = \"Random\", \n        x = \"Latitude\",\n        y = \"Longitude\",\n        caption = \"Source: r4pd R package\")\n\npaggr &lt;- aggr |&gt; \n  ggplot(aes(x, y))+\n  geom_point()+\n  scale_color_manual(values = c(\"black\", NA))+\n  coord_fixed()+\n  coord_flip()+\n  theme_r4pde(font_size = 12)+\n  theme(legend.position = \"none\")+\n  labs (title = \"Aggregated\", \n        x = \"Latitude\",\n        y = \"Longitude\",\n        caption = \"Source: r4pd R package\")\n\nlibrary(patchwork)\nprand | paggr\n\n\n\n\n\n\n\n\n\n16.1.2.1 Quadrat count\nIn the Quadrat Count method, the study region is divided into a regular grid of smaller, equally-sized rectangular or square subregions known as quadrats. For each quadrat, the number of points falling inside it is counted. If the points are uniformly and independently distributed across the region (i.e., random), then the number of points in each quadrat should follow a Poisson distribution. If the variance of the counts is roughly equal to the mean of the counts, then the pattern is considered to be random. If the variance is greater than the mean, it suggests that the pattern is aggregated or clumped.\nUsing the {spatstats} package, we first need to create a ppp object which represents a point pattern. This is the primary object type in spatstat for point patterns.\n\nlibrary(spatstat)\n\n### Create rectangular window around the points\nwindow_rand &lt;- bounding.box.xy(rand$x, rand$y)\n\n# create the point pattern object\nppp_rand &lt;- ppp(rand$x, rand$y, window_rand)\nplot(ppp_rand)\n\n\n\n\n\n\n\n\nUsing the quadratcount function, we can divide the study region into a grid and count the number of points in each cell:\n\n## Quadrat count 8 x 8\nqq &lt;- quadratcount(ppp_rand,8,8, keepempty = TRUE) \n\n# plot the quadrat count\nplot(qq)\n\n\n\n\n\n\n\n\nTo determine whether the observed distribution of points is consistent with a random Poisson process, we can use thequadrat.test function:\n\n# Quadrat test\nqt &lt;- quadrat.test(qq, alternative=\"clustered\", method=\"M\")\nqt\n\n\n    Conditional Monte Carlo test of CSR using quadrat counts\n    Test statistic: Pearson X2 statistic\n\ndata:  \nX2 = 47.04, p-value = 0.9375\nalternative hypothesis: clustered\n\nQuadrats: 8 by 8 grid of tiles\n\n\nThe result will give a Chi-squared statistic and a p-value. If the p-value is very low, then the pattern is likely not random. Keep in mind that the choice of the number and size of quadrats can affect the results. It’s often helpful to try a few different configurations to ensure robust conclusions.\nLets repeat this procedure for the situation of aggregated data.\n\n### Create a rectangular window around the points \nwindow_aggr &lt;- bounding.box.xy(aggr$x, aggr$y)\n\n# create the point pattern object\nppp_aggr &lt;- ppp(aggr$x, aggr$y, window_aggr)\nplot(ppp_aggr)\n\n\n\n\n\n\n\n## Quadrat count 8 x 8\nqq_aggr &lt;- quadratcount(ppp_aggr,8,8, keepempty=TRUE) \n\n# plot the quadrat count\nplot(qq_aggr)\n\n\n\n\n\n\n\n# Quadrat test\nqt_aggr &lt;- quadrat.test(qq_aggr, alternative=\"clustered\", method=\"M\")\nqt_aggr\n\n\n    Conditional Monte Carlo test of CSR using quadrat counts\n    Test statistic: Pearson X2 statistic\n\ndata:  \nX2 = 244.92, p-value = 5e-04\nalternative hypothesis: clustered\n\nQuadrats: 8 by 8 grid of tiles\n\n\n\n\n16.1.2.2 Spatial KS test\nThe Spatial Kolmogorov-Smirnov (KS) Test is a method to assess the goodness-of-fit of a given point pattern to the assumptions of Complete Spatial Randomness (CSR) Baddeley et al. (2005). Essentially, this means that it helps in determining whether a set of spatial points is distributed randomly or if there is some underlying pattern or interaction.\nHowever, unlike other goodness-of-fit tests in the spatial context, the Spatial KS Test leverages the values of a spatial covariate at the observed data points and contrasts them against the expected distribution of the same covariate under the assumption of CSR. The idea behind this test is to check if there’s any difference between the observed distribution of a spatial covariate’s values and the expected distribution if the points were distributed in a completely spatial random fashion.\nKey points of the test are:\n\nCovariate: For this test, a spatial covariate must be chosen. This is a spatially varying feature or value that might influence the point process. Examples include elevation, soil quality, or distance from a specific feature. The distribution of this covariate’s values at the observed data points is the crux of the test.\nComparison with CSR: The observed distribution of the spatial covariate’s values at the data points is juxtaposed with what would be expected under the CSR model. The CSR model posits that points are distributed purely by chance, without any underlying structure or influence.\nMethodology: The test employs a classical goodness-of-fit approach to contrast the observed and expected distributions. Specifically, it utilizes the Kolmogorov-Smirnov statistic, a non-parametric measure to gauge the similarity between two distributions.\n\nAs example, we may want to select spatial coordinates themselves (like x, y, or both) as the covariates. This means the test will assess how the observed distribution of x-coordinates (or y-coordinates or both) of the data points compares to what would be anticipated under CSR.\nLet’s test for the aggregated data.\n\n# y as covariate\nks_y &lt;- cdf.test(ppp_aggr, test=\"ks\", \"y\", jitter=FALSE)\nks_y\n\n\n    Spatial Kolmogorov-Smirnov test of CSR in two dimensions\n\ndata:  covariate 'y' evaluated at points of 'ppp_aggr' \n     and transformed to uniform distribution under CSR\nD = 0.070995, p-value = 0.2453\nalternative hypothesis: two-sided\n\nplot(ks_y)\n\n\n\n\n\n\n\n# x as covariate\nks_x &lt;- cdf.test(ppp_aggr, test=\"ks\", \"x\", jitter=FALSE)\nks_x\n\n\n    Spatial Kolmogorov-Smirnov test of CSR in two dimensions\n\ndata:  covariate 'x' evaluated at points of 'ppp_aggr' \n     and transformed to uniform distribution under CSR\nD = 0.093863, p-value = 0.0512\nalternative hypothesis: two-sided\n\nplot(ks_x)\n\n\n\n\n\n\n\n# x and y as covariates\nfun &lt;- function(x,y){2* x + y}\nks_xy &lt;- cdf.test(ppp_aggr, test=\"ks\", fun, jitter=FALSE)\nks_xy\n\n\n    Spatial Kolmogorov-Smirnov test of CSR in two dimensions\n\ndata:  covariate 'fun' evaluated at points of 'ppp_aggr' \n     and transformed to uniform distribution under CSR\nD = 0.12687, p-value = 0.002471\nalternative hypothesis: two-sided\n\nplot(ks_xy)\n\n\n\n\n\n\n\n\nAs shown above, we have sufficient evidence to reject the null hypothesis of complete spatial randomness.\n\n\n16.1.2.3 Distance based\n\n16.1.2.3.1 Ripley’s K\nA spatial point process is a set of irregularly distributed locations within a defined region which are assumed to have been generated by some form of stochastic mechanism. The K function, a.k.a. Ripley’s K-function, is a statistical measure used in spatial analysis to examine the spatial distribution of a single type of point in a given area. Named after its developer, the British statistician B.D. Ripley, the K-function measures the expected number of points within a given distance of an arbitrary point, assuming homogeneous intensity (a constant probability of a point occurring in a particular place).\nTo describe it simply: imagine you have a map of diseased trees in a forest, and you select a tree at random. The K-function helps you answer the question: “How many other diseased trees do I expect to find within a certain distance from the diseased tree I’ve chosen?”\nThe K-function is often used to identify and analyze patterns within spatial data, such as clustering, randomness, or regularity (dispersion). It is particularly useful because it looks at the distribution at all scales (distances) simultaneously. To interpret the results of Ripley’s K-function:\n\nRandom distribution: If the points (like trees in our example) are randomly distributed, the plot of the K-function will be a straight line at a 45-degree angle.\nClustered distribution: If the points are clustered (grouped closer together than you’d expect by chance), the plot will be above the 45-degree line of the random expectation.\nRegular or dispersed distribution: If the points are regularly spaced or dispersed (further apart than you’d expect by chance), the plot will be below the 45-degree line.\n\nRipley’s K checks the density of diseased units in each area by the variance as a function of radial distances (r) from the diseased unit, hence K(r). If the spatial localization of a diseased unit is independent, the process is random in space.\nLet’s use the Kest function of the spatstat package to obtain K(r).\n\nk_rand &lt;- Kest(ppp_rand)\nplot(k_rand)\n\n\n\n\n\n\n\nk_aggr &lt;- Kest(ppp_aggr)\nplot(k_aggr)\n\n\n\n\n\n\n\n\nThe envelope function performs simulations and computes envelopes of a summary statistic based on the simulations. The envelope can be used to assess the goodness-of-fit of a point process model to point pattern data (Baddeley et al. 2014). Let’s simulate the envelope and plot the values using ggplot. Because observed K(r) (solid line) lied outside the simulation envelope, aggregation was detected.\n\nke &lt;- envelope(ppp_aggr, fun = Kest)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\ndata.frame(ke) |&gt; \n  ggplot(aes(r, theo))+\n  geom_line(linetype =2)+\n  geom_line(aes(r, obs))+\n  geom_ribbon(aes(ymin = lo, ymax = hi),\n              fill = \"steelblue\", alpha = 0.5)+\n  labs(y = \"K(r)\", x = \"r\")+\n  theme_r4pde(font_size = 16)\n\n\n\n\n\n\n\n\nmad.test performs the ‘global’ or ‘Maximum Absolute Deviation’ test described by Ripley (1977, 1981). See (Baddeley et al. 2014). This performs hypothesis tests for goodness-of-fit of a point pattern data set to a point process model, based on Monte Carlo simulation from the model.\n\n# Maximum absolute deviation test\nmad.test(ppp_aggr, Kest)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n    Maximum absolute deviation test of CSR\n    Monte Carlo test based on 99 simulations\n    Summary function: K(r)\n    Reference function: theoretical\n    Alternative: two.sided\n    Interval of distance values: [0, 49.477517555]\n    Test statistic: Maximum absolute deviation\n    Deviation = observed minus theoretical\n\ndata:  ppp_aggr\nmad = 2234.7, rank = 1, p-value = 0.01\n\nmad.test(ppp_rand, Kest)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n    Maximum absolute deviation test of CSR\n    Monte Carlo test based on 99 simulations\n    Summary function: K(r)\n    Reference function: theoretical\n    Alternative: two.sided\n    Interval of distance values: [0, 44.352774865]\n    Test statistic: Maximum absolute deviation\n    Deviation = observed minus theoretical\n\ndata:  ppp_rand\nmad = 106.7, rank = 87, p-value = 0.87\n\n\n\n\n16.1.2.3.2 O-ring statistics\nAnother statistics that can be used is the O-ring statitics which are used in spatial analysis to quantify and test the degree of interaction between two types of spatial points (Wiegand and A. Moloney 2004). The name derives from the method of placing a series of concentric circles (O-rings) around each point of type 1 and counting how many points of type 2 fall within each ring. The plot generated by O-ring statistics is called an O-ring plot or an O-function plot. It plots the radius of the rings on the x-axis and the estimated intensity of points of type 2 around points of type 1 on the y-axis.\nInterpreting the plot is as follows:\n\nRandom pattern: If points of type 2 are randomly distributed around points of type 1, the O-ring plot will be a flat line. This means that the intensity of points of type 2 does not change with the distance to points of type 1.\nAggregation or clustering: If points of type 2 are aggregated around points of type 1, the O-ring plot will be an upward-sloping curve. This indicates that the intensity of points of type 2 increases with proximity to points of type 1.\nDispersion: If points of type 2 are dispersed away from points of type 1, the O-ring plot will be a downward-sloping curve. This shows that the intensity of points of type 2 decreases as you get closer to points of type 1.\n\nThe O-ring plot often includes a confidence envelope. If the O-ring statistic falls within this envelope, it suggests that the observed pattern could be the result of random spatial processes. If it falls outside the envelope, it suggests that the pattern is not random. Therefore, to decide whether a pattern is aggregated or random using O-ring statistics:\n\nLook at the shape of the O-ring plot.\nCompare the O-ring statistic to the confidence envelope.\n\nAn aggregated pattern will show an increasing curve that lies outside the confidence envelope, indicating that the density of type 2 points is higher close to type 1 points. On the other hand, a random pattern will show a flat line that lies within the confidence envelope, indicating no significant difference in the density of type 2 points around type 1 points at varying distances.\nIn R, we can use the estimate_o_ring() function of the onpoint package. We will use the point pattern object ppp_fw used in the previous examples\n\nlibrary(onpoint)\nplot(estimate_o_ring(ppp_rand))\n\n\n\n\n\n\n\nplot(estimate_o_ring(ppp_aggr))\n\n\n\n\n\n\n\n\nThe function can be used in combination with spatstat’s envelope() function.\n\noring_envelope &lt;- envelope(ppp_aggr, fun = estimate_o_ring, nsim = 199, verbose = FALSE)\nplot(oring_envelope)\n\n\n\n\n\n\n\n\nTo plot simulation envelopes using quantum plots (Esser et al. 2014), just pass an envelope object as input to plot_quantums().\n\nplot_quantums(oring_envelope, ylab = \"O-ring\")+\n  theme_r4pde()\n\n\n\n\n\n\n\n\n\n\n\n\n16.1.3 Grouped data\nIf the data are intensively mapped, meaning that the spatial locations of the sampling units are known, we are not limited to analyse presence/absence (incidence) only data at the unit level. The sampling units may be quadrats where the total number of plants and the number of disease plants (or number of pathogen propagules) are known. Alternatively, it could be a continuous measure of severity. The question here, similar to the previous section, is whether a plant being diseased makes it more (or less) likely that neighboring plants will be diseased. If that is the case, diseased plants are exhibiting spatial autocorrelation. The most common methods are:\n\nAutocorrelation (known as Moran’s I)\nSemivariance\nSADIE (an alternative approach to autocorrelation.)\n\n\n16.1.3.1 Autocorrelation\nSpatial autocorrelation analysis provides a quantitative assessment of whether a large value of disease intensity in a sampling unit makes it more (positive autocorrelation) or less (negative auto- correlation) likely that neighboring sampling units tend to have a large value of disease intensity (Madden et al. 2007a).\nWe will illustrate the basic concept by reproducing the example provided in page 264 of the chapter on spatial analysis (Madden et al. 2007a), which was extracted from table 11.3 of Campbell and Madden. L. (1990). The data represent a single transect with the number of Macrophomia phaseolina propagules per 10 g air-dry soil recorded in 16 contiguous quadrats across a field.\n\nmp &lt;- data.frame(\n  i = c(1:16),\n  y = c(41, 60, 81, 22, 8, 20, 28, 2, 0, 2, 2, 8, 0, 43, 61, 50)\n)\nmp\n\n    i  y\n1   1 41\n2   2 60\n3   3 81\n4   4 22\n5   5  8\n6   6 20\n7   7 28\n8   8  2\n9   9  0\n10 10  2\n11 11  2\n12 12  8\n13 13  0\n14 14 43\n15 15 61\n16 16 50\n\n\nWe can produce a plot to visualize the number of propagules across the transect.\n\nmp |&gt;\n  ggplot(aes(i, y)) +\n  geom_col(fill = \"darkred\") +\n  theme_r4pde()+\n  labs(\n    x = \"Relative position within a transect\",\n    y = \"Number of propagules\",\n    caption = \"Source: Campbell and Madden (1990)\"\n  )\n\n\n\n\n\n\n\nFigure 16.6: Number of propagules of Macrophomina phaseolina in the soil at various positions within a transect\n\n\n\n\n\nTo calculate the autocorrelation coefficient in R, we can use the ac() function of the tseries package.\n\nlibrary(tseries)\nac_mp &lt;- acf(mp$y, lag = 5, pl = FALSE)\nac_mp\n\n\nAutocorrelations of series 'mp$y', by lag\n\n     0      1      2      3      4      5 \n 1.000  0.586  0.126 -0.033 -0.017 -0.181 \n\n\nLet’s store the results in a data frame to facilitate visualization.\n\nac_mp_dat &lt;- data.frame(index = ac_mp$lag, ac_mp$acf)\nac_mp_dat\n\n  index   ac_mp.acf\n1     0  1.00000000\n2     1  0.58579374\n3     2  0.12636306\n4     3 -0.03307249\n5     4 -0.01701392\n6     5 -0.18092810\n\n\nAnd now the plot known as autocorrelogram.\n\nac_mp_dat |&gt;\n  ggplot(aes(index, ac_mp.acf, label = round(ac_mp.acf, 3))) +\n  geom_col(fill = \"darkred\") +\n  theme_r4pde()+\n  geom_text(vjust = 0, nudge_y = 0.05) +\n  scale_x_continuous(n.breaks = 6) +\n  geom_hline(yintercept = 0) +\n  labs(x = \"Distance lag\", y = \"Autocorrelation coefficient\")\n\n\n\n\n\n\n\nFigure 16.7: Autocorrelogram for the spatial distribution of Macrophomina phaseolina in soil\n\n\n\n\n\nThe values we obtained here are not the same but quite close to the values reported in Madden et al. (2007b). For the transect data, the calculated coefficients in the book example for lags 1, 2 and 3 are 0.625, 0.144, and - 0.041. The conclusion is the same, the smaller the distance between sampling units, the stronger is the correlation between the count values.\n\n16.1.3.1.1 Moran’s I\nThe method above is usually referred to Moran’s I (Moran 1950), a widely-used statistic to measure spatial autocorrelation in spatial datasets. The presence of spatial autocorrelation implies that nearby locations exhibit more similar (positive autocorrelation) or dissimilar (negative autocorrelation) values than would be expected under random arrangements.\nLet’s use another example dataset from the book to calculate the Moran’s I in R. The data is shown in page 269 of the book. The data represent the number of diseased plants per quadrat (out of a total of 100 plants in each) in 144 quadrats. It was based on an epidemic generated using the stochastic simulator of Xu and Madden (2004). The data is stored in a CSV file.\n\nepi &lt;- read_csv(\"https://raw.githubusercontent.com/emdelponte/epidemiology-R/main/data/xu-madden-simulated.csv\")\nepi1 &lt;- epi |&gt;\n  pivot_longer(2:13,\n               names_to = \"y\",\n               values_to = \"n\") |&gt;\n  pull(n)\n\nThe {spdep} package in R provides a suite of functions for handling spatial dependence in data, and one of its main functions for assessing spatial autocorrelation is moran(). The moran() function calculates Moran’s I statistic for a given dataset. It requires two primary inputs: 1) A Numeric Vector: This vector represents the values for which spatial autocorrelation is to be measured. Typically, this could be a variable like population density, temperature, or any other spatial attribute; and 2) A Spatial Weights Matrix: This matrix represents the spatial relationships between the observations. It can be thought of as defining the “neighbors” for each observation. The {spdep} package provides functions to create various types of spatial weights, such as contiguity-based weights or distance-based weights. Let’s load the library and start with the procedures.\n\n\nset.seed(100)\nlibrary(spdep)\n\nThe cell2nb() function creates the neighbor list with 12 rows and 12 columns, which is how the 144 quadrats are arranged.\n\nnb &lt;- cell2nb(12, 12, type = \"queen\", torus = FALSE)\n\nThe nb2listw() function supplements a neighbors list with spatial weights for the chosen coding scheme. We use the default W, which is the row standardized (sums over all links to n). We then create the col.W neighbor list.\n\ncol.W &lt;- nb2listw(nb, style = \"W\")\n\nThe Moran’s I statistic is given by the moran() function\n\nmoran(x = epi1, # numeric vector\n      listw = col.W, # the nb list\n      n = 12, # number of zones\n      S0 = Szero(col.W)) # global sum of weights\n\n$I\n[1] 0.05818595\n\n$K\n[1] 2.878088\n\n\nThe $I is Moran’s I and $K output is the sample kurtosis of x, or a measure of the “tailedness” of the probability distribution of a real-valued random variable.\nThe interpretations for Moran’s I is as follows: A positive Moran’s I indicates positive spatial autocorrelation. Nearby locations tend to have similar values, while a Negative Moran’s I suggests negative spatial autocorrelation. Neighboring locations have dissimilar values. Moran’s close to zero indicates no spatial autocorrelation. The distribution appears random.\nIn the context of Moran’s I and spatial statistics, kurtosis of the data (x) can provide additional insights. For instance, if the data is leptokurtic ($K &gt; 3), it might suggest that there are some hotspots or cold spots (clusters of high or low values) in the spatial dataset. On the other hand, platykurtic ($K &lt; 3) data might indicate a more even spread without pronounced clusters. This information can be useful when interpreting the results of spatial autocorrelation tests and in understanding the underlying spatial structures.\n\n\n16.1.3.1.2 Moran’s test\nThe Moran’s I test is a cornerstone of spatial statistics, used to detect spatial autocorrelation in data. In the realm of the spdep package in R, the moran.test() function is employed to perform this test. The Moran’s test for spatial autocorrelation uses spatial weights matrix in weights list form. The key inputs are:\n\nx: This is a numeric vector containing the values we wish to test for spatial autocorrelation. It could be anything like population densities, number of diseased unites or severity, in the context of plant disease.\nlistw: This represents the spatial weights, and it’s in list form. The spatial weights matrix is essential for the Moran’s test because it defines the “relationship” between different observations. How we define these relationships can vary: it might be based on distance (e.g., closer observations have higher weights), contiguity (e.g., observations that share a border), or other criteria.\n\n\nmoran.test(x = epi1, \n           listw = col.W)\n\n\n    Moran I test under randomisation\n\ndata:  epi1  \nweights: col.W    \n\nMoran I statistic standard deviate = 15.919, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.698231416      -0.006993007       0.001962596 \n\n\nThe function will return the Moran’s I statistic value, an expected value under the null hypothesis of no spatial autocorrelation, the variance, and a p-value. The p-value can be used to determine the statistical significance of the observed Moran’s I value. The interpretation is as follows:\n\nA significant positive Moran’s I value indicates positive spatial autocorrelation, meaning that similar values cluster together in the spatial dataset.\nA significant negative Moran’s I value suggests negative spatial autocorrelation, implying that dissimilar values are adjacent to one another.\nIf Moran’s I is not statistically significant (based on the p-value), it suggests that the spatial pattern might be random, and there’s no evidence of spatial autocorrelation.\n\nIn conclusion, the moran.test() function offers a structured way to investigate spatial autocorrelation in datasets. Spatial weights play a crucial role in this analysis, representing the spatial relationships between observations.\nAs before, we can construct a correlogram using the output of the sp.correlogram() function. Note that the figure below is very similar to the one shown in Figure 91.5 in page 269 of the book chapter (Madden et al. 2007a). Let’s store the results in a data frame.\n\ncorrel_I &lt;- sp.correlogram(nb, epi1, \n                           order = 10,\n                           method = \"I\",  \n                           zero.policy = TRUE)\n\n\ndf_correl &lt;- data.frame(correl_I$res) |&gt; \n  mutate(lag = c(1:10))\n# Show the spatial autocorrelation for 10 distance lags\nround(df_correl$X1,3)\n\n [1]  0.698  0.340  0.086 -0.002 -0.009 -0.024 -0.090 -0.180 -0.217 -0.124\n\n\nThen, we can generate the plot using ggplot.\n\ndf_correl |&gt;\n  ggplot(aes(lag, X1)) +\n  geom_col(fill = \"darkred\") +\n  theme_r4pde()+\n  scale_x_continuous(n.breaks = 10) +\n  labs(x = \"Distance lag\", y = \"Spatial autocorrelation\")\n\n\n\n\n\n\n\nFigure 16.8: Autocorrelogram for the spatial distribution of simulated epidemics\n\n\n\n\n\n\n\n\n16.1.3.2 Semivariance\nSemi-variance is a key quantity in geostatistics. This differs from spatial autocorrelation because distances are usually measured in discrete spatial lags. The semi-variance can be defined as half the variance of the differences between all possible points spaced a constant distance apart.\nThe semi-variance at a distance d = 0 will be zero, because there are no differences between points that are compared to themselves. However, as points are compared to increasingly distant points, the semi-variance increases. At some distance, called the Range, the semi-variance will become approximately equal to the variance of the whole surface itself. This is the greatest distance over which the value at a point on the surface is related to the value at another point. In fact, when the distance between two sampling units is small, the sampling units are close together and, usually, variability is low. As the distance increases, so (usually) does the variability.\nResults of semi-variance analysis are normally presented as a graphical plot of semi-variance against distance, which is referred to as a semi-variogram. The main characteristics of the semi-variogram of interest are the nugget, the range and the sill, and their estimations are usually based on an appropriate (non-linear) model fitted to the data points representing the semi-variogram.\nFor the semi-variance, we will use the variog() function of the geoR package. We need the data in the long format (x, y and z). Let’s reshape the data to the long format and store it in epi2 dataframe.\n\nepi2 &lt;- epi |&gt;\n  pivot_longer(2:13,\n               names_to = \"y\",\n               values_to = \"n\") |&gt;\n  mutate(y = as.numeric(y))\n\nhead(epi2)\n\n# A tibble: 6 × 3\n      x     y     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     2\n2     1     2     2\n3     1     3     3\n4     1     4    33\n5     1     5     4\n6     1     6     0\n\n\n\nlibrary(geoR)\n# the coordinates are x and y and the data is the n\nv1 &lt;- variog(coords = epi2[,1:2], data = epi2[,3])\n\nvariog: computing omnidirectional variogram\n\n\n\nv2 &lt;- variofit(v1, ini.cov.pars = c(1200, 12), \n               cov.model = \"exponential\", \n               fix.nugget = F)\n\nvariofit: covariance model used is exponential \nvariofit: weights used: npairs \nvariofit: minimisation function used: optim \n\n# Plotting \nplot(v1, xlim = c(0,15))\nlines(v2, lty = 1, lwd = 2)\n\n\n\n\n\n\n\nFigure 16.9: Semivariance plot for the spatial distribution simulated epidemic\n\n\n\n\n\n\n\n16.1.3.3 SADIE\nSADIE (spatial analysis by distance indices) is an alternative to autocorrelation and semi-variance methods described previously, which has found use in plant pathology (Li et al. 2011; Madden et al. 2007a; Xu and Madden 2004). Similar to those methods, the spatial coordinates for the disease intensity (count of diseased individuals) or pathogen propagules values should be provided.\nSADIE quantifies spatial pattern by calculating the minimum total distance to regularity. That is, the distance that individuals must be moved from the starting point defined by the observed counts to the end point at which there is the same number of individuals in each sampling unit. Therefore, if the data are highly aggregated, the distance to regularity will be large, but if the data are close to regular to start with, the distance to regularity will be smaller.\nThe null hypothesis to test is that the observed pattern is random. SADIE calculates an index of aggregation (Ia). When this is equal to 1, the pattern is random. If this is greater than 1, the pattern is aggregated. Hypothesis testing is based on the randomization procedure. The null hypothesis of randomness, with an alternative hypothesis of aggregation.\nAn extension was made to quantify the contribution of each sampling unit count to the observed pattern. Regions with large counts are defined as patches and regions with small counts are defined as gaps. For each sampling unit, a clustering index is calculated and can be mapped.\nIn R, we can use the sadie() function of the epiphy package (Gigot 2018). The function computes the different indices and probabilities based on the distance to regularity for the observed spatial pattern and a specified number of random permutations of this pattern. To run the analysis, the dataframe should have only three columns: the first two must be the x and y coordinates and the third one the observations. Let’s continue working with the simulated epidemic dataset named epi2. We can map the original data as follows:\n\nepi2 |&gt;\n  ggplot(aes(x, y, label = n, fill = n)) +\n  geom_tile() +\n  geom_text(size = 5, color = \"white\") +\n  theme_void() +\n  coord_fixed() +\n  scale_fill_gradient(low = \"gray70\", high = \"darkred\")\n\n\n\n\n\n\n\nFigure 16.10: Spatial map for the number of diseased plants per quadrat (n = 144) in simulated epidemic\n\n\n\n\n\n\nlibrary(epiphy)\nsadie_epi2 &lt;- sadie(epi2)\n\nComputation of Perry's indices:\n\nsadie_epi2\n\nSpatial Analysis by Distance IndicEs (sadie)\n\nCall:\nsadie.data.frame(data = epi2)\n\nIa: 2.4622 (Pa = &lt; 2.22e-16)\n\n\nThe simple output shows the Ia value and associated P-value. As suggested by the low value of the P-value, the pattern is highly aggregated. The summary() function provides a more complete information such as the overall inflow and outflow measures. A data frame with the clustering index for each sampling unit is also provided using the summary() function.\n\nsummary(sadie_epi2)\n\n\nCall:\nsadie.data.frame(data = epi2)\n\nFirst 6 rows of clustering indices:\n  x y  i cost_flows      idx_P idx_LMX prob\n1 1 1  2 -11.382725 -7.2242617      NA   NA\n2 1 2  2  -9.461212 -6.2258877      NA   NA\n3 1 3  3  -7.299482 -5.3390880      NA   NA\n4 1 4 33   1.000000  0.8708407      NA   NA\n5 1 5  4  -5.830952 -3.6534511      NA   NA\n6 1 6  0  -5.301329 -2.9627172      NA   NA\n\nSummary indices:\n                      overall    inflow  outflow\nPerry's index        2.495346 -2.811023 2.393399\nLi-Madden-Xu's index       NA        NA       NA\n\nMain outputs:\nIa: 2.4622 (Pa = &lt; 2.22e-16)\n\n'Total cost': 201.6062\nNumber of permutations: 100\n\n\nThe plot() function allows to map the clustering indices and so to identify regions of patches (red, outflow) and gaps (blue, inflow).\n\nplot(sadie_epi2) \n\n\n\n\n\n\n\nFigure 16.11: Map of the SADIE clustering indices where red identifiy patches (outflow) and blue identify gaps (inflow)\n\n\n\n\n\nA isocline plot can be obtained by setting the isocline argument as TRUE.\n\nplot(sadie_epi2, isoclines = TRUE)\n\n\n\n\n\n\n\nFigure 16.12: Map of the SADIE clustering indices",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Tests for patterns</span>"
    ]
  },
  {
    "objectID": "spatial-tests.html#sparsely-sampled-data",
    "href": "spatial-tests.html#sparsely-sampled-data",
    "title": "16  Tests for patterns",
    "section": "16.2 Sparsely sampled data",
    "text": "16.2 Sparsely sampled data\nSparsely sampled data differs significantly from intensively mapped data. While the latter provides intricate details about the spatial location of each unit, sparsely sampled data lacks this granularity. This absence means that the spatial location of individual units isn’t factored into the analysis of sparsely sampled data.\nIn the realm of analyzing such data, particularly in understanding disease intensity, researchers often aim to characterize the range of variability in the average level of disease intensity for each sampling unit, as indicated by (Madden et al. 2007a). The methodology to analyze sparsely sampled data, especially in relation to the spatial patterns seen in plant disease epidemics, can be broadly categorized into two primary approaches:\n\nGoodness of Fit to Statistical Distributions: This approach focuses on testing how well the observed data matches expected statistical probability distributions. By doing so, researchers can assess the likelihood that the observed data is a good fit for a particular statistical model or distribution.\nIndices of Aggregation Calculation: This involves computing various metrics that can help determine the degree to which data points, or disease incidents in this case, tend to cluster or aggregate in certain areas or patterns.\n\nFurthermore, the choice of analysis method often hinges on the nature of the data in question. For instance, data can either be in the form of raw counts or as incidences (proportions). Depending on this distinction, specific statistical distributions are presumed to best represent and describe the data. Subsequent sections will delve deeper into these methods, distinguishing between count and incidence data to provide clarity on the best analytical approach for each type.\n\n16.2.1 Count data\n\n16.2.1.1 Fit to distributions\nTwo statistical distributions can be adopted as reference for the description of random or aggregated patterns of disease data in the form of counts of infection within sampling units. Take the count of lesions on a leaf, or the count of diseased plants on a quadrat, as an example. If the presence of a lesion/diseased plant does not increase or decrease the chance that other lesions/diseased plants will occur, the Poisson distribution describes the distribution of lesions on the leaf. Otherwise, the negative binomial provides a better description.\nLet’s work with the previous simulation data of 144 quadrats with a variable count of diseased plants per quadrat (in a maximum of 100). Notice that we won’t consider the location of each quadrat as in the previous analyses of intensively mapped data. We only need the vector with the number of infected units per sampling unit.\nThe epiphy package provides a function called fit_two_distr(), which allows fitting these two distribution for count data. In this case, either randomness assumption (Poisson distributions) or aggregation assumption (negative binomial) are made, and then, a goodness-of-fit comparison of both distributions is performed using a log-likelihood ratio test. The function requires a data frame created using the count() function where the number of infection units is designated as i. It won’t work with a single vector of numbers. We create the data frame using:\n\ndata_count &lt;- epi2 |&gt; \n  mutate(i = n) |&gt;  # create i vector\n  epiphy::count()   # create the map object of count class\n\nWe can now run the function that will look fo the the vector i. The function returns a list of four components including the outputs of the fitting process for both distribution and the result of the log-likelihood ratio test, the llr.\n\nfit_data_count &lt;- fit_two_distr(data_count)\nsummary(fit_data_count)\n\nFitting of two distributions by maximum likelihood\nfor 'count' data.\nParameter estimates:\n\n(1) Poisson (random):\n       Estimate  Std.Err Z value    Pr(&gt;z)    \nlambda 27.85417  0.43981  63.333 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(2) Negative binomial (aggregated):\n       Estimate    Std.Err Z value    Pr(&gt;z)    \nk     0.6327452  0.0707846  8.9390 &lt; 2.2e-16 ***\nmu   27.8541667  2.9510202  9.4388 &lt; 2.2e-16 ***\nprob  0.0222118  0.0033463  6.6378 3.184e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nfit_data_count$llr\n\nLikelihood ratio test\n\n               LogLik Df  Chisq Pr(&gt;Chisq)    \nrandom :     -2654.71                         \naggregated :  -616.51  1 4076.4  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe very low value of the P-value of the LLR test suggest that the negative binomial provides a better fit to the data. The plot() function allows for visualizing the expected random and aggregated frequencies together with the observed frequencies. The number of breaks can be adjusted as indicated.\n\nplot(fit_data_count, breaks = 5) \n\n\n\n\n\n\n\nFigure 16.13: Frequencies of the observed and expected aggregated and random distributions\n\n\n\n\n\nSee below another way to plot by extracting the frequency data (and pivoting from wide to long format) from the generated list and using ggplot. Clearly, the negative binomial is a better description for the observed count data.\n\ndf &lt;- fit_data_count$freq |&gt;\n  pivot_longer(2:4, names_to = \"pattern\", values_to = \"value\")\n\ndf |&gt;\n  ggplot(aes(category, value, fill = pattern)) +\n  geom_col(position = \"dodge\", width = 2) +\n  scale_fill_manual(values = c(\"gray70\", \"darkred\", \"steelblue\")) +\n  theme_r4pde()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nFigure 16.14: Frequencies of the observed and expected aggregated and random distributions\n\n\n\n\n\n\n\n16.2.1.2 Aggregation indices\n\nidx &lt;- agg_index(data_count, method = \"fisher\")\nidx\n\nFisher's index of dispersion:\n(Version for count data)\n34.25\n\nchisq.test(idx)\n\n\n    Chi-squared test for (N - 1)*index following a chi-squared distribution\n    (df = N - 1)\n\ndata:  idx\nX-squared = 4897.2, df = 143, p-value &lt; 2.2e-16\n\nz.test(idx)\n\n\n    One-sample z-test\n\ndata:  idx\nz = 82.085, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n# Lloyd index\n\nidx_lloyd &lt;- agg_index(data_count, method = \"lloyd\")\nidx_lloyd\n\nLloyd's index of patchiness:\n2.194\n\nidx_mori &lt;- agg_index(data_count, method = \"morisita\")\nidx_mori\n\nMorisita's coefficient of dispersion:\n(Version for count data)\n2.186\n\n# Using the vegan package\nlibrary(vegan)\nz &lt;- data_count$data$i\nmor &lt;- dispindmorisita(z)\nmor\n\n      imor     mclu      muni      imst pchisq\n1 2.185591 1.008728 0.9922162 0.5041152      0\n\n\n\n\n16.2.1.3 Power law\n[forthcoming]\n\n\n\n16.2.2 Incidence data\n\n16.2.2.1 Fit to distributions\n\ntas &lt;-\n  read.csv(\n    \"https://www.apsnet.org/edcenter/disimpactmngmnt/topc/EcologyAndEpidemiologyInR/SpatialAnalysis/Documents/tasmania_test_1.txt\",\n    sep = \"\"\n  )\nhead(tas,10)\n\n   quad group_size count\n1     1          6     4\n2     2          6     6\n3     3          6     6\n4     4          6     6\n5     5          6     6\n6     6          6     6\n7     7          6     6\n8     8          6     6\n9     9          6     4\n10   10          6     6\n\n# Create incidence object for epiphy\ndat_tas &lt;- tas |&gt;\n  mutate(n = group_size, i = count) |&gt;\n  epiphy::incidence()\n\n## Fit to two distributions\nfit_tas &lt;- fit_two_distr(dat_tas)\nsummary(fit_tas)\n\nFitting of two distributions by maximum likelihood\nfor 'incidence' data.\nParameter estimates:\n\n(1) Binomial (random):\n     Estimate Std.Err Z value    Pr(&gt;z)    \nprob  0.90860 0.01494  60.819 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(2) Beta-binomial (aggregated):\n      Estimate  Std.Err Z value    Pr(&gt;z)    \nalpha 1.923479 0.869621  2.2119  0.026976 *  \nbeta  0.181337 0.075641  2.3973  0.016514 *  \nprob  0.913847 0.023139 39.4943 &lt; 2.2e-16 ***\nrho   0.322080 0.096414  3.3406  0.000836 ***\ntheta 0.475101 0.209789  2.2647  0.023534 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfit_tas$llr\n\nLikelihood ratio test\n\n              LogLik Df  Chisq Pr(&gt;Chisq)    \nrandom :     -75.061                         \naggregated : -57.430  1 35.263   2.88e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(fit_tas)\n\n\n\n\n\n\n\n\n\n\n16.2.2.2 Aggregation indices\nglm model\n\nbinom.tas = glm(cbind(count, group_size - count) ~ 1,\n                family = binomial,\n                data = tas)\nsummary(binom.tas)\n\n\nCall:\nglm(formula = cbind(count, group_size - count) ~ 1, family = binomial, \n    data = tas)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.2967     0.1799   12.77   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 117.76  on 61  degrees of freedom\nResidual deviance: 117.76  on 61  degrees of freedom\nAIC: 152.12\n\nNumber of Fisher Scoring iterations: 5\n\nlibrary(performance)\ncheck_overdispersion(binom.tas)\n\n# Overdispersion test\n\n dispersion ratio =   2.412\n          p-value = &lt; 0.001\n\n\nepiphy(c-alpha test)\n\nlibrary(epiphy)\ntas2 &lt;- tas |&gt;\n  mutate(i = count,\n         n = group_size) |&gt;  # create i vector\n  epiphy::incidence()\n\nt &lt;- agg_index(tas2, flavor = \"incidence\")\nt\n\nFisher's index of dispersion:\n(Version for incidence data)\n2.348\n\n\n\ncalpha.test(t)\n\n\n    C(alpha) test\n\ndata:  t\nz = 7.9886, p-value = 1.365e-15\n\n\n\n\n16.2.2.3 Binary power law\nThe Binary Power Law (BPL), introduced by (Hughes and Madden 1992), is a fundamental concept in plant pathology for describing the relationship between observed variance and the variance expected under a binomial distribution. The BPL is particularly useful for understanding spatial heterogeneity and overdispersion in plant disease incidence data (Madden et al. 2007a). The year 2017 marked the 25th anniversary of the introduction of the Binary Power Law by Hughes and Madden - over the past quarter-century, the BPL has been widely adopted and extensively cited in the field of plant pathology (Madden et al. 2018). The BPL provides a robust framework for analyzing and interpreting spatial patterns in plant pathology data. By comparing observed variance to binomial variance, researchers can infer the degree and nature of overdispersion in plant disease incidence.\nThe BPL relates two variances:\n\nObserved Variance (V): \\(V = \\text{var}[X] = E[(X - np)^2]\\)\nExpected Binomial Variance (Vbin): \\(V_{bin} = np(1 - p)\\)\n\nFor proportions:\n\nObserved Variance (v): \\(v = \\text{var}[x] = E[(x - p)^2]\\)\nExpected Binomial Variance (v_bin): \\(v_{bin} = \\frac{p(1 - p)}{n}\\)\n\nThe binomial distribution assumes independent observations of \\(X\\), often interpreted as a random spatial pattern. If \\(X\\) follows a binomial distribution, then \\(\\beta = 1\\) in the BPL equations. Deviations from this indicate spatial heterogeneity or overdispersion. The log-log transformationis often used in statistical analyses due to the linear relationship between the log-transformed variables. Interpretation is as follows: when \\(\\beta = 1\\): Spatial heterogeneity is constant, indicating fixed overdispersion; when \\(\\beta &gt; 1\\): Overdispersion varies systematically with incidence, showing a change in spatial heterogeneity with disease incidence levels. In practice, researchers fit the log-log model to empirical data to estimate \\(\\beta\\) and \\(A_p\\). This involves ordinary least squares (OLS) regression:\n\\[\n\\ln(v_j) = \\ln(A_p) + \\beta \\ln\\left[\\frac{\\hat{p}_j(1 - \\hat{p}_j)}{n_j}\\right] + \\epsilon_j\n\\]\nwhere: - \\(j\\) refers to specific observations, - \\(\\hat{p}\\) is the estimated proportion, - \\(\\epsilon_j\\) is the residual error.\nThe intercept of the fitted model (\\(\\ln(A_p)\\)) provides the scale parameter, and the slope (\\(\\beta\\)) gives insight into the nature of spatial heterogeneity and overdispersion in the data.\n\n\n\n\nBaddeley, A., Diggle, P. J., Hardegen, A., Lawrence, T., Milne, R. K., and Nair, G. 2014. On tests of spatial pattern based on simulation envelopes. Ecological Monographs 84:477–489. https://doi.org/10.1890/13-2042.1.\n\n\nBaddeley, A., Turner, R., Moller, J., and Hazelton, M. 2005. Residual analysis for spatial point processes (with discussion). Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67:617–666. https://doi.org/10.1111/j.1467-9868.2005.00519.x.\n\n\nCafé-Filho, A. C., Santos, G. R., and Laranjeira, F. F. 2010. Temporal and spatial dynamics of watermelon gummy stem blight epidemics. European Journal of Plant Pathology 128:473–482. https://doi.org/10.1007/s10658-010-9674-1.\n\n\nCampbell, C. L., and Madden. L., V. 1990. Introduction to plant disease epidemiology. Wiley.\n\n\nEsser, D. S., Leveau, J. H. J., Meyer, K. M., and Wiegand, K. 2014. Spatial scales of interactions among bacteria and between bacteria and the leaf surface. FEMS Microbiology Ecology 91. https://doi.org/10.1093/femsec/fiu034.\n\n\nGigot, C. 2018. Epiphy: Analysis of plant disease epidemics.\n\n\nHughes, G., and Madden, L. V. 1992. Aggregation and incidence of disease. Plant Pathology 41:657–660. https://doi.org/10.1111/j.1365-3059.1992.tb02549.x.\n\n\nJesus Junior, W. C. de, and Bassanezi, R. B. 2004. Análise da dinâmica e estrutura de focos da morte súbita dos citros. Fitopatologia Brasileira 29:399–405. https://doi.org/10.1590/s0100-41582004000400007.\n\n\nLaranjeira, F. F., Bergamin Filho, A. R., and Amorim, L. I. 1998. Dinâmica e estrutura de focos da clorose variegada dos citros (CVC). Fitopatologia Brasileira 23:36–41.\n\n\nLaranjeira, F. F., Bergamin Filho, A., Amorim, L., and Gottwald, T. R. 2004. Dinâmica espacial da clorose variegada dos citros em três regiões do estado de são paulo. Fitopatologia Brasileira 29:56–65. https://doi.org/10.1590/s0100-41582004000100009.\n\n\nLi, B., Madden, L. V., and Xu, X. 2011. Spatial analysis by distance indices: an alternative local clustering index for studying spatial patterns. Methods in Ecology and Evolution 3:368–377. https://doi.org/10.1111/j.2041-210x.2011.00165.x.\n\n\nMadden, L. V. 1982. Evaluation of tests for randomness of infected plants. Phytopathology 72:195. https://doi.org/10.1094/phyto-72-195.\n\n\nMadden, L. V., Hughes, G., Moraes, W. B., Xu, X.-M., and Turechek, W. W. 2018. Twenty-Five Years of the Binary Power Law for Characterizing Heterogeneity of Disease Incidence. Phytopathology® 108:656–680. https://doi.org/10.1094/phyto-07-17-0234-rvw.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007a. Spatial aspects of epidemicsIII: Patterns of plant disease. In The American Phytopathological Society, pp. 235–278. https://doi.org/10.1094/9780890545058.009.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007b. The study of plant disease epidemics. The American Phytopathological Society. https://doi.org/10.1094/9780890545058.\n\n\nMoran, P. A. P. 1950. Notes on continuous stochastic phenomena. Biometrika 37:17. https://doi.org/10.2307/2332142.\n\n\nNelson, S. C. 1996. A simple analysis of disease foci. Phytopathology 86:432–439.\n\n\nWiegand, T., and A. Moloney, K. 2004. Rings, circles, and null-models for point pattern analysis in ecology. Oikos 104:209–229. https://doi.org/10.1111/j.0030-1299.2004.12497.x.\n\n\nXu, X.-M., and Madden, L. V. 2004. Use of SADIE statistics to study spatial dynamics of plant disease epidemics. Plant Pathology 53:38–49. https://doi.org/10.1111/j.1365-3059.2004.00949.x.",
    "crumbs": [
      "Spatial analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Tests for patterns</span>"
    ]
  },
  {
    "objectID": "yieldloss-concepts.html",
    "href": "yieldloss-concepts.html",
    "title": "17  Definitions and concepts",
    "section": "",
    "text": "17.1 Introduction\nPlant disease epidemics significantly impact agricultural production, particularly affecting crop yield - the measurable produce such as seed, fruit, leaves, roots or tubers - and quality, which includes factors such as blemishes on fruit and toxins in grain. Studying these impacts is crucial to understanding the overall repercussions of plant diseases on agriculture.\nThe yield of some crops can be severely diminished if they host a pathogen for a prolonged period of time. The plant’s physiology is dynamically and negatively affected as the crop grows, leading to an increase in biomass and advancement through phenological stages. For some diseases that primarily infect the end product, like grains, yield is directly impacted with a reduction in size and weight of the affected plant part.\nCertain plant diseases cause visual damage to the product, such as fruit or tubers, which may not result in a reduction in yield, but the presence of the symptoms can adversely affect sales due to decreased marketability. Furthermore, the presence of toxins in the product caused by some diseases can significantly downgrade its value, posing both health risks and economic losses.\nLosses due to plant diseases can be categorized as direct, affecting the farm itself, or indirect, having broader impacts on society. Direct losses on the farm due to plant diseases are primarily due to reductions in the quantity and quality of yield, as well as the costs associated with disease control. These are classified as primary losses.\nSecondary losses on the farm are indirect consequences of disease epidemics, such as the buildup of inoculum in the soil, which can lead to subsequent disease outbreaks. Other secondary impacts include the reduced efficacy of disease control methods due to the emergence of resistance to chemicals within the pathogen population over time.\nIn addition to these on-farm losses, plant diseases can have significant indirect impacts on society. These can include increased food prices due to reduced supply, loss of export markets due to trade restrictions, and environmental damage due to increased use of pesticides. Understanding the full spectrum of losses caused by plant diseases is critical for developing effective disease management strategies and policies.\nThe famous epidemics in the ancient history, such as the late blight of potatoes, serve us as a remind of worst-case scenarios of major impact of epidemics causing both direct and indirect losses. However, crop losses due to diseases occur regularly and at levels that depend on the intensity of epidemics (Madden et al. 2007). Expert opinion estimates have indicated that around 20% (on average) of the yield of major crops like wheat, rice, maize, potato and soybean is lost due to the pests and pathogens globally (Savary et al. 2019).",
    "crumbs": [
      "Epidemics and yield",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Definitions and concepts</span>"
    ]
  },
  {
    "objectID": "yieldloss-concepts.html#introduction",
    "href": "yieldloss-concepts.html#introduction",
    "title": "17  Definitions and concepts",
    "section": "",
    "text": "Figure 17.1: Tipology of losses caused by plant diseases",
    "crumbs": [
      "Epidemics and yield",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Definitions and concepts</span>"
    ]
  },
  {
    "objectID": "yieldloss-concepts.html#crop-loss-assessment",
    "href": "yieldloss-concepts.html#crop-loss-assessment",
    "title": "17  Definitions and concepts",
    "section": "17.2 Crop loss assessment",
    "text": "17.2 Crop loss assessment\nAccording to Madden et al. (2007), knowledge about the disease:yield relationship falls within crop loss assessment, a general branch of epidemiology that study the relationship between the attack by harmful organisms and the resulting yield (or yield loss) of crops. In fact, the study (analysis and modeling) of crop losses is considered central to plant pathology as no plant protection scientific reasoning could be possible without a measure of crop loss (Savary et al. 2006).\nThe concept of yield levels is important to recognize as a framework to study crop losses. There are three levels (from higher to lower) of yield: theoretical, attainable and actual.\n\nTheoretical (also known as potential) yield is determined mainly by defining factors such as the genotype of the crop grown under ideal conditions. It can be obtained in experimental plots managed with high input of fertilizers and pesticides.\nAttainable yield is obtained in commercial crops managed with a full range of modern technology to maximize yield. It considers the presence of limiting factors such as water and fertilizers.\nActual yield is generally less than or equal to attainable yield, and is obtained under the effect of reducing factors such as those caused by pest (disease, insects, weeds) injuries - defined as measurable symptom caused by a harmful organism. It is the crop yield actually harvested in a farmer’s field.\n\nYield loss (expressed in absolute or relative terms) is the difference between the attainable and the actual yield. Yield loss studies are only possible when reliable field data are collected in sufficient number to allow the development of statistical (empirical) models as well as the validation of mechanistic simulation yield loss models.\n\n\nCode\nlibrary(tidyverse)\nyl &lt;- tibble::tribble(\n  ~yield, ~value, ~class,\n  \"Theoretical\", 25,1,\n  \"Attainable\", 20,2,\n  \"Actual\", 15,3,\n  \"\", 0, 4\n)\nyl |&gt; \n  ggplot(aes(reorder(yield,-value), value, fill = class))+\n  geom_col(width = 0.5)+\n  r4pde::theme_r4pde(font_size = 14)+\n  ggthemes::scale_fill_gradient_tableau(palette = \"Green\")+\n  geom_hline(yintercept = 25, linetype = 2, color = \"gray60\")+\n  geom_hline(yintercept = 20, linetype = 2, color = \"gray60\")+\n  geom_hline(yintercept = 15, linetype = 2, color = \"gray60\")+\n  theme(legend.position = \"none\", \n        axis.text.y=element_blank(),\n        axis.ticks.x = element_blank())+\n  annotate(geom = \"text\", x = 1.8, y = 22, label =\"Defining factors \n           (genotype and environment)\")+\n  annotate(geom = \"text\", x = 2.8, y = 17.5, label =\"Limiting factors\n        (fertilizers, water)\")+\n  annotate(geom = \"text\", x = 3.8, y = 12, label =\"Reducing factors\n           (pest, weeds, diseases)\")+\n  annotate(\"segment\", x = 4, y = 20, xend = 4, yend = 15,\n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\")))+\n  annotate(geom = \"text\", x = 4.3, y = 17, label =\"Yield loss\")+\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nFigure 17.2: Yield levels",
    "crumbs": [
      "Epidemics and yield",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Definitions and concepts</span>"
    ]
  },
  {
    "objectID": "yieldloss-concepts.html#generating-diseaseyield-data",
    "href": "yieldloss-concepts.html#generating-diseaseyield-data",
    "title": "17  Definitions and concepts",
    "section": "17.3 Generating disease:yield data",
    "text": "17.3 Generating disease:yield data\nThe datasets utilized to characterize a disease-yield relationship should ideally encompass a broad spectrum of yield and disease values. There are primarily two approaches to acquiring such data:\n\nconducting experiments in controlled environments such as fields or greenhouses\nconducting surveys in commercial fields that are naturally infected.\n\nIn experimental setups, researchers rely on different treatments that are designed to result in varying disease epidemics, under the assumption that the disease has an impact on yield. These treatments often include manipulating the level of inoculum when the disease is expected to be minimal. This is achieved through inoculations with different amounts of the pathogen. Conversely, when the disease is expected to be severe, researchers might use fungicides at different rates, frequencies, or timings. An alternative strategy is to use different host genotypes, preferably isolines or near-isolines, which exhibit varying degrees of susceptibility to the disease. Another method is to manipulate the environment, for example by altering the irrigation levels.",
    "crumbs": [
      "Epidemics and yield",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Definitions and concepts</span>"
    ]
  },
  {
    "objectID": "yieldloss-concepts.html#damage-curves",
    "href": "yieldloss-concepts.html#damage-curves",
    "title": "17  Definitions and concepts",
    "section": "17.4 Damage curves",
    "text": "17.4 Damage curves\nIn any case, the relationship between a measure of yield (either absolute or relative) and the disease variable can be evaluated using scatter plots that depict a “damage curve” (Madden et al. 2007). The disease variable most commonly represents the assessment of the disease at a singular critical point. However, sometimes data obtained from multiple assessments throughout the disease epidemic is used to calculate the area under the disease progress curve, which is then used to represent the disease variable. This offers a more comprehensive view of the disease’s impact over time, and can better capture the complex relationships between disease progression and yield loss (Madden et al. 2007).\nLet’s work with actual data on the incidence of white mold disease and yield of soybean determined across different locations and years in Brazil (Lehner et al. 2016). The variation in disease and yield was obtained by applying different fungicides that varied in efficacy, thus resulting in variable final disease incidence. The data was made freely available in this repository and was included the the package that accompanies the book.\n\nlibrary(tidyverse)\nlibrary(r4pde)\nwm &lt;- WhiteMoldSoybean\n\nglimpse(wm, 60)\n\nRows: 382\nColumns: 17\n$ study           &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9…\n$ treat           &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,…\n$ season          &lt;chr&gt; \"2009/2010\", \"2009/2010\", \"2009/20…\n$ harvest_year    &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010…\n$ location        &lt;chr&gt; \"Agua Fria\", \"Agua Fria\", \"Agua Fr…\n$ state           &lt;chr&gt; \"GO\", \"GO\", \"GO\", \"GO\", \"GO\", \"GO\"…\n$ country         &lt;chr&gt; \"Brazil\", \"Brazil\", \"Brazil\", \"Bra…\n$ elevation       &lt;dbl&gt; 891, 891, 891, 891, 891, 891, 891,…\n$ region          &lt;chr&gt; \"Northern\", \"Northern\", \"Northern\"…\n$ elevation_class &lt;chr&gt; \"low\", \"low\", \"low\", \"low\", \"low\",…\n$ inc_check       &lt;dbl&gt; 37.7, 37.7, 37.7, 37.7, 37.7, 37.7…\n$ inc_class       &lt;chr&gt; \"low\", \"low\", \"low\", \"low\", \"low\",…\n$ yld_check       &lt;dbl&gt; 3729, 3729, 3729, 3729, 3729, 3729…\n$ yld_class       &lt;chr&gt; \"high\", \"high\", \"high\", \"high\", \"h…\n$ inc             &lt;dbl&gt; 37.7, 11.6, 33.5, 1.0, 5.6, 1.0, 3…\n$ scl             &lt;dbl&gt; 5092, 6154, 200, 180, 1123, 641, 1…\n$ yld             &lt;dbl&gt; 3729, 3739, 3863, 3904, 4471, 4313…\n\n\nAs seen above, the full data set has 17 variables. Let’s reduce the data set to a few variables of interest (study, inc and yld) and the trials number 1 to 4.\n\nwm2 &lt;- wm |&gt; \n  select(study, inc, yld) |&gt; \n  filter(study %in% c(1,2, 3, 4)) \n\nwm2\n\n# A tibble: 52 × 3\n   study   inc   yld\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1    76  2265\n 2     1    53  2618\n 3     1    42  2554\n 4     1    37  2632\n 5     1    29  2820\n 6     1    42  2799\n 7     1    55  2503\n 8     1    40  2967\n 9     1    26  2965\n10     1    18  3088\n# ℹ 42 more rows\n\n\nWe can now produce the damage curves for each study. As it can be seen, the relationship can be adequately described by a straight line.\n\nwm2 |&gt; \n  ggplot(aes(inc, yld, \n             group = study))+\n  geom_point(size = 2)+\n   theme_r4pde(font_size= 14)+\n  geom_smooth(method = \"lm\", se = F, color = \"black\", fullrange = T)+\n  ylim(1800, 3500)+\n  facet_wrap(~study)+\n  labs(x = \"White mold incidence (%)\",\n       y = \"Soybean yield (kg/ha)\",\n       color = \"Study\")\n\n\n\n\n\n\n\nFigure 17.3: Relationship between soybean yield and incidence of white mold in two experiments\n\n\n\n\n\nWe can plot the relationships for all studies combined, which will resemble a “spaghetti” plot after adding the individual regression lines.\n\np1 &lt;- wm |&gt; \n  ggplot(aes(inc, yld, group = study))+\n  theme_r4pde(font_size = 14)+\n  geom_point(size = 2, alpha = 0.5)+\n  ylim(0, 5000)+\n  labs(x = \"White mold incidence (%)\",\n       y = \"Soybean yield (kg/ha)\")\n\np2 &lt;- wm |&gt; \n  ggplot(aes(inc, yld, group = study))+\n  theme_r4pde(font_size = 14)+\n  geom_smooth(method = \"lm\", se = F, fullrange = T, color = \"black\")+\n  ylim(0, 5000)+\n  labs(x = \"White mold incidence (%)\",\n       y = \"Soybean yield (kg/ha)\")\n\nlibrary(patchwork)\np1 | p2\n\n\n\n\n\n\n\nFigure 17.4: Relationship between soybean yield and incidence of white mold across trials. Observed (left) and fitted regression lines (right)\n\n\n\n\n\n\n\n\n\nLehner, M. S., Pethybridge, S. J., Meyer, M. C., and Del Ponte, E. M. 2016. Meta-analytic modelling of the incidenceyield and incidencesclerotial production relationships in soybean white mould epidemics. Plant Pathology 66:460–468. https://doi.org/10.1111/ppa.12590.\n\n\nMadden, L. V., Hughes, G., and Bosch, F. van den, eds. 2007. CHAPTER 12: Epidemics and crop yield. In The American Phytopathological Society, pp. 353–388. https://doi.org/10.1094/9780890545058.012.\n\n\nSavary, S., Teng, P. S., Willocquet, L., and Nutter, F. W. 2006. Quantification and Modeling of Crop Losses: A Review of Purposes. Annual Review of Phytopathology 44:89–112. https://doi.org/10.1146/annurev.phyto.44.070505.143342.\n\n\nSavary, S., Willocquet, L., Pethybridge, S. J., Esker, P., McRoberts, N., and Nelson, A. 2019. The global burden of pathogens and pests on major food crops. Nature Ecology & Evolution 3:430–439. https://doi.org/10.1038/s41559-018-0793-y.",
    "crumbs": [
      "Epidemics and yield",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Definitions and concepts</span>"
    ]
  },
  {
    "objectID": "yieldloss-regression-models.html",
    "href": "yieldloss-regression-models.html",
    "title": "18  Statistical models",
    "section": "",
    "text": "18.1 Introduction\nWhen we aim to empirically describe the relationship between yield (either in absolute terms represented as [YLD], or in relative terms such as relative yield [RY] or relative yield loss [RL]) and disease intensity (denoted as y), the most straightforward approach is to employ a single metric of disease intensity. This could be in the form of disease incidence, severity, electronic measurements, among others. A linear model, as suggested by (Madden et al. 2007), often proves to be a suitable choice for this purpose. Such models are frequently termed as single-point or critical-point models. The nomenclature stems from the fact that the assessment of the disease occurs at one pivotal moment during the epidemic. This moment is typically chosen because it’s when the disease’s impact correlates with yield outcomes.\nA version of the equation, for the absolute quantity of yield (YLD), is written as:\n\\(YLD = 𝛽_0 - 𝛽_1y\\)\nwhere \\(𝛽_0\\) and \\(𝛽_1\\) are parameters and y is a disease measure. For this particular case, the intercept is a yield property of a given plant genoptye in a given environment when the disease of interest is absent (or the attainable yield). The (negative) slope represents the change (reduction) in yield with change (increase) in disease intensity.\nThe interpretation of the model parameters depends on the specific yield variable being related to disease. If relative yield loss is related to disease, the slope will be positive, because the loss (relative decrease in yield) will increase with the increase in disease intensity assessed at the critical time. Anyway, the differences in the intercept reflect the environmental effect on yield when disease is absent, while varying slopes reflect the environmental effect on yield response to disease (Madden et al. 2007).",
    "crumbs": [
      "Epidemics and yield",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical models</span>"
    ]
  },
  {
    "objectID": "yieldloss-regression-models.html#example-1-single-study",
    "href": "yieldloss-regression-models.html#example-1-single-study",
    "title": "18  Statistical models",
    "section": "18.2 Example 1: single study",
    "text": "18.2 Example 1: single study\nWe demonstrate the fitting of linear regression models (assuming a straight line relationship) to crop loss data on the effects of white mold on soybean yield (Lehner et al. 2016). This is the same data illustrated in the previous chapter. Let’s load the data and assign them to a data frame named as wm.\n\nlibrary(r4pde)\nwm &lt;- WhiteMoldSoybean\n\nFirst, we will work with data from a single trial (trial 1).\n\nlibrary(tidyverse)\nwm1 &lt;- wm |&gt; \n  dplyr::select(study, inc, yld) |&gt; \n  filter(study %in% c(1)) \nhead(wm1, 13)\n\n# A tibble: 13 × 3\n   study   inc   yld\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1    76  2265\n 2     1    53  2618\n 3     1    42  2554\n 4     1    37  2632\n 5     1    29  2820\n 6     1    42  2799\n 7     1    55  2503\n 8     1    40  2967\n 9     1    26  2965\n10     1    18  3088\n11     1    27  3044\n12     1    28  2925\n13     1    36  2867",
    "crumbs": [
      "Epidemics and yield",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical models</span>"
    ]
  },
  {
    "objectID": "yieldloss-regression-models.html#linear-regression",
    "href": "yieldloss-regression-models.html#linear-regression",
    "title": "18  Statistical models",
    "section": "18.3 Linear regression",
    "text": "18.3 Linear regression\nAssuming a linear relationship between yld and inc, we can employ a linear regression model for trial 1 using the lm() function.\n\nlm1 &lt;-  lm(yld ~ inc, data = wm1) \njtools::summ(lm1)\n\n\n\n\n\nObservations\n13\n\n\nDependent variable\nyld\n\n\nType\nOLS linear regression\n\n\n\n\n \n\n\n\n\nF(1,11)\n46.86\n\n\nR²\n0.81\n\n\nAdj. R²\n0.79\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n3329.14\n86.84\n38.33\n0.00\n\n\ninc\n-14.21\n2.08\n-6.85\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nIn the summary output, we can notice that the model explains a statistically significant and substantial proportion of variance. The model’s intercept, corresponding to inc = 0, is at 3329.14. The effect of inc is statistically significant and negative ( beta = -14.21). In another words, 140.21 kg is lost for each 10 percent point increase in incidence, given the attainable yield of 3,329.14 kg.",
    "crumbs": [
      "Epidemics and yield",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical models</span>"
    ]
  },
  {
    "objectID": "yieldloss-regression-models.html#damage-coefficients",
    "href": "yieldloss-regression-models.html#damage-coefficients",
    "title": "18  Statistical models",
    "section": "18.4 Damage coefficients",
    "text": "18.4 Damage coefficients\nDamage curves offer a visual representation of how plant diseases can impact a given crop in terms of yield loss. When we want to normalize these effects to better compare across different systems or conditions, it is useful to express these curves in relative terms rather than absolute ones. To achieve this, we can adjust the derived slope by dividing it by the intercept. This step essentially scales the rate of damage in relation to the baseline or the starting point (when there’s no damage). By subsequently multiplying the result by 100, we convert this value into a percentage. This percentage is termed the “relative damage coefficient”. What makes this coefficient particularly useful is its ability to standardize the measurement of damage, facilitating comparisons across diverse pathosystems.\nTwo plots can be produced, one that shows the effect of the disease on the relative yield and the other on the effect on yield loss (which in this case represents a positive slope). Both representations can be found in the literature. Let’s use the estimated coefficients and produce these two plots.\n\n# Extract the coefficients from model fit\ndc &lt;- (lm1$coefficients[2]/lm1$coefficients[1])*100\ndc \n\n      inc \n-0.426775 \n\n# Plot the relative damage curve\nx = seq(0,100,0.1)\ny = seq(0,100,0.1)\ndat &lt;- data.frame(x,y)\n\np1 &lt;- dat |&gt; \n  ggplot(aes(x,y))+\n  theme_r4pde(font_size = 14)+\n  geom_point(color = \"NA\")+  \n  scale_y_continuous(expand = c(0,0))+\n  scale_x_continuous(expand = c(0,0))+\n  geom_abline(aes(intercept = 100, slope = dc))+\n  labs(x = \"Incidence (%)\", y = \"Yield (%)\")+\n  annotate(geom = \"text\", x = 60, y = 60, label = \"DC = -0.42\")\n\np1 \n\n\n\n\n\n\n\n# Plot for the relative yield decrease\ndc2 &lt;- (-lm1$coefficients[2]/lm1$coefficients[1])*100\ndc2 \n\n     inc \n0.426775 \n\ndat &lt;- data.frame(x,y)\np2 &lt;- dat |&gt; \n  ggplot(aes(x,y))+\n  theme_r4pde(font_size = 14)+\n  geom_point(color = \"NA\")+  \n  scale_y_continuous(expand = c(0,0))+\n  scale_x_continuous(expand = c(0,0))+\n  geom_abline(aes(intercept = 0, slope = dc2))+\n  labs(x = \"Incidence (%)\", y = \"Yield loss (%)\")+\n  annotate(geom = \"text\", x = 60, y = 60, label = \"DC = 0.42\")\np2",
    "crumbs": [
      "Epidemics and yield",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical models</span>"
    ]
  },
  {
    "objectID": "yieldloss-regression-models.html#example-2-multiple-studies",
    "href": "yieldloss-regression-models.html#example-2-multiple-studies",
    "title": "18  Statistical models",
    "section": "18.5 Example 2: Multiple studies",
    "text": "18.5 Example 2: Multiple studies\n\n18.5.1 Introduction\nWhen managing data sourced from multiple studies (or experiments), a naive and straightforward approach is to pool all the data and fit a “global” linear regression. This option, while efficient, operates under the assumption that the different studies share common underlying characteristics, which might not always be the case.\nAnother simplistic methodology entails running independent linear regressions for each trial and subsequently averaging the intercepts and slopes. While this provides a glimpse into the general behavior of the data, it potentially sidesteps important variations that exist between individual trials. This variation is crucial, as different trials could have unique environments, treatments, or experimental conditions, all of which can influence results.\nNeglecting these variations can mask important nuances and lead to overgeneralized or inaccurate conclusions. In order to accommodate the inherent variability and structure present in multitrial data, researchers often turn to more refined analytical frameworks. Two such frameworks stand out in their ability to provide a nuanced view.\nThe first is a meta-analytic modelling. This approach synthesizes results across multiple studies to arrive at a more comprehensive understanding Madden and Paul (2011). In the context of linear regressions across multiple trials, the coefficients (both intercepts and slopes) from each trial can be treated as effect sizes Dalla Lana et al. (2015). The standard error of these coefficients, which reflects the precision of the estimate, can then be used to weight each coefficient, ensuring that more reliable estimates have greater influence on the overall mean. This method can also provide insights into heterogeneity across trials, and if required, moderators can be explored to account for this variability Lehner et al. (2016).\nA second approach is to fit a random coefficients (mixed effects) models. This approach allows the intercepts and slopes to vary across different trials, treating them as random effects Madden and Paul (2009). By modeling the coefficients as random effects, we’re assuming they come from a distribution, and we aim to estimate the parameters of this distribution. This structure acknowledges that variability exists between different trials and allows for the sharing of information across trials, thereby improving the estimation.\nBoth methods have their strengths and are appropriate under different circumstances. The choice largely depends on the research question, data structure, and the assumptions one is willing to make.\n\n\n18.5.2 Global regression\nLet’s begin by fitting a “global regression”, but we might want to first inspect the damage curve withe the fit of the global regression model visually.\n\nggplot(wm, aes(inc, yld))+\n      theme_r4pde(font_size = 12)+\n       geom_point(shape = 1)+\n       stat_smooth(method = lm, fullrange=TRUE, se = TRUE, col = \"black\")+\n       ylab(\"Yield (kg/ha)\")+\n       xlab(\"White mold incidence (%)\")\n\n\n\n\n\n\n\n\nA “global” regression can be performed using:\n\nlibrary(broom)\nfit_global &lt;- wm%&gt;%\n  do(tidy(lm(.$yld ~ .$inc), conf.int=TRUE))\nfit_global\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  3300.       56.5      58.5  5.14e-192   3189.    3411.  \n2 .$inc          -9.26      2.11     -4.39 1.45e-  5    -13.4     -5.12\n\n\nThe global intercept and slope were estimated as 3,299 kg/ha and -9.26 kg/p.p. (percent point), respectively.\n\n\n18.5.3 Individual regressions\nNow we can fit separate regressions for each trial.\n\nggplot(wm, aes(inc, yld, group = study))+\n      theme_r4pde(font_size = 12)+\n       geom_point(shape = 1)+\n       stat_smooth(method = lm, se = F, col = \"black\")+\n       ylab(\"Yield (kg/ha)\")+\n       xlab(\"White mold incidence (%)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n      #facet_wrap(~ study, ncol = 7, scales = \"fixed\") \n\nTo fit separate regression lines for each study and extract the coefficients, we can use the group_by() function along with do() and of the {dplyr} package and tidy() from {broom} package.\nThe data is first grouped by the study column. Then, for each study, a linear regression is fitted with yld as the response variable and inc as the predictor. The tidy function from the broom package is then used to extract the coefficients and confidence intervals for each regression line. The resulting output should give us a tidy dataframe with coefficients for each trial.\n\nfit_all &lt;- wm%&gt;%\n  group_by(study) |&gt; \n  do(broom::tidy(lm(.$yld ~ .$inc), conf.int=TRUE))\nfit_all\n\n# A tibble: 70 × 8\n# Groups:   study [35]\n   study term        estimate std.error statistic  p.value conf.low conf.high\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1     1 (Intercept)  3329.       86.8      38.3  4.60e-13   3138.    3520.  \n 2     1 .$inc         -14.2       2.08     -6.85 2.78e- 5    -18.8     -9.64\n 3     2 (Intercept)  2682.       48.6      55.2  8.55e-15   2575.    2789.  \n 4     2 .$inc          -6.93      1.49     -4.66 6.89e- 4    -10.2     -3.66\n 5     3 (Intercept)  4017.       61.6      65.2  1.37e-15   3882.    4153.  \n 6     3 .$inc         -18.6       1.71    -10.9  3.11e- 7    -22.4    -14.9 \n 7     4 (Intercept)  2814.      151.       18.6  1.15e- 9   2481.    3147.  \n 8     4 .$inc         -43.5      16.8      -2.58 2.56e- 2    -80.5     -6.38\n 9     5 (Intercept)  3317.      234.       14.2  2.07e- 8   2802.    3832.  \n10     5 .$inc         -21.2       5.69     -3.72 3.36e- 3    -33.7     -8.67\n# ℹ 60 more rows\n\n\nNow we can plot the distributions of each coefficient.\n\np3 &lt;- fit_all |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  ggplot(aes(x = estimate))+\n  geom_histogram(bins = 8, color = \"white\", fill = \"gray50\")+\n  theme_r4pde()+\n  labs(x = \"Intercept\", y = \"Frequency\")\n\np4 &lt;- fit_all |&gt; \n  filter(term == \".$inc\") |&gt; \n  ggplot(aes(x = estimate))+\n  geom_histogram(bins = 8, color = \"white\", fill = \"gray50\")+\n  theme_r4pde()+\n  labs(x = \"Slope\", y = \"Frequency\")\n\n\nlibrary(patchwork)\np3 | p4\n\n\n\n\n\n\n\n\nLet’s summarize the data on the slopes and intercept, using the summary() function.\n\nfit_all |&gt; \n  filter(term == \"(Intercept)\") |&gt;\n  ungroup() |&gt; \n  dplyr::select(estimate) |&gt; \n  summary()\n\n    estimate   \n Min.   :1760  \n 1st Qu.:2863  \n Median :3329  \n Mean   :3482  \n 3rd Qu.:4080  \n Max.   :4923  \n\nfit_all |&gt; \n  filter(term == \".$inc\") |&gt;\n  ungroup() |&gt; \n  dplyr::select(estimate) |&gt; \n  summary()\n\n    estimate      \n Min.   :-43.455  \n 1st Qu.:-27.676  \n Median :-16.926  \n Mean   :-19.529  \n 3rd Qu.:-13.054  \n Max.   :  2.712  \n\n\n\n\n18.5.4 Meta-analysis\nHere the objective is to combine the estimates from multiple studies or trials into a single overall estimate using a random-effects meta-analysis using {metafor} R package Viechtbauer (2010). The goal is to capture both the central tendency (i.e., the overall effect) and the variability (heterogeneity) among those individual estimates. Let’s first prepare data for analysis and then run separate meta-analysis for the intercepts and slopes.\n\n# data preparation\nIntercepts &lt;- fit_all |&gt; \n  filter(term == \"(Intercept)\")\n\nSlopes &lt;-  fit_all |&gt; \n  filter(term == \".$inc\") \n  \n\n# Model for the intercepts\nlibrary(metafor)\nma1 &lt;- rma(yi = estimate, sei = std.error, data = Intercepts)\nsummary(ma1)\n\n\nRandom-Effects Model (k = 35; tau^2 estimator: REML)\n\n   logLik   deviance        AIC        BIC       AICc   \n-274.9958   549.9916   553.9916   557.0444   554.3787   \n\ntau^2 (estimated amount of total heterogeneity): 607939.3750 (SE = 150941.5658)\ntau (square root of estimated tau^2 value):      779.7047\nI^2 (total heterogeneity / total variability):   98.88%\nH^2 (total variability / sampling variability):  89.38\n\nTest for Heterogeneity:\nQ(df = 34) = 3402.9633, p-val &lt; .0001\n\nModel Results:\n\n estimate        se     zval    pval      ci.lb      ci.ub      \n3479.3087  133.3611  26.0894  &lt;.0001  3217.9258  3740.6917  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Model for the slopes\nma2 &lt;- rma(yi = estimate, sei = std.error, data = Slopes)\nsummary(ma2)\n\n\nRandom-Effects Model (k = 35; tau^2 estimator: REML)\n\n   logLik   deviance        AIC        BIC       AICc   \n-127.4587   254.9174   258.9174   261.9701   259.3045   \n\ntau^2 (estimated amount of total heterogeneity): 65.0917 (SE = 22.6013)\ntau (square root of estimated tau^2 value):      8.0679\nI^2 (total heterogeneity / total variability):   82.28%\nH^2 (total variability / sampling variability):  5.64\n\nTest for Heterogeneity:\nQ(df = 34) = 151.3768, p-val &lt; .0001\n\nModel Results:\n\nestimate      se      zval    pval     ci.lb     ci.ub      \n-18.1869  1.6648  -10.9245  &lt;.0001  -21.4499  -14.9240  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the output above, we can notice that there is considerable heterogeneity in both the intercepts and slopes (P &lt; 0.01 for Q statistics), meaning that there may be moderators variables that, if added to the model, could potentially reduce the variance. The estimates for the intercept and slopes were 3479.30 kg/ha and -18.18 kg/p.p. (percentage point), respectively.\n\n\n18.5.5 Random coefficients model\nAmong several options in the R ecosystem, the {lme4} package in R offers a comprehensive suite of tools to fit linear mixed-effects models Bates et al. (2015). When analyzing data from multiple trials using this package, it allows for both intercepts and slopes to vary across the trials by treating them as random effects. By doing so, the inherent assumption is that these coefficients (intercepts and slopes) are drawn from certain distributions, and the goal becomes estimating the parameters of these distributions. This modeling technique acknowledges and captures the variability present across different trials. Importantly, by treating the coefficients as random effects, this enables the sharing of information across trials. This not only provides a more holistic understanding of the underlying data structure but also enhances the precision and robustness of the coefficient estimates.\nThe lmer() function allows to fit a mixed model with both the fixed (inc) and random effects (inc | study).\n\nlibrary(lme4)\nrc1 &lt;- lmer(yld ~ inc + (inc |study), data = wm, \n            REML = F)\nsummary(rc1)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: yld ~ inc + (inc | study)\n   Data: wm\n\n     AIC      BIC   logLik deviance df.resid \n  5319.4   5343.1  -2653.7   5307.4      376 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7078 -0.5991 -0.0295  0.5077  3.2364 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n study    (Intercept) 557573.08 746.708       \n          inc             36.85   6.071  -0.29\n Residual              37228.73 192.947       \nNumber of obs: 382, groups:  study, 35\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 3455.432    128.063   26.98\ninc          -17.236      1.451  -11.88\n\nCorrelation of Fixed Effects:\n    (Intr)\ninc -0.300\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.416806 (tol = 0.002, component 1)\n\n\n\n\n18.5.6 Conclusion\nResults from various regression methods show that the calculated damage coefficients fall within the range of -0.28 to -0.56 (see table below). This range of values represents the extent to which damage is influenced by the method in consideration. The most straightforward method, often referred to as the naive approach, produced the most conservative estimate, positioning the damage coefficient at the lower bound of the observed range. In stark contrast, computing the mean values from multiple individual regressions yielded a dc that topped the range, signifying a greater estimated impact.\n\nTable: Intercept, slope and damage coefficients for the four regression approaches to summarize the relationship between soybean yield and white mold incidence.\n\n\nModel\nintercept\nslope\ndamage coefficient\n\n\n\n\nGlobal regression\n3299.6\n-9.261\n-0.28\n\n\nMean of regressions\n3482\n-19.529\n-0.56\n\n\nmeta-analysis\n3479.3\n-18.1869\n-0.52\n\n\nmixed-models\n3455.43\n-17.236\n-0.49\n\n\n\nHowever, it’s worth noting that the coefficients derived from the more advanced techniques - meta-analysis and mixed-models - were quite congruent. Their close alignment suggests that both methodologies, while operating on different principles, capture the underlying dynamics of the data in somewhat analogous ways. A prominent advantage of employing these advanced techniques is their ability to encapsulate and quantify uncertainty. This capability is crucial in scientific analyses as it provides a clearer understanding of the confidence levels associated with the derived coefficients. By being able to measure and articulate this uncertainty, researchers can ensure their interpretations and subsequent decisions are founded on a solid empirical base.\n\n\n\n\nBates, D., Mächler, M., Bolker, B., and Walker, S. 2015. Fitting Linear Mixed-Effects Models Usinglme4. Journal of Statistical Software 67. https://doi.org/10.18637/jss.v067.i01.\n\n\nDalla Lana, F., Ziegelmann, P. K., Maia, A. de H. N., Godoy, C. V., and Del Ponte, E. M. 2015. Meta-Analysis of the Relationship Between Crop Yield and Soybean Rust Severity. Phytopathology® 105:307–315. https://doi.org/10.1094/phyto-06-14-0157-r.\n\n\nLehner, M. S., Pethybridge, S. J., Meyer, M. C., and Del Ponte, E. M. 2016. Meta-analytic modelling of the incidenceyield and incidencesclerotial production relationships in soybean white mould epidemics. Plant Pathology 66:460–468. https://doi.org/10.1111/ppa.12590.\n\n\nMadden, L. V., Hughes, G., and Bosch, F. van den, eds. 2007. CHAPTER 12: Epidemics and crop yield. In The American Phytopathological Society, pp. 353–388. https://doi.org/10.1094/9780890545058.012.\n\n\nMadden, L. V., and Paul, P. A. 2009. Assessing Heterogeneity in the Relationship Between Wheat Yield and Fusarium Head Blight Intensity Using Random-Coefficient Mixed Models. Phytopathology® 99:850–860. https://doi.org/10.1094/phyto-99-7-0850.\n\n\nMadden, L. V., and Paul, P. A. 2011. Meta-Analysis for Evidence Synthesis in Plant Pathology: An Overview. Phytopathology® 101:16–30. https://doi.org/10.1094/phyto-03-10-0069.\n\n\nViechtbauer, W. 2010. Conducting Meta-Analyses inRwith themetaforPackage. Journal of Statistical Software 36. https://doi.org/10.18637/jss.v036.i03.",
    "crumbs": [
      "Epidemics and yield",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical models</span>"
    ]
  },
  {
    "objectID": "prediction-warning-systems.html",
    "href": "prediction-warning-systems.html",
    "title": "19  Warning systems",
    "section": "",
    "text": "19.1 Introduction\nOne practical application of plant disease epidemiology is to predict disease occurrences to guide timely management interventions, reducing crop damage and rationalizing pesticide use. Since the early to mid 1900s, warning systems (synonyms: disease forecaster, predictor) have advanced considerably. More comprehensively, these systems have evolved to a Decision Support System (DSS), when they integrate expert input, models, and databases for more nuanced management recommendations, transcending simple prediction to encompass various goals within computerized frameworks. In fact, technological progress in recent decades has boosted the development and automation of DSSs, now widely available in public and private sectors, providing direct, sophisticated guidance to crop advisors and growers (Figure 19.1).\nIn its core, a DSS targeting risk prediction for plant disease management are based on a disease model, or a simplified, often mathematical, representation of a system (here the pathosystem) used for making predictions or suggesting management decisions based on the risk information they provide. The models range from basic rules (e.g. if-then) and static thresholds to sophisticated simulation models covering entire disease epidemics. Historically, these models collect weather information from on site automatic weather stations, but can be fed with data from remote sensing (e.g. satellite) or data reanalysis sources.",
    "crumbs": [
      "Disease prediction",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Warning systems</span>"
    ]
  },
  {
    "objectID": "prediction-warning-systems.html#introduction",
    "href": "prediction-warning-systems.html#introduction",
    "title": "19  Warning systems",
    "section": "",
    "text": "Figure 19.1: Core elements of a decision support system that provides risk information for plant disease management",
    "crumbs": [
      "Disease prediction",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Warning systems</span>"
    ]
  },
  {
    "objectID": "prediction-warning-systems.html#risk-assessment-and-decision-framework",
    "href": "prediction-warning-systems.html#risk-assessment-and-decision-framework",
    "title": "19  Warning systems",
    "section": "19.2 Risk assessment and decision framework",
    "text": "19.2 Risk assessment and decision framework\nThere are basically two types of decisions related to plant disease management: tactical and strategical and these can be related to the distinct time frames of information (i.e., historical data, pre-season, growing season, and future seasons). A risk assessment and decision framework, with associated terminology, can be proposed based on such relations (Figure 19.2) and its components are explained next.\n\n\n\n\n\n\nFigure 19.2: Framework and associated terminology for plant disease risk assessment\n\n\n\nHistorical data and prediction refers to those obtained from or simulated for previous years or seasons. This can include past observed weather patterns, observed or simulated disease outbreaks. The goal is to learn about the “normal” patterns associated with disease prevalence and severity. In the pre-season period, predictions are made before the actual planting or growing season starts. This could be based on early predictions of weather patterns for the season. During the growing season, real-time data is used to provide short-term predictions. This is the period when crops are in the fields and are actively monitored. Finally, for future seasons, projections (e.g. climate change scenarios) can be made for the subsequent years or decades.\nStrategical decisions are those with a long-term impact and are typically based on historical data and projections for future seasons. Examples might include choosing where to plant the crops in the next years, which cultivar to plant, and making infrastructure investments. Tactical are short-term decisions typically based on real-time data or short-term predictions, especially those relevant to the current growing season. A chief example include the timing of fungicide applications.\nAs to risk terminology, risk analysis is a comprehensive assessment of potential disease risks, considering both historical data and projections of the future. The goal is to understand and mitigate potential threats to agricultural production. Risk prediction can be broken down into a) outlook: Broad predictions or estimations about potential risks, often based on pre-season data. Example: effects that El Niño Southern Oscillation (ENSO) may have on disease patterns; b) forecasting: short-term predictions, typically for the growing season, such as critical weather events; and c) warning: Immediate alerts about imminent risks, like an upcoming risk of plant infection, crucial for tactical decisions.\nIn this chapter we will focus on warning systems that make use of disease monitoring or seasonal weather to provide risk information for tactical decisions.",
    "crumbs": [
      "Disease prediction",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Warning systems</span>"
    ]
  },
  {
    "objectID": "prediction-warning-systems.html#when-is-a-warning-system-needed",
    "href": "prediction-warning-systems.html#when-is-a-warning-system-needed",
    "title": "19  Warning systems",
    "section": "19.3 When is a warning system needed?",
    "text": "19.3 When is a warning system needed?\nThe figure and the box below provide some information on their utility. In the figure, risk analysis is defined as an approach for pathogens/diseases that are not present in a target region and for which modeling can be used for risk estimation if the disease is highly damaging where it occurs. Warning systems can be used if the disease is more erratic but still damaging when occuring. If the epidemics are too frequent and no effective and economic control is available (for example, some nematodes and virus), plant host resistance is the way to go. If there are effective and economic control measures (e.g. fungicide sprays) during the season, farmers should follows scheduled application of treatments.\n\n\n\n\n\n\nflowchart\n    Start((Start))\n    A{Disease&lt;BR&gt;present?}\n    B{Highly&lt;BR&gt;damaging?}\n    C[Risk Analysis]\n    D{Damaging?}\n    E{Frequent&lt;BR&gt;epidemics?}\n    F[Scheduled treatments]\n    G[Warning System]\n    H{Economic&lt;BR&gt;control?}\n    I[Host Resistance]\n\n    Start --&gt; A\n    A --&gt;|N| B\n    B --&gt;|N| Start\n    D --&gt;|N| Start\n    A --&gt;|Y| D\n    D --&gt;|Y| E\n    E --&gt;|Y| H\n    B --&gt;|Y| C\n    E --&gt;|N| G\n    H --&gt;|N| I\n    H --&gt;|Y| F\n    \n\n\n\n\n\nFigure 19.3: Decision chart for the need of risk analysis, warning systems, schedule treatment or host resistance in disease management\n\n\n\n\n\n\n\n\n\n\n\nWhen is a warning system useful?\n\n\n\nFor operational and economical use, warning systems must fulfill four criteria to be practical. A more comprehensive discussion on this topic is presented in (Campbell and Madden. L. 1990):\n\nThe targeted disease must be economically damaging to crop yield or quality. Minor diseases with negligible effects on yield or quality are unlikely to gain attention from growers and advisors.\nThe diseases should exhibit variability across seasons in terms of onset, epidemic growth rate, severity, or another aspect, creating uncertainty in decision-making. Diseases with predictable patterns provide minimal information and little management advantage, reducing the relevance of warning systems.\nUsers should be capable of acting on the system’s alerts, necessitating available and effective control measures and sufficient response time to prevent crop damage. Systems are unhelpful if practitioners can’t adapt their strategies promptly.\nThe system must encompass comprehensive knowledge about the disease, synthesizing accurate risk estimates. Understanding the specific interactions between host, pathogen, and environment is crucial for the system’s effectiveness and relevance.",
    "crumbs": [
      "Disease prediction",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Warning systems</span>"
    ]
  },
  {
    "objectID": "prediction-warning-systems.html#what-types-of-systems-are-there",
    "href": "prediction-warning-systems.html#what-types-of-systems-are-there",
    "title": "19  Warning systems",
    "section": "19.4 What types of systems are there?",
    "text": "19.4 What types of systems are there?\nThese systems vary significantly in structure and design, reflecting the multitude of plant diseases, objectives, available data, control strategies, developer preferences, and operational infrastructures. Usually, warning system are based on weather inputs, but they might leverage other inputs like host, pathogen, and economic factors, catering to the complexities of disease prediction.\nDisease warning systems can include static or dynamic disease thresholds, direct detection of inoculum, simple rules of thumb (e.g. if-then) based on weather, infection risk during defined periods (risk models), or complex simulation models that estimate all phases of an epidemic. Let’s see some examples of these systems together with a possible implementation in R.\n\n19.4.1 Disease thresholds\nDamaging thresholds, integral to integrated pest management in entomology, can serve as a basic disease warning system. They involve economic injury levels, denoting pest abundance that equates control costs with incurred losses, and economic or action thresholds, indicating when action is necessary to avoid reaching injury levels (Pedigo et al. 1986). These concepts, while straightforward, can be quite complex in practical scenarios.\nThough less prevalent than in arthropod management, thresholds guide actions like fungicide application in plant diseases, especially those directly impacting yield through photosynthetic area reduction (Leiminger and Hausladen 2012; Nutter et al. 1993). However, their application is challenging for rapid, recurrent diseases affecting high-value crops, requiring prompt intervention even at minimal disease levels. With potato late blight, for instance, the first fungicide application may need to be applied by the time disease severity reaches as low as 0.1% of the foliage. Hence, disease monitoring for detection and quantification is vital for this system. In reality, for some rapid spreading and highly damaging diseases one cannot wait to “see” the disease to start protecting the crops, for which yield protection is best when applications are made preventatively.\nHowever, the concept of economic damage threshold (EDT) may be used as a criteria do indicate when to start with fungicide sprays. By definition, EDT is the amount of disease intensity (e.g. severity when dealing with foliar diseases) that corresponds to an economic loss that equates the control cost to combat the disease. A formula for the EDT was proposed by Mumford and Norton (Mumford and Norton 1984) and further modified by Reis (Reis et al. 2002) for use in foliar fungal diseases, as described in Equation 3:\n\\(EDT = \\frac{F_C}{C_P . D_C} . C_e\\) ,\nwhere EDT is the disease intensity, \\(F_C\\) is the fungicide cost (USD/ha), \\(C_P\\) is the crop selling price (USD/ton), \\(D_C\\) is the damage coefficient (adjusted to potential yield) and \\(C_e\\) is the control efficacy of the fungicide (proportion of disease reduction relative to non-treated). In practice, sprays should be applied prior to reaching the EDT, which gives rise to the ADT (action damage threshold).\nIn a study on northern corn leaf blight in Argentina, the following values were used to calculate the EDT (De Rossi et al. 2022). Note that the authors adjusted the Dc to potential yield by multiplying by the potential yield value (8.5 ton.ha) in metric tons, since the Dc is normalized to one metric ton. More about Dc in the dedicated chapter on yield loss. The action damage threshold (ADT) was defined in that study as 20% reduction of the EDT.\n\ncalculate_EDT &lt;- function(Fc, Cp, Dc, Ec) {\n  EDT &lt;- (Fc / (Cp * Dc)) * Ce\n  return(EDT)\n}\nFc &lt;- 30      # fixed cost of control is 30 USD/ha.\nCp &lt;- 112     # fixed crop price is 112 USD/ton.\nDc &lt;- 0.1712  # for potential yield of 8.5 t/ha so 8.5 x 0.02015 = 0.1712.\nCe &lt;- 0.70    # control efficacy of fungicide is 70%.\nEDT_value &lt;- calculate_EDT(Fc, Cp, Dc, Ce)\nprint(EDT_value)\n\n[1] 1.09521\n\nADT = EDT_value * 0.80\nADT\n\n[1] 0.8761682\n\n\nAs we can see, the EDT is variable depending on these four parameters. But how does the EDT varies according to variations on each of the parameters individually? Let’s perform some simulations with vary values of the parameter.\n\nlibrary(tidyverse)\nlibrary(r4pde)\n\n\n# vary Crop price\nCp_values = seq(100, 500, by = 5)\nEDT_values &lt;- vector(\"numeric\", length(Cp_values))  \nfor(i in seq_along(Cp_values)) {\n  Cp = Cp_values[i]\n  EDT_values[i] &lt;- calculate_EDT(Fc, Cp, Dc, Ce)\n}\np_cp &lt;- data.frame(Cp_values, EDT_values) |&gt; \n  ggplot(aes(Cp_values, EDT_values))+\n  geom_line()+\n  labs(x = \"Crop price (US$/ton)\",\n       y = \"EDT\")+\n  theme_r4pde(font_size =14)\n\n# Vary fungicide price\nFc_values = seq(10, 100, by = 5)\nEDT_values &lt;- vector(\"numeric\", length(Fc_values))  \nfor(i in seq_along(Fc_values)) {\n  Fc = Fc_values[i]\n  EDT_values[i] &lt;- calculate_EDT(Fc, Cp, Dc, Ce)\n}\np_fc &lt;- data.frame(Fc_values, EDT_values) |&gt; \n  ggplot(aes(Fc_values, EDT_values))+\n  geom_line()+\n   labs(x = \"Control cost (US$/ha)\",\n       y = \"EDT\")+\n  theme_r4pde(font_size =14)\n\n# Vary damage coefficient\nDc_values = seq(0.001, 0.01, by = 0.0001)\nEDT_values &lt;- vector(\"numeric\", length(Dc_values))  \nfor(i in seq_along(Dc_values)) {\n  Dc = Dc_values[i]\n  EDT_values[i] &lt;- calculate_EDT(Fc, Cp, Dc, Ce)\n}\np_dc &lt;- data.frame(Dc_values, EDT_values) |&gt; \n  ggplot(aes(Dc_values, EDT_values))+\n  geom_line()+\n   labs(x = \"Damage coefficient\",\n       y = \"EDT\")+\n  theme_r4pde(font_size =14)\n\n# Vary Control efficacy\nCe_values = seq(0.2, 1, by = 0.01)\nEDT_values &lt;- vector(\"numeric\", length(Ce_values))  \nfor(i in seq_along(Ce_values)) {\n  Ce = Ce_values[i]\n  EDT_values[i] &lt;- calculate_EDT(Fc, Cp, Dc, Ce)\n}\np_ce &lt;- data.frame(Ce_values, EDT_values) |&gt; \n  ggplot(aes(Ce_values, EDT_values))+\n  geom_line()+\n   labs(x = \"Control efficacy (%)\",\n       y = \"EDT\")+\n  theme_r4pde(font_size =14)\n\n\nlibrary(patchwork)\n(p_cp | p_fc) /\n  (p_dc | p_ce)\n\n\n\n\n\n\n\nFigure 19.4: Variation of the economic damage threshold (EDT) according to fluctuations on each parameter in the formula for its calculation\n\n\n\n\n\n\n19.4.1.1 Bonus app: EDT calculator\nThe scenario for EDT calculation is intricate, influenced by fluctuations in several parameters. To streamline these computations, a Shiny application called EDT Calculator was developed, enabling users to specify each parameter and assess EDT (and respective 95% CI) under uncertainties in variables such as crop price and fungicide costs.\n\n\n\n\n\n\nFigure 19.5: Screenshot of the EDT calculator, a Shiny app for interactive calculation of economic damage threshold (EDT)\n\n\n\n\n\n\n19.4.2 Monitoring season-long weather\nDisease warning systems frequently predict conditions conducive to infection of the plant by the pathogen, with wetness and temperature being key variables for many foliar diseases (Bourke 1970). BLITECAST, the first computerized system (Krause et al. 1975) (which combined the Wallin and Hyre systems) provided the means of performing necessary calculations accurately and quickly and issuing recommendations to growers, is an example of successful automated warning system (Krause and Massie 1975). While initial inoculum is often undetectable, the presence of inoculum of the pathogen is assumed in many weather-based warning systems. Predictions of an outbreak are possible by tracking environmental conditions favorable for disease development.\nWeather-based disease warning systems, like FAST (Forecasting Alternaria solani on tomatoes) and Wallin (Madden 1978; Wallin 1962), continuously monitor moisture and temperature for various crop diseases. These systems calculate weather favorability from environmental data, predicting infection and disease severity. They serve to guide growers on optimal spraying schedules, initiating treatments or determining application intervals based on accumulated severity values over time.\n\n19.4.2.1 Wallin model\nJ.R. Wallin developed a model in the mid-20th century focusing on forecasting potato late blight, detailed across several publications (Wallin 1962). The model tracks hourly relative humidity and temperature, emphasizing periods with relative humidity of 90% or more. It calculates the number of high-humidity hours and the corresponding average temperature during the wet period. By accumulating ‘disease severity values’ (DSV) from plant emergence throughout the season, based on humidity and temperature measures, the model predicts the initial onset and subsequent spread of potato late blight. The table below summarizes the way the DSVs are obtained based on combinations of hours of relative humdity &gt; 90% and the air temperature within the wet period.\nTable. Relationship of temperature and relative humidity (RH) periods as used in the Wallin late blight forecasting system to predict disease severity values (0 to 4).\n\n\n\n\nDaily\nseverity\nvalue\n\n\n\n\nAverage Temperature Range\n0\n1\n2\n3\n4\n\n\n7.2 - 11.6 C\n15\n16-18\n19-21\n22-24\n&gt;25\n\n\n11.7 - 15.0 C\n12\n13-15\n16-18\n19-21\n&gt;22\n\n\n15.1 - 26.6 C\n9\n10-12\n13-15\n16-18\n&gt;19\n\n\n\nLet’s calculate DVS in R based on Wallin’s system. But first we need to download hourly weather data from NASA Power project using {nasapower} R package for the locality of Viçosa, MG, Brazil during the month of March 2022.\n\nlibrary(nasapower)\nweather &lt;- get_power(\n      community = \"ag\",\n      lonlat = c(-42.88, -20.7561),\n      pars = c(\"RH2M\", \"T2M\"),\n      dates = c(\"2022-03-02\", \"2022-03-31\"),\n      temporal_api = \"hourly\"\n    )\nhead(weather)\n\nNASA/POWER CERES/MERRA2 Native Resolution Hourly Data  \n Dates (month/day/year): 03/02/2022 through 03/31/2022  \n Location: Latitude  -20.7561   Longitude -42.88  \n Elevation from MERRA-2: Average for 0.5 x 0.625 degree lat/lon region = 665.27 meters \n The value for missing source data that cannot be computed or is outside of the sources availability range: NA  \n Parameter(s):  \n \n Parameters: \n RH2M     MERRA-2 Relative Humidity at 2 Meters (%) ;\n T2M      MERRA-2 Temperature at 2 Meters (C)  \n \n# A tibble: 6 × 8\n    LON   LAT  YEAR    MO    DY    HR  RH2M   T2M\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -42.9 -20.8  2022     3     2     0  96.3  19.4\n2 -42.9 -20.8  2022     3     2     1  97.2  19.0\n3 -42.9 -20.8  2022     3     2     2  98.1  18.8\n4 -42.9 -20.8  2022     3     2     3  99.1  18.6\n5 -42.9 -20.8  2022     3     2     4  98.9  18.6\n6 -42.9 -20.8  2022     3     2     5  98.6  18.6\n\n\nWe now need to obtain the wet period (let’s call it leaf wetness, or LW) based on hours of relative humidity &gt;90% and then the average temperature during the LW period for each day. We can obtain these by grouping the variables by year, month and day and use mutate() and summarise().\n\nlibrary(tidyverse)\nweather2 &lt;- weather |&gt; \n      group_by(YEAR, MO, DY) |&gt; \n      mutate(LW = case_when(RH2M &gt; 90 ~ 1,\n                            TRUE ~ 0)) |&gt; \n      filter(LW &gt; 0) |&gt;\n      summarise(Air_LWD = mean(T2M, na.rm = TRUE),\n                LWD = n())\n\nNow we are ready to calculate the daily DSV based on Wallin’s rules on the table and inspect the first 6 rows of the new table called df_wallin.\n\ndf_wallin &lt;- weather2 |&gt; \n      mutate(\n        DSV = case_when(\n          # Temperature Range: 7.2 - 11.6 C\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &lt;= 15 ~ 0,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 15 & LWD &lt;= 18 ~ 1,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 18 & LWD &lt;= 21 ~ 2,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 21 & LWD &lt;= 24 ~ 3,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 24 ~ 4,\n          \n          # Temperature Range: 11.7 - 15.0 C\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &lt;= 12 ~ 0,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 12 & LWD &lt;= 15 ~ 1,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 15 & LWD &lt;= 18 ~ 2,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 18 & LWD &lt;= 21 ~ 3,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 21 ~ 4,\n          \n          # Temperature Range: 15.1 - 26.6 C\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &lt;= 9 ~ 0,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 9 & LWD &lt;= 12 ~ 1,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 12 & LWD &lt;= 15 ~ 2,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 15 & LWD &lt;= 18 ~ 3,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 18 ~ 4,\n          \n          # Default (For temperatures out of the specified ranges or any other scenarios)\n          TRUE ~ 0  # Assigning a default value of 0\n        )\n      )\n    head(df_wallin)\n\n# A tibble: 6 × 6\n# Groups:   YEAR, MO [1]\n   YEAR    MO    DY Air_LWD   LWD   DSV\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1  2022     3     2    19.2    10     1\n2  2022     3     3    19.7    12     1\n3  2022     3     4    19.9    10     1\n4  2022     3     5    19.5     7     0\n5  2022     3     6    19.0     9     0\n6  2022     3     7    18.4     7     0\n\n\nWe can visualize the daily and cumulative DSV for the monthly period after transforming the date format using as.Date() function. The dashed horizontal line in the plot indicates the action threshold of 20 cumulative DSV points, or when a spray should be applied. Please note that in real systems, the DSV is reduced to zero and another DSV counting is initiated after the spray.\n\ndf_wallin2 &lt;- df_wallin |&gt; \n  mutate(DSV2 = cumsum(DSV),\n  date = as.Date(sprintf('%04d-%02d-%02d', YEAR, MO, DY)))\n\ndf_wallin2 |&gt; \n  ggplot(aes(date, DSV))+\n  geom_col(fill = \"darkred\")+\n  geom_line(aes(date, DSV2))+\n  geom_hline(yintercept = 20, linetype = 2)+\n  annotate(geom = \"text\", x = as.Date(\"2022-03-04\"), y = 20.5, label = \"Action threshold\")+\n  r4pde::theme_r4pde()+\n  labs(x = \"Date\", y = \"Daily and cumulative DSV\")\n\n\n\n\n\n\n\n\n\n\n19.4.2.2 FAST model\nHere is a code to calculate daily DSV values based on the FAST table (Madden 1978) that relates the hours of relative humidity &gt; 90% (wet period) and temperature during the wet period during a 24-hour period.\n\n\n\nMean temp (°C)\n0\n1\n2\n3\n4\n\n\n\n\n13-17\n0-6\n7-15\n16-20\n21+\n23+\n\n\n18-20\n0-3\n4-8\n9-15\n16-22\n23+\n\n\n21-25\n0-2\n3-5\n6-12\n13-20\n21+\n\n\n26-29\n0-3\n4-8\n9-15\n16-22\n23+\n\n\n\n\n df_fast &lt;- weather2 %&gt;% \n      mutate(\n        DSV = case_when(\n          # Temperature Range: 13 &lt;= T &lt; 18\n          Air_LWD &gt;= 13 & Air_LWD &lt; 18 & LWD &gt;= 0 & LWD &lt;= 6 ~ 0,\n          Air_LWD &gt;= 13 & Air_LWD &lt; 18 & LWD &gt;= 7 & LWD &lt;= 15 ~ 1,\n          Air_LWD &gt;= 13 & Air_LWD &lt; 18 & LWD &gt;= 16 & LWD &lt;= 20 ~ 2,\n          Air_LWD &gt;= 13 & Air_LWD &lt; 18 & LWD &gt; 20 ~ 3,\n          \n          # Temperature Range: 18 &lt;= T &lt; 21\n          Air_LWD &gt;= 18 & Air_LWD &lt; 21 & LWD &gt;= 0 & LWD &lt;= 3 ~ 0,\n          Air_LWD &gt;= 18 & Air_LWD &lt; 21 & LWD &gt;= 4 & LWD &lt;= 8 ~ 1,\n          Air_LWD &gt;= 18 & Air_LWD &lt; 21 & LWD &gt;= 9 & LWD &lt;= 15 ~ 2,\n          Air_LWD &gt;= 18 & Air_LWD &lt; 21 & LWD &gt;= 16 & LWD &lt;= 22 ~ 3,\n          Air_LWD &gt;= 18 & Air_LWD &lt; 21 & LWD &gt; 22 ~ 4,\n          \n          # Temperature Range: 21 &lt;= T &lt; 26\n          Air_LWD &gt;= 21 & Air_LWD &lt; 26 & LWD &gt;= 0 & LWD &lt;= 2 ~ 0,\n          Air_LWD &gt;= 21 & Air_LWD &lt; 26 & LWD &gt;= 3 & LWD &lt;= 5 ~ 1,\n          Air_LWD &gt;= 21 & Air_LWD &lt; 26 & LWD &gt;= 6 & LWD &lt;= 12 ~ 2,\n          Air_LWD &gt;= 21 & Air_LWD &lt; 26 & LWD &gt;= 13 & LWD &lt;= 20 ~ 3,\n          Air_LWD &gt;= 21 & Air_LWD &lt; 26 & LWD &gt; 20 ~ 4,\n          \n          # Temperature Range: 26 &lt;= T &lt; 30\n          Air_LWD &gt;= 26 & Air_LWD &lt; 30 & LWD &gt;= 0 & LWD &lt;= 3 ~ 0,\n          Air_LWD &gt;= 26 & Air_LWD &lt; 30 & LWD &gt;= 4 & LWD &lt;= 8 ~ 1,\n          Air_LWD &gt;= 26 & Air_LWD &lt; 30 & LWD &gt;= 9 & LWD &lt;= 15 ~ 2,\n          Air_LWD &gt;= 26 & Air_LWD &lt; 30 & LWD &gt;= 16 & LWD &lt;= 22 ~ 3,\n          Air_LWD &gt;= 26 & Air_LWD &lt; 30 & LWD &gt; 22 ~ 4,\n          \n          # Default (For temperatures out of the specified ranges or any other scenarios)\n          TRUE ~ 0  # Assigning a default value of 0\n        )\n      )\ndf_fast\n\n# A tibble: 30 × 6\n# Groups:   YEAR, MO [1]\n    YEAR    MO    DY Air_LWD   LWD   DSV\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1  2022     3     2    19.2    10     2\n 2  2022     3     3    19.7    12     2\n 3  2022     3     4    19.9    10     2\n 4  2022     3     5    19.5     7     1\n 5  2022     3     6    19.0     9     2\n 6  2022     3     7    18.4     7     1\n 7  2022     3     8    19.2    10     2\n 8  2022     3     9    19.6    12     2\n 9  2022     3    10    19.6    13     2\n10  2022     3    11    19.7    12     2\n# ℹ 20 more rows\n\ndf_fast2 &lt;- df_fast |&gt; \n  mutate(DSV2 = cumsum(DSV),\n  date = as.Date(sprintf('%04d-%02d-%02d', YEAR, MO, DY)))\n\ndf_fast2 |&gt; \n  ggplot(aes(date, DSV))+\n  geom_col(fill = \"darkred\")+\n  geom_line(aes(date, DSV2))+\n  geom_hline(yintercept = 20, linetype = 2)+\n  annotate(geom = \"text\", x = as.Date(\"2022-03-04\"), y = 21.5, label = \"Action threshold\")+\n  r4pde::theme_r4pde()+\n  labs(x = \"Date\", y = \"Daily and cumulative DSV\")\n\n\n\n\n\n\n\n\n\n\n19.4.2.3 Bonus app: Dashboard\nFor educational purposes, an interactive dashboard was developed using R Shiny. This app demonstrates the application of the Wallin (forecast for late blight of potato) and FAST (Forecast for Alternaria solani on Tomatoes) rules for calculating DSV, determining the appropriate timing for fungicide sprays (based on a defined threshold), and counting the total sprays during a selected period (Figure 19.6).\nTo utilize the system, users should select the model, input the latitude and longitude (or choose a location from the map), and specify the time period for the simulation, such as from plant emergence to harvest. The weather data for this simulation is sourced from the NASA Power project via the {nasapower} R package.\n\n\n\n\n\n\nFigure 19.6: Screenshot of a web-based warning system for plant diseases based on Wallin's and FAST rules\n\n\n\n\n\n\n19.4.3 Infection risk during defined periods\nMany diseases like late blight of potatoes and early blight of tomatoes, as seen in the previous section, require continuous (season long) risk monitoring to indicate multiple fungicide sprays. However, some warning systems assess risk at only at one time points, such as those crops that are most vulnerable to diseases during specific growth phases. In these cases, a single warning is issued, such as the risk of disease occurrence or occurrence at a intensity above a threshold (e.g. incidence &gt; 30%).\nSome of these systems use statistical (risk algorithms) to predict probabilities of occurrence of the event (yes/no situation) based on weather variables that occur over specific periods of time, usually when primary infections take place. For example, Fusarium head blight (FHB, caused by Fusarium graminearum) disease depends on weather events that occur around the flowering stage, when the crop is most vulnerable. This is also the case of Sclerotinia (white mold) diseases that affect flowers and the presence/absence of apothecia is key for risk assessment(Willbur et al. 2018). Such events of binary nature can be predicted using risk algorithms that consider weather variables, cultivar susceptibility, soil moisture, etc. A commonly used algorithm is the logistic regression model which deals with a binary classification.\n\n19.4.3.1 Logistic regression: Ohio FHB models\nFHB risk models were developed in the United States in the early 2000s to predict the risk of an epidemic, or when disease severity in the field was greater than 10% (De Wolf et al. 2003). In the paper, the epidemic cases were classified as 0 and 1, according to that threshold, and several logistic regression models were fitted to the data using a summary of weather-related variables from two defined periods: seven days prior to flowering date and 10 days following flowering date (50% of wheat heads with anthers).\nAmong several models, the authors found that a single variable model (model A) exhibited a good accuracy (0.83). The selected variable consisted of a combination of temperature and humidity for the 10-day period after flowering. Named TRH9010, it corresponds to the total number of hours, within the 10-day post-flowering period, when the temperature (T) was &gt;=15 and &lt;= 30 oC and relative humidity (RH) was &gt;90%.\nLet’s implement this model in R and calculate the probability of infection. As usual, we first need to download hourly data from NASA Power project for a period of three years. Let’s work with data from Wooster, OH, United States.\n\nlibrary(nasapower)\nweather &lt;- get_power(\n      community = \"ag\",\n      lonlat = c(-81.9399, 40.7982),\n      pars = c(\"RH2M\", \"T2M\", \"PRECTOTCORR\"),\n      dates = c(\"2020-01-01\", \"2022-12-31\"),\n      temporal_api = \"hourly\"\n    )\nhead(weather)\n\nNASA/POWER CERES/MERRA2 Native Resolution Hourly Data  \n Dates (month/day/year): 01/01/2020 through 12/31/2022  \n Location: Latitude  40.7982   Longitude -81.9399  \n Elevation from MERRA-2: Average for 0.5 x 0.625 degree lat/lon region = 309.08 meters \n The value for missing source data that cannot be computed or is outside of the sources availability range: NA  \n Parameter(s):  \n \n Parameters: \n RH2M            MERRA-2 Relative Humidity at 2 Meters (%) ;\n T2M             MERRA-2 Temperature at 2 Meters (C) ;\n PRECTOTCORR     MERRA-2 Precipitation Corrected (mm/hour)  \n \n# A tibble: 6 × 9\n    LON   LAT  YEAR    MO    DY    HR  RH2M   T2M PRECTOTCORR\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 -81.9  40.8  2020     1     1     0  86.4 -3.55        0.01\n2 -81.9  40.8  2020     1     1     1  88.8 -3.92        0.01\n3 -81.9  40.8  2020     1     1     2  88.3 -3.74        0.01\n4 -81.9  40.8  2020     1     1     3  85.3 -3.49        0   \n5 -81.9  40.8  2020     1     1     4  80.2 -3.18        0   \n6 -81.9  40.8  2020     1     1     5  75.6 -3.06        0   \n\n\nWe can then create a function named calculate_TRH9010(). This function calculates the number of hours when specified condition is met (i.e., hours when 15 &lt;= T &lt;= 30 & RH &gt; 90), the probability (p) of infection, and the classification of the epidemic (epi) as 0 or 1 based on a defined threshold for p (0.36 in this case) (De Wolf et al. 2003). The function’s arguments are the weather data frame, YEAR, MO, and DY, allowing us to estimate the risk for a specific day (the flowering date) within the downloaded period. It’s important to note that, in the original model, the variables were normalized (its value divided by the maximum observed value). Thus, the hour count for TRH9010 should be divided by the constant 136.\n\ncalculate_TRH9010 &lt;- function(data, YEAR, MO, DY){\n  start_row &lt;- which(data$YEAR == YEAR & data$MO == MO & data$DY == DY & data$HR == 0)[1] \n  if ((nrow(data) - start_row) &lt; 9 * 24){ \n    return(list(TRH9010 = NA, y = NA, p = NA, epi = NA))\n  }\n  subset_data &lt;- data[start_row:(start_row + 9 * 24 - 1),] \n  condition_met &lt;- with(subset_data, T2M &gt;= 15 & T2M &lt;= 30 & RH2M &gt; 90)\n  total_hours &lt;- sum(condition_met)\n  y &lt;- -3.3756 + 6.8128 * (total_hours / 136) # divide by maximum of 136 as in the paper\n  p &lt;- exp(y) / (1 + exp(y))\n  epi &lt;- ifelse(p &gt; 0.36, 1, 0)\n  return(list(TRH9010 = total_hours, p = p, epi = epi))\n}\n\nNow we apply the function for the 1st of June 2021 as flowering date. The output is the hour count, the probability and the epidemic classification.\n\nresults &lt;- calculate_TRH9010(weather, YEAR = 2021, MO = 6, DY = 1)\nresults\n\n$TRH9010\n[1] 62\n\n$p\n[1] 0.4329649\n\n$epi\n[1] 1\n\n\nWe may want to apply the function for a series of days within a given month. Let’s write code to predict the risk for the 15 days of June using a for() loop which will store the results in a list, but we can further transform to a dataframe using map_dfr() function.\n\nnum_days &lt;- 15\nall_results &lt;- list()\nfor(day in 1:num_days){\n  all_results[[paste0(\"\", day)]] &lt;- calculate_TRH9010(weather, YEAR = 2021, MO = 6, DY = day)\n}\n\n# transform results to a dataframe\ndf &lt;- all_results %&gt;%\n  map_dfr(~ as.data.frame(t(.)), .id = \"day\")\n\n# Change variables  to numeric \ndf$p &lt;- as.numeric(df$p)\ndf$day &lt;- as.numeric(df$day)\ndf$epi &lt;- as.numeric(df$epi)\n\nFinally we plot the results.\n\nggplot(df, aes(day, p, fill = factor(epi))) + \n  geom_col()+\n  scale_x_continuous(n.breaks = 15)+\n  scale_fill_manual(values = c(\"grey60\", \"darkred\"))+\n  geom_hline(yintercept = 0.36, linetype = 2)+\n  r4pde::theme_r4pde(font_size = 12)+\n  ylim(0,1)+\n  theme(legend.position = \"bottom\")+\n  labs(x = \"Wheat flowering day in june 2021\", y = \"Risk probability\",\n       fill = \"Epidemic classification\",\n       title = \"FHB risk by Ohio (TRH9010) model\",\n       subtitle = \"Location: Wooster, Ohio\",\n       caption = \"Model source: De Wolf et al. (2002)\")\n\n\n\n\n\n\n\n\n\n\n19.4.3.2 Mechanistic models: Brazilian FHB model\nDifferent from the previous approach, where statistical models are fitted to data, some models are called mechanistic, dynamic or process-based. These are designed based on the knowledge of the processes of the disease cycle and the factors that affect these processes (González-Domínguez et al. 2023). These usually operate in a hourly to daily basis and can be quite complex and sophisticated with numerous examples being found in the literature (Caffi et al. 2007, 2010; Rossi et al. 2014; Salotti et al. 2022; Salotti and Rossi 2023).\nOne example is a mechanistic model for FHB developed in Brazil. In its core, the model is based on the daily simulation of the flowering process, the most vulnerable phase of the wheat crop, and events that contribute to infect the flowers. The original model (Del Ponte et al. 2005) integrates several sub models that estimate the daily proportion of flowers, a density of inoculum in a “spore cloud” and a proportion of infected flowers - all these variables being driven by daily variables of temperature and wetness variables such as rainfall and relative humidity.\nA modified, simplified version of the model that takes into account only the flowering process and weather-variables that drives infection, was written in R and is presented here for illustration.\n\n\n\n\n\n\nFigure 19.7: A simplified version of a mechanistic Fusarium head blight model that takes into account the amount of flowers on each day which will be infected depending on rules that define a wet period and temperature during the wet period.\n\n\n\nThe function simulate_FHB() requires a starting date and weather data frame (adapted here to use NASA Power data set) to run a simulation during 20 days.\nAmong the several simplifications, which were based on the simulated results of the original model, flowering dynamics was assumed to follow a normal distribution over 10 days, and is adjusted to ensure that the peak in the proportion of anthers is set to 0.8. An adjustment is made so that anthers are set to 0 after the 10th day.\n\\([ \\text{antherproportions} = \\left\\{ \\begin{array}{ll} \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right) & \\text{for } x = 1, \\ldots, 10 \\\\ 0 & \\text{for } x = 11, \\ldots, 20 \\end{array} \\right. ]\\) ,\nwhere \\(𝞵\\) = 6 and \\(𝝈\\) = 2.\nThe probability of the infection of the flowers is calculated as in the paper. It is based on an equation that considers daily temperature (T) and gives the proportion of flowers getting infected on any given day when a condition for infection is met.\n\\(INF=0.001029×exp(0.1957×T)\\)\nThere are three conditions for an infection event, similar to what is described in the article (Del Ponte et al. 2005):\n\nContinuous 2-day Rain & High Humidity: There should be rain (&gt; 0.5 mm) and the average humidity over two consecutive days should be high (&gt; 80%).\nRain Followed by Dry Day & High Humidity: Rain and high humidity (&gt; 80%), followed by a day without rain but still with high humidity (&gt; 85%).\nDry Day & High Humidity Followed by Rain: A day without rain but with high humidity (&gt; 85%), followed by a day with both rain and high humidity (&gt; 80%).\n\nInfection then occurs when any of the above conditions are met. In this simpler version (without the inoculum factor), the proportion of infected spikelets, assuming that infected anthers will give rise to infected spikelets, is given by\n\\(InfectedSpikelets = AntherProportions×INF\\)\n\nsimulate_FHB &lt;- function(start_date, weather_data) {\n  start_date &lt;- as.Date(start_date, format = \"%Y-%m-%d\")\n  \n  # Extract weather data for 30 days starting from the provided start_date\n  weather_subset &lt;- subset(weather_data, YYYYMMDD &gt;= start_date & YYYYMMDD &lt; start_date + 20)\n  \n  # Simulation for 20 days\n  x &lt;- 1:20\n  \n  # Normal distribution formula for 10 days using fixed peak day (mu) and spread (sigma)\n  mu &lt;- 6\n  sigma &lt;- 2\nanther_proportions &lt;- c((1 / (sigma * sqrt(2 * pi))) * exp(-0.5 * ((x[1:10] - mu) / sigma)^2), rep(0, 10))\n\n  \n  # Adjust the values to have a peak of 0.8\n max_value &lt;- max(anther_proportions)\nscaling_factor &lt;- 0.8 / max_value\nanther_proportions &lt;- anther_proportions * scaling_factor # This scales the entire vector\n\n  # Zero out the anthers after the day they should be zero\n  anther_proportions[11:20] &lt;- 0\n  \n  # Calculate the INF value\n  INF &lt;- 0.001029 * exp(0.1957 * weather_subset$T2M)\n  \n  # Calculate average RH for two consecutive days\n  avg_RH2M_2days &lt;- (weather_subset$RH2M + c(weather_subset$RH2M[-1], NA)) / 2\n  \n  # Check conditions for infection\n  condition1 &lt;- (weather_subset$PRECTOTCORR &gt; 0.3) & (avg_RH2M_2days &gt; 80)\n  consecutive_rain_high_humidity &lt;- condition1 & c(FALSE, head(condition1, -1))\n  preceded_condition &lt;- (weather_subset$PRECTOTCORR &gt; 0.3) & (avg_RH2M_2days &gt; 80) &\n    c(FALSE, head(weather_subset$RH2M, -1) &gt; 85) &\n    c(head(weather_subset$PRECTOTCORR, -1) &lt;= 0.3, FALSE)\n  succeeded_condition &lt;- (weather_subset$PRECTOTCORR &gt; 0.3) & (avg_RH2M_2days &gt; 80) &\n    c(tail(weather_subset$RH2M, -1) &gt; 85, FALSE) &\n    c(FALSE, tail(weather_subset$PRECTOTCORR, -1) &lt;= 0.3)\n  \n  # Combine conditions\n  infection_trigger &lt;- consecutive_rain_high_humidity == 1 | preceded_condition | succeeded_condition\n  \n  # Calculate infected anthers based on conditions\n  infected_spikelets &lt;- ifelse(infection_trigger, anther_proportions * INF, 0)\n  \n  # Cumulative infected anthers\n  cumulative_infected_spikelets &lt;- cumsum(infected_spikelets)\n  \n  # Compile results\n  result_df &lt;- data.frame(\n    Date = weather_subset$YYYYMMDD,\n    T2M = weather_subset$T2M,\n    RAIN = weather_subset$PRECTOTCORR,\n    RH2M = weather_subset$RH2M,\n    Flowers = round(anther_proportions,3),\n    InfSpikelets = round(infected_spikelets,3),\n    CumInfSpikelets = round(cumulative_infected_spikelets, 3)\n  )\n  return(result_df)\n}\n\nLet’s download the data again for the location of Wooster, Ohio during a 9-year period.\n\nlibrary(nasapower)\nwooster &lt;- get_power(\n  community = \"ag\",\n  lonlat = c(-81.9399, 40.7982),\n  pars = c(\"RH2M\", \"T2M\", \"PRECTOTCORR\"),\n  dates = c(\"2015-01-01\", \"2023-09-30\"),\n  temporal_api = \"daily\"\n)\nwooster\n\nNASA/POWER CERES/MERRA2 Native Resolution Daily Data  \n Dates (month/day/year): 01/01/2015 through 09/30/2023  \n Location: Latitude  40.7982   Longitude -81.9399  \n Elevation from MERRA-2: Average for 0.5 x 0.625 degree lat/lon region = 309.08 meters \n The value for missing source data that cannot be computed or is outside of the sources availability range: NA  \n Parameter(s):  \n \n Parameters: \n RH2M            MERRA-2 Relative Humidity at 2 Meters (%) ;\n T2M             MERRA-2 Temperature at 2 Meters (C) ;\n PRECTOTCORR     MERRA-2 Precipitation Corrected (mm/day)  \n \n# A tibble: 3,195 × 10\n     LON   LAT  YEAR    MM    DD   DOY YYYYMMDD    RH2M    T2M PRECTOTCORR\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n 1 -81.9  40.8  2015     1     1     1 2015-01-01  67.4  -5.04        0   \n 2 -81.9  40.8  2015     1     2     2 2015-01-02  79.8  -2.1         0   \n 3 -81.9  40.8  2015     1     3     3 2015-01-03  94     1.96       25.7 \n 4 -81.9  40.8  2015     1     4     4 2015-01-04  88.9   3.89        8.65\n 5 -81.9  40.8  2015     1     5     5 2015-01-05  63.1  -9.22        0.54\n 6 -81.9  40.8  2015     1     6     6 2015-01-06  76.6  -9.92        2.3 \n 7 -81.9  40.8  2015     1     7     7 2015-01-07  70.9 -12.3         0.58\n 8 -81.9  40.8  2015     1     8     8 2015-01-08  72.2 -13.7         0.54\n 9 -81.9  40.8  2015     1     9     9 2015-01-09  75.6 -10.8         1.41\n10 -81.9  40.8  2015     1    10    10 2015-01-10  78.6 -14.2         0.02\n# ℹ 3,185 more rows\n\n\nNow that we have the data, we can simulate FHB for a specific start flowering date.\n\nstart_date &lt;- \"2018-05-15\"\nresult &lt;- simulate_FHB(start_date, wooster)\nhead(result, 15)\n\n         Date   T2M  RAIN  RH2M Flowers InfSpikelets CumInfSpikelets\n1  2018-05-15 19.08  4.11 88.19   0.035        0.000           0.000\n2  2018-05-16 17.19  0.09 84.75   0.108        0.000           0.000\n3  2018-05-17 18.76  0.05 80.50   0.260        0.000           0.000\n4  2018-05-18 18.50  0.15 82.75   0.485        0.000           0.000\n5  2018-05-19 19.95  9.49 86.50   0.706        0.000           0.000\n6  2018-05-20 16.60  1.43 87.38   0.800        0.021           0.021\n7  2018-05-21 17.75 15.55 83.50   0.706        0.023           0.045\n8  2018-05-22 19.94  9.70 87.06   0.485        0.025           0.069\n9  2018-05-23 16.47  0.05 76.00   0.260        0.000           0.069\n10 2018-05-24 20.43  0.00 60.81   0.108        0.000           0.069\n11 2018-05-25 21.20  0.03 68.94   0.000        0.000           0.069\n12 2018-05-26 21.05  9.51 86.19   0.000        0.000           0.069\n13 2018-05-27 23.13  0.60 80.81   0.000        0.000           0.069\n14 2018-05-28 25.53  0.01 73.81   0.000        0.000           0.069\n15 2018-05-29 24.91  0.01 80.69   0.000        0.000           0.069\n\n\nFinally we can plot the dynamics of the flowering and the percent of infected spikelets during the period of 20 days after first flowers appear in the field.\n\n# Plotting the data\nlibrary(tidyverse)\np1 &lt;- result |&gt; \n  ggplot(aes(x = Date)) + \n  geom_area(aes(y = Flowers,  \n                color = \"Proportion of flowers\"),\n            linewidth = 1, fill = \"yellow\", alpha = 0.5) + \n  geom_line(aes(y = CumInfSpikelets, \n                color = \"Cum. Infected Spikelets\"), \n            linewidth =1) +\n  labs(title = \"FHB simulation model\",\n       subtitle = paste(\"% Infected spikelets = \", \n                        round(max(result$CumInfSpikelets*100, na.rm = TRUE),2), \"%\"),\n       y = \"Proportion\", x = \"Date\", color = \"Legend\") +\n  r4pde::theme_r4pde(font_size = 12)+\n  theme(legend.position = \"bottom\")\n\n\np2 &lt;- result |&gt; \n  ggplot(aes(x = Date))+\n  geom_col(aes(x = Date, y = RAIN))+\n  geom_line(aes(y = T2M))+\n  geom_line(aes(y = RH2M), linetype = 2)+\n  labs(y = \"Value\", \n       title = \"Rain, T and RH\")+\n  r4pde::theme_r4pde(font_size = 12)\n\n\nlibrary(patchwork)\np1 / p2\n\n\n\n\n\n\n\n\nAs in the previous section, we can apply the function for a series of days and see how the percent of infected spikelets fluctuates over time considering each day as a starting day for flowering.\n\n# Vector of dates\ndates_vector &lt;- seq(as.Date(\"2019-05-15\"), as.Date(\"2019-05-30\"), by=\"days\")\n\n# Apply function and extract max cumulative infection\nmax_cumulative_infections &lt;- sapply(dates_vector, function(date) {\n  result &lt;- simulate_FHB(date, wooster)\n  max(result$CumInfSpikelets, na.rm = TRUE)\n})\n\ndf_results &lt;- data.frame(\n  Date = dates_vector,\n  MaxCumulativeInfection = max_cumulative_infections\n)\n\n# plot the results\ndf_results |&gt; \n  ggplot(aes(Date, MaxCumulativeInfection*100))+\n  geom_col(fill = \"darkred\", alpha = 0.8)+\n  labs(title = \"FHB simulation model\", \n       subtitle = \"Wooster, Ohio - 2019\",\n         x = \"Starting day of flowering\", y = \"% Infected Spikelets\")+\n  \n  r4pde::theme_r4pde(font_size = 14)",
    "crumbs": [
      "Disease prediction",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Warning systems</span>"
    ]
  },
  {
    "objectID": "prediction-warning-systems.html#practical-considerations",
    "href": "prediction-warning-systems.html#practical-considerations",
    "title": "19  Warning systems",
    "section": "19.5 Practical considerations",
    "text": "19.5 Practical considerations\nWhile initial disease warning systems faced underuse, this was attributed to their primary deployment by advisors rather than growers directly, and an evolution where users, through experience, develop simplified rules that diminish the need for constant system consultation. Notable examples, such as the CPO in Denmark and FARMSCAPE in Australia, have documented this learning process, leading to less reliance on such tools. Growers often favor ‘good enough’ solutions that evolve from system use, which may reduce the need for direct interaction with the system over time.\nHowever, certain diseases with specific control timing requirements, regulatory pressures, or those affecting high-value crops necessitate direct engagement with warning systems. In these scenarios, growers prioritize yield consistency and economic security over reduced pesticide usage, especially when faced with asymmetric risks associated with incorrect management decisions. It has been suggested that disease warning systems might be more beneficial and accepted for crops of intermediate or lower value, where the cost of mismanagement is more balanced. Essential to the success and adoption of these systems is the active involvement of end-users throughout the development process to ensure the system’s features align with their practical farming goals and constraints.\n\n\n\n\nBourke, P. M. A. 1970. Use of Weather Information in the Prediction of Plant Disease Epiphytotics. Annual Review of Phytopathology 8:345–370. https://doi.org/10.1146/annurev.py.08.090170.002021.\n\n\nCaffi, T., Rossi, V., Cossu, A., and Fronteddu, F. 2007. Empirical vs. mechanistic models for primary infections of  Plasmopara viticola*. EPPO Bulletin 37:261–271. https://doi.org/10.1111/j.1365-2338.2007.01120.x.\n\n\nCaffi, T., Rossi, V., Legler, S. E., and Bugiani, R. 2010. A mechanistic model simulating ascosporic infections by Erysiphe necator, the powdery mildew fungus of grapevine. Plant Pathology 60:522–531. https://doi.org/10.1111/j.1365-3059.2010.02395.x.\n\n\nCampbell, C. L., and Madden. L., V. 1990. Introduction to plant disease epidemiology. Wiley.\n\n\nDe Rossi, R. L., Guerra, F. A., Plazas, M. C., Vuletic, E. E., Brücher, E., Guerra, G. D., and Reis, E. M. 2022. Crop damage, economic losses, and the economic damage threshold for northern corn leaf blight. Crop Protection 154:105901. https://doi.org/10.1016/j.cropro.2021.105901.\n\n\nDe Wolf, E. D., Madden, L. V., and Lipps, P. E. 2003. Risk Assessment Models for Wheat Fusarium Head Blight Epidemics Based on Within-Season Weather Data. Phytopathology® 93:428–435. https://doi.org/10.1094/phyto.2003.93.4.428.\n\n\nDel Ponte, E. M., Fernandes, J. M. C., and Pavan, W. 2005. A risk infection simulation model for fusarium head blight of wheat. Fitopatologia Brasileira 30:634–642. https://doi.org/10.1590/s0100-41582005000600011.\n\n\nGonzález-Domínguez, E., Caffi, T., Rossi, V., Salotti, I., and Fedele, G. 2023. Plant Disease Models and Forecasting: Changes in Principles and Applications over the Last 50 Years. Phytopathology® 113:678–693. https://doi.org/10.1094/phyto-10-22-0362-kd.\n\n\nKrause, R. A., and Massie, L. B. 1975. Predictive Systems: Modern Approaches to Disease Control. Annual Review of Phytopathology 13:31–47. https://doi.org/10.1146/annurev.py.13.090175.000335.\n\n\nKrause, R. A., Massie, L. B., and Hyre, R. A. 1975. BLITECAST: A computerized forecast of potato late blight. The Plant Disease Reporter 59:95.\n\n\nLeiminger, J. H., and Hausladen, H. 2012. Early Blight Control in Potato Using Disease-Orientated Threshold Values. Plant Disease 96:124–130. https://doi.org/10.1094/pdis-05-11-0431.\n\n\nMadden, L. 1978. FAST, a forecast system for alternaria solani on tomato. Phytopathology 68:1354. https://doi.org/10.1094/phyto-68-1354.\n\n\nMumford, J. D., and Norton, G. A. 1984. Economics of Decision Making in Pest Management. Annual Review of Entomology 29:157–174. https://doi.org/10.1146/annurev.en.29.010184.001105.\n\n\nNutter, F., Teng, P., and Royer, M. 1993. Terms and concepts for yield, crop loss, and disease thresholds. Plant Disease 77:193–211.\n\n\nPedigo, L. P., Hutchins, S. H., and Higley, L. G. 1986. Economic Injury Levels in Theory and Practice. Annual Review of Entomology 31:341–368. https://doi.org/10.1146/annurev.en.31.010186.002013.\n\n\nReis, E. M., Hoffmann, L. L., and Blum, M. 2002. Modelo de ponto crítico para estimar os danos causados pelo oídio em cevada. Fitopatologia Brasileira 27:644–646.\n\n\nRossi, V., Onesti, G., Legler, S. E., and Caffi, T. 2014. Use of systems analysis to develop plant disease models based on literature data: grape black-rot as a case-study. European Journal of Plant Pathology 141:427–444. https://doi.org/10.1007/s10658-014-0553-z.\n\n\nSalotti, I., Bove, F., and Rossi, V. 2022. Development and validation of a mechanistic, weather-based model for predicting puccinia graminis f. Sp. Tritici infections and stem rust progress in wheat. Frontiers in Plant Science 13. https://doi.org/10.3389/fpls.2022.897680.\n\n\nSalotti, I., and Rossi, V. 2023. A Mechanistic Model Accounting for the Effect of Soil Moisture, Weather, and Host Growth Stage on the Development of Sclerotinia sclerotiorum. Plant Disease 107:514–533. https://doi.org/10.1094/pdis-12-21-2743-re.\n\n\nWallin, J. R. 1962. Summary of recent progress in predicting late blight epidemics in United States and Canada. American Potato Journal 39:306–312. https://doi.org/10.1007/bf02862155.\n\n\nWillbur, J. F., Fall, M. L., Bloomingdale, C., Byrne, A. M., Chapman, S. A., Isard, S. A., Magarey, R. D., McCaghey, M. M., Mueller, B. D., Russo, J. M., Schlegel, J., Chilvers, M. I., Mueller, D. S., Kabbage, M., and Smith, D. L. 2018. Weather-Based Models for Assessing the Risk of Sclerotinia sclerotiorum Apothecial Presence in Soybean (Glycine max) Fields. Plant Disease 102:73–84. https://doi.org/10.1094/pdis-04-17-0504-re.",
    "crumbs": [
      "Disease prediction",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Warning systems</span>"
    ]
  },
  {
    "objectID": "prediction-disease-modeling.html",
    "href": "prediction-disease-modeling.html",
    "title": "20  Disease modeling",
    "section": "",
    "text": "20.1 Introduction\nAs seen in the previous chapter, plant disease modeling is a crucial tool for predicting disease dynamics and informing management decisions when integrated into decision support systems. By leveraging models, researchers and practitioners can anticipate disease outbreaks, assess potential risks, and implement timely interventions to mitigate losses (Rossi et al. 2010; Savary et al. 2018).\nMathematical modeling involves representing empirical phenomena and experimental outcomes using mathematical functions. The data used for these models may be collected specifically for modeling purposes or drawn from existing experiments and observations originally conducted to address different research questions, with such data often found in the literature (Hau and Kranz 1990).\nMathematical models integrating plant, disease, and environmental - in most cases weather-based variables - factors have been developed since the mid-1900s (See recent review by González-Domínguez et al. (2023) ). Dynamic modeling of disease epidemics gained traction in the early 1960s with foundational work by Vanderplank and Zadoks, setting the stage for future advancements. Since then, researchers have contributed extensively to model development, mainly focusing on the plant disease cycle which outline pathogen development stages, such as dormancy, reproduction, dispersal, and pathogenesis, driven by interactions among host, pathogen, and environmental factors (De Wolf and Isard 2007).\nA systematic map by Fedele et al. (2022) identified over 750 papers on plant disease models, primarily aimed at understanding system interactions (n = 680). This map revealed that while most models focus on system understanding, fewer are devoted to tactical management (n = 40), strategic planning (n = 38), or scenario analysis (n = 9).\nIn terms of model development, we can classify the models into two main groups based on the approach taken (González-Domínguez et al. 2023): Empirical or mechanistic approaches, which differ fundamentally in their basis, complexity and application (Figure 20.1).\nEmpirical models, which emerged in the mid-20th century, rely on data-driven statistical relationships between variables collected under varying field or controlled environments. These models often lack cause-effect understanding, making them less robust and requiring rigorous validation and calibration when applied in diverse environments, especially in regions that did not provide data for model construction. The parameters of the model change every time new data are incorporated during model development.\nIn contrast, mechanistic models, developed from a deep understanding of biological and epidemiological processes, explain disease dynamics based on known system behaviors in response to external variables—a concept-driven approach. These dynamic models quantitatively characterize the state of the pathosystem over time, offering generally more robust predictions by utilizing mathematical equations to describe how epidemics evolve under varying environmental conditions.\nBoth empirical and mechanistic approaches are valid methodologies extensively used in plant pathology research. The choice between these approaches depends on several factors, including data availability, urgency in model development, and, frequently, the researcher’s experience or preference. Empirical models focus on statistical relationships derived directly from data, whereas mechanistic models aim to represent the biological processes of disease progression through linked mathematical equations.\nIn mechanistic modeling, the equations used to predict specific disease components—such as infection frequency or the latency period—are often empirically derived from controlled experiments. For example, an infection frequency equation is typically based on data collected under specific environmental conditions, with models fitted to accurately describe observed patterns. These process-based models are then built by integrating empirically-derived equations or rules, which collectively simulate the disease cycle. Data and equations are sourced from published studies or generated from new experiments conducted by researchers.\nBeyond their practical predictive value, mechanistic models are valuable tools for organizing existing knowledge about a particular disease, helping to identify gaps and guide future research efforts. An example of such work is the extensive collection of comprehensive mechanistic models developed for various plant diseases by the research group led by Prof. Vittorio Rossi in Italy (Rossi et al. 2008, 2014; Salotti et al. 2022; Salotti and Rossi 2023).\nThis chapter focuses mainly on empirical modeling. We begin by examining the types of data utilized in model development, focusing on those collected under controlled conditions, such as replicated laboratory or growth chamber experiments, as well as field data collected from several locations and years. We will also analyze real-world case studies, drawing on examples from the literature to replicate and understand model applications. Through these examples, we aim to illustrate the process of fitting models to data and underscore the role of modeling in advancing plant disease management practices.",
    "crumbs": [
      "Disease prediction",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Disease modeling</span>"
    ]
  },
  {
    "objectID": "prediction-disease-modeling.html#introduction",
    "href": "prediction-disease-modeling.html#introduction",
    "title": "20  Disease modeling",
    "section": "",
    "text": "Figure 20.1: Steps of model development from data collection to modeling based on statistical relationships (data-driven) between data collected from field or controlled environment to mechanistic approach based on the elements of the disease cycles (concept-driven).",
    "crumbs": [
      "Disease prediction",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Disease modeling</span>"
    ]
  },
  {
    "objectID": "prediction-disease-modeling.html#controlled-environment",
    "href": "prediction-disease-modeling.html#controlled-environment",
    "title": "20  Disease modeling",
    "section": "20.2 Controlled environment",
    "text": "20.2 Controlled environment\nIn this section, we will demonstrate, using examples from the literature, how statistical models can be fitted to data that represent various stages of the disease cycle.\nResearch on disease-environment interactions under controlled conditions - such as laboratory or growth chamber studies - lays the groundwork for building foundational models, including infection-based models and sub-models for specific processes like dormancy, dispersal, infection, and latency (De Wolf and Isard 2007; Krause and Massie 1975; Magarey et al. 2005).\nGrowth chambers and phytotrons are essential for testing the effects of individual variables, though these controlled results may not fully replicate field conditions. Anyway, laboratory experiments help clarify specific questions by isolating interactions, unlike complex field trials where host, pathogen, and environment factors interact. Polycyclic or “mini epidemic” experiments enable observation of disease dynamics under targeted conditions (Hau and Kranz 1990; Rotem 1988).\nOnce developed, these sub-models can be incorporated into larger mechanistic models that simulate the entire disease cycle, thereby mimicking disease progression over time (Rossi et al. 2008; Salotti and Rossi 2023). Alternatively, sub-models can also be used in stand-alone predictive systems where the process being modeled - such as infection - is the key factor in determining disease occurrence (MacHardy 1989; Magarey and Sutton 2007). For example, infection sub-models can be integrated into prediction systems that help schedule crop protection measures by forecasting when infection risk is highest.\n\n20.2.1 Infection-based models\nTo model infection potential based on environmental factors, simple rules can be used with daily weather data, such as temperature and rainfall thresholds (Magarey et al. 2002). Simple decision aids, such as charts and graphs, also exist to help model infection potential by using combinations of daily average temperature and hours of wetness. These tools offer a straightforward approach to evaluate infection risks based on readily available weather data, supporting decision-making without complex modeling (Seem 1984). However, for many pathogens, hourly data is needed, requiring complex models that track favorable conditions hour by hour. These models start with specific triggers and can reset due to conditions like dryness or low humidity, simulating a biological clock for infection risk assessment (Magarey and Sutton 2007).\nModeling approaches vary based on available data and model goals. A common method is the matrix approach, like the Wallin potato late blight model, which uses rows for temperature and columns for moisture duration to estimate disease severity (Krause and Massie 1975) (see previous chapter on warning systems). Bailey enhanced this with an interactive matrix that combines temperature, relative humidity, and duration to assess infection risk across pathogens, making it versatile for various modeling needs (Bailey 1999).\nWhen infection responses are measured at various temperature and wetness combinations, regression models can be developed to predict infection rates. These models often use polynomial, logistic, or complex three-dimensional response surface equations to represent the relationship between environmental conditions and infection potential. In an excellent review title “How to create and deploy infection models for plant pathogens” Magarey and Sutton (2007) discusses that many modeling approaches lack biological foundations and are not generic, making them unsuitable for developing a unified set of disease forecast models. While three-dimensional response surfaces, such as those created with sufficient temperature-moisture observations, offer detailed infection responses, they are often too complex and data-intensive for widespread use (seeTable 1 adapted from Magarey and Sutton (2007)).\n\nComparison of different infection modeling approaches. Source: Magarey and Sutton (2007)\n\n\n\n\n\n\n\nApproach\nStrengths\nWeaknesses\n\n\n\n\nMatrix (Krause and Massie 1975; Mills 1944; Windels et al. 1998)\nEasy; converts moisture/temperature combinations into severity values or risk categories. Tried and true approach.\nData to populate matrix may not be readily available.\n\n\nRegression:– Polynomial (Evans 1992)– Logistic (Bulger 1987)\nWidely used in plant pathology. Available for many economically important pathogens.\nParameters not biologically based. Requires dataset for development.\n\n\nThree-dimensional response surface (Duthie 1997)\nDescribes infection response in detail.\nParameters not biologically based. Complex, requires extensive data and processing time.\n\n\nDegree wet hours (Pfender 2003)\nSimple; based on degree hours, commonly used in entomology. Requires only Tmin and Tmax\nRecently developed; assumes linear thermal response.\n\n\nTemperature-moisture response function (Magarey et al. 2005)\nSimple; based on crop modeling functions, requires only Tmin, Topt and Tmax\nRecently developed.\n\n\n\nIn the following sections, I will demonstrate how various biologically meaningful models fit infection data, using temperature, wetness duration, or a combination of both as predictors.\n\n20.2.1.1 Temperature effects\n\n20.2.1.1.1 Generalized beta-function\nAmong several non-linear models that can be fitted to infection responses to temperature, the generalized beta-function is an interesting alternative (Hau and Kranz 1990). This is a nonlinear model with five parameters. Two of them, namely \\(b\\) and \\(c\\) , have a biological meaning because they are estimates of the minimum and maximum temperature of the biological process under consideration.\nWe will use a subset of the data obtained from a study conducted under controlled conditions that aimed to assess the influence of temperature on the symptom development of citrus canker in sweet orange (Dalla Pria et al. 2006). The data used here is only for severity on the cultivar Hamlin (plot a in Figure 20.2). The data was extracted using the R package {digitize} as shown here on this tweet.\n\n\n\n\n\n\nFigure 20.2: Effect of temperature (12, 15, 20, 25, 30, 35 or 40°C) on disease severity of citrus canker on sweet orange cvs Hamlin (a), Natal (b), Pera (c) and Valencia (d) with a leaf wetness duration of 24 h. Each point represents the mean of three repetitions. Vertical bars represent standard errors. Lines show the generalized beta function fitted to data. Source: Dalla Pria et al. (2006)\n\n\n\nLet’s enter the data manually. Where \\(t\\) is the temperature and \\(y\\) is the severity on leaves.\n\ntemp &lt;- tibble::tribble(\n  ~t, ~y,\n12.0, 0.00,\n15.0, 0.1,\n20.0, 0.5,\n25.0, 1.2,\n30.0, 1.5,\n35.0, 1.2,\n40.0, 0.1\n)\n\nFit the generalized beta-function (Hau and Kranz 1990). The model can be written as:\n\\[\ny = a*((t - b )^d)*((c - t)^e)\n\\]\nwhere \\(b\\) and \\(c\\) represent minimum and maximum temperatures, respectively, for the development of the disease, \\(a\\), \\(d\\) and \\(e\\) are parameters to be estimated, \\(t\\) is the temperature and \\(y\\) is disease severity. We need the {minpack.lm} library to avoid parameterization issues.\n\nlibrary(tidyverse)\nlibrary(minpack.lm)\nfit_temp &lt;- nlsLM(\n  y ~ a * ((t - b) ^ d) * ((c - t) ^ e),\n  start = list(\n    a = 0,\n    b = 10,\n    c = 40,\n    d = 1.5,\n    e = 1\n  ),\n  algorithm = \"port\",\n  data = temp\n)\nsummary(fit_temp)\n\n\nFormula: y ~ a * ((t - b)^d) * ((c - t)^e)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \na  0.001303   0.006295   0.207    0.855    \nb 11.999999   4.875414   2.461    0.133    \nc 40.137236   0.346763 115.748 7.46e-05 ***\nd  1.760101   1.193017   1.475    0.278    \ne  0.830868   0.445213   1.866    0.203    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1121 on 2 degrees of freedom\n\nAlgorithm \"port\", convergence message: Relative error between `par' and the solution is at most `ptol'.\n\nmodelr::rsquare(fit_temp, temp)\n\n[1] 0.9898275\n\n\nStore the model parameters in objects.\n\nfit_temp$m$getAllPars()\n\n          a           b           c           d           e \n 0.00130259 11.99999931 40.13723602  1.76010097  0.83086798 \n\na &lt;- fit_temp$m$getAllPars()[1]\nb &lt;- fit_temp$m$getAllPars()[2]\nc &lt;- fit_temp$m$getAllPars()[3]\nd &lt;- fit_temp$m$getAllPars()[4]\ne &lt;- fit_temp$m$getAllPars()[5]\n\nCreate a data frame for predictions at each temperature unit from 10 to 45 degree Celsius.\n\nt &lt;- seq(10, 45, 0.1)\ny &lt;- a * ((t - b) ^ d) * ((c - t) ^ e)\ndat &lt;- data.frame(t, y)\n\nPlot the observed and predicted data using {ggplot2} package.\n\nlibrary(ggplot2)\nlibrary(r4pde)\ndat |&gt;\n  ggplot(aes(t, y)) +\n  geom_line() +\n  geom_point(data = temp, aes(t, y)) +\n  theme_r4pde(font_size = 16) +\n  labs(x = \"Temperature\", y = \"Severity\",\n       title = \"Generalized beta-function\")\n\n\n\n\n\n\n\n\n\n\n20.2.1.1.2 Analytis beta function\nJi et al. (2023) tested and compared various mathematical equations to describe the response of mycelial growth to temperature for several fungi associated with Grapevine trunk diseases. The authors found that the beta equation (Analytis 1977) provided the best fit and, therefore, was considered the most suitable for all fungi.\nThe model equation for re-scaled severity (0 to 1) as a function of temperature is given by:\n\\(Y = \\left( a \\cdot T_{eq}^b \\cdot (1 - T_{eq}) \\right)^c \\quad ; \\quad \\text{if } Y &gt; 1, \\text{ then } Y = 1\\)\nwhere\n\\(T_{eq} = \\frac{T - T_{\\text{min}}}{T_{\\text{max}} - T_{\\text{min}}}\\)\n\\(T\\) is the temperature in degrees Celsius. \\(T_{\\text{min}}\\) is the minimum temperature, \\(T_{\\text{max}}\\) is the maximum temperature for severity. The \\(a\\) , \\(b\\) , and \\(c\\) are parameters that define the top, symmetry, and size of the unimodal curve.\nLet’s rescale (0 to 1) the data on the citrus canker using the function rescale of the {scales} package.\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\ntemp$yscaled &lt;- rescale(temp$y)\ntemp\n\n# A tibble: 7 × 3\n      t     y yscaled\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1    12   0    0     \n2    15   0.1  0.0667\n3    20   0.5  0.333 \n4    25   1.2  0.8   \n5    30   1.5  1     \n6    35   1.2  0.8   \n7    40   0.1  0.0667\n\n\nNow we can fit the model using the same nlsLM function.\n\n# Define the minimum and maximum temperatures\nTmin &lt;- 12\nTmax &lt;- 40\n\nlibrary(minpack.lm)\nfit_temp2 &lt;- nlsLM(\n  yscaled ~ (a * ((t - Tmin) / (Tmax - Tmin))^b * (1 - ((t - Tmin) / (Tmax - Tmin))))^c,\n  data = temp,\n  start = list(a = 1, b = 2, c = 3), # Initial guesses for parameters\n  algorithm = \"port\" \n)\n\nsummary(fit_temp2)\n\n\nFormula: yscaled ~ (a * ((t - Tmin)/(Tmax - Tmin))^b * (1 - ((t - Tmin)/(Tmax - \n    Tmin))))^c\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na   6.7625     0.3218  21.013 3.03e-05 ***\nb   1.9648     0.1030  19.072 4.45e-05 ***\nc   1.1607     0.1507   7.701  0.00153 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03955 on 4 degrees of freedom\n\nAlgorithm \"port\", convergence message: Relative error in the sum of squares is at most `ftol'.\n\nmodelr::rsquare(fit_temp2, temp)\n\n[1] 0.9948325\n\n\nLets’s store the model parameters in objects.\n\nfit_temp2$m$getAllPars()\n\n       a        b        c \n6.762509 1.964817 1.160702 \n\na &lt;- fit_temp2$m$getAllPars()[1]\nb &lt;- fit_temp2$m$getAllPars()[2]\nc &lt;- fit_temp2$m$getAllPars()[3]\n\nAgain, we create a data frame for predictions at each temperature unit from 10 to 45 degree Celsius.\n\nTmin &lt;- 12\nTmax &lt;- 40\nt &lt;- seq(10, 45, 0.1)\ny &lt;- (a * ((t - Tmin) / (Tmax - Tmin))^b * (1 - ((t - Tmin) / (Tmax - Tmin))))^c\ndat2 &lt;- data.frame(t, y)\n\nAnd now we can plot the observed and predicted data using {ggplot2} package.\n\nlibrary(ggplot2)\nlibrary(r4pde)\ndat2 |&gt;\n  ggplot(aes(t, y)) +\n  geom_line() +\n  geom_point(data = temp, aes(t, yscaled)) +\n  theme_r4pde(font_size = 16) +\n  labs(x = \"Temperature\", y = \"Scaled severity\", \n       title = \"Analytis beta function\")\n\n\n\n\n\n\n\n\n\n\n\n20.2.1.2 Moisture effects\n\n20.2.1.2.1 Monomolecular function\nFor this example, we will use a subset of the data obtained from a study conducted under controlled conditions that aimed to assess the effects of moisture duration on the symptom development of citrus canker in sweet orange (Dalla Pria et al. 2006). As in the previous example for temperature effects, the data used here is only for severity on the cultivar Hamlin (plot a in Figure 20.3). The data was also extracted using the R package digitize.\nLet’s look at the original data and the predictions by the model fitted in the paper.\n\n\n\n\n\n\nFigure 20.3: Effect of leaf wetness duration (0, 4, 8, 12, 16, 20 or 24 h) on disease severity of citrus canker on sweet orange cvs Hamlin (a), Natal (b), Pera (c) and Valencia (d) at 30°C. Each point represents the mean of three repetitions. Vertical bars represent standard errors. Lines show the monomolecular model fitted to data. Source: Dalla Pria et al. (2006)\n\n\n\nFor this pattern in the data, we will fit a three-parameter asymptotic regression model. These models describe a limited growth, where y approaches an horizontal asymptote as x tends to infinity. This equation is also known as Monomolecular Growth, Mitscherlich law or von Bertalanffy law. See this tutorial for comprehensive information about fitting several non-linear regression models in R.\nAgain, we enter the data manually. The 𝑥x is wetness duration in hours and 𝑦y is severity.\n\nwet &lt;- tibble::tribble(~ x, ~ y,\n                       0 ,  0,\n                       4 ,  0.50,\n                       8 ,  0.81,\n                       12,  1.50,\n                       16,  1.26,\n                       20,  2.10,\n                       24,  1.45)\n\nThe model can be written as:\n\\(y = c1 + (d1-c1)*(1-exp(-x/e1))\\)\nwhere \\(c\\) is the lower limit (at \\(x = 0\\)), the parameter \\(d\\) is the upper limit and the parameter \\(e\\) (greater than 0) is determining the steepness of the increase as \\(x\\).\nWe will solve the model again using the nlsLM function. We should provide initial values for the three parameters.\n\nfit_wet &lt;- nlsLM(y ~ c1 + (d1 - c1) * (1 - exp(-x / e1)),\n                 start = list(c1 = 0.5,\n                              d1 = 3,\n                              e1 = 1),\n                 data = wet)\n\nsummary(fit_wet)\n\n\nFormula: y ~ c1 + (d1 - c1) * (1 - exp(-x/e1))\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)  \nc1 -0.04898    0.31182  -0.157   0.8828  \nd1  2.00746    0.70594   2.844   0.0467 *\ne1 11.63694    9.33183   1.247   0.2804  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3296 on 4 degrees of freedom\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 1.49e-08\n\nmodelr::rsquare(fit_wet, wet)\n\n[1] 0.8532282\n\n\nStore the value of the parameters in the respective object.\n\nHW &lt;- seq(0, 24, 0.1)\nc1 &lt;-  fit_wet$m$getAllPars()[1]\nd1 &lt;- fit_wet$m$getAllPars()[2]\ne1 &lt;- fit_wet$m$getAllPars()[3]\ny &lt;-  (c1 + (d1 - c1) * (1 - exp(-HW / e1)))\ndat2 &lt;- data.frame(HW, y)\n\nNow we can plot the predictions and the original data.\n\ndat2 |&gt;\n  ggplot(aes(HW, y)) +\n  geom_line() +\n  geom_point(data = wet, aes(x, y)) +\n  theme_r4pde(font_size = 16) +\n  labs(x = \"Wetness duration\", y = \"Severity\")\n\n\n\n\n\n\n\n\n\n\n20.2.1.2.2 Weibull function\nIn the study by (Ji et al. 2021, 2023), a Weibull model was fitted to the re-scaled data (0 to 1) on the effect of moisture duration on spore germination or infection. Let’s keep working with the re-scaled data on the citrus canker.\nThe model is given by:\n\\(y = 1 - \\exp(-(a \\cdot x)^b)\\)\nwhere \\(y\\) is the response variable, \\(x\\) is the moist duration, \\(a\\) is the scale parameter influencing the rate of infection and \\(b\\) is the shape parameter affecting the curve’s shape and acceleration\n\nwet$yscaled &lt;- rescale(wet$y) \nwet\n\n# A tibble: 7 × 3\n      x     y yscaled\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1     0  0      0    \n2     4  0.5    0.238\n3     8  0.81   0.386\n4    12  1.5    0.714\n5    16  1.26   0.6  \n6    20  2.1    1    \n7    24  1.45   0.690\n\n\n\nfit_wet2 &lt;- nlsLM(\n  yscaled ~ 1 - exp(-(a * x)^b),\n  data = wet,\n  start = list(a = 1, b = 2),  # Initial guesses for parameters a and b\n  )\nsummary(fit_wet2)\n\n\nFormula: yscaled ~ 1 - exp(-(a * x)^b)\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)   \na  0.07684    0.01296    5.93  0.00195 **\nb  1.07610    0.37103    2.90  0.03378 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1404 on 5 degrees of freedom\n\nNumber of iterations to convergence: 26 \nAchieved convergence tolerance: 1.49e-08\n\nmodelr::rsquare(fit_wet2, wet)\n\n[1] 0.8534077\n\n\nSet the value of the parameters in the respective objects\n\nx &lt;- seq(0, 24, 0.1)\na &lt;-  fit_wet2$m$getAllPars()[1]\nb &lt;- fit_wet2$m$getAllPars()[2]\ny &lt;-  1 - exp(-(a * x)^b)\ndat3 &lt;- data.frame(x, y)\n\n\ndat3 |&gt;\n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_point(data = wet, aes(x, yscaled)) +\n  theme_r4pde(font_size = 16) +\n  labs(x = \"Wetness duration\", y = \"Scaled severity\")\n\n\n\n\n\n\n\n\n\n\n\n20.2.1.3 Integrating temperature and wetness effects\nThe equations developed for the separate effects can be integrated to create a surface response curve or a simple contour plot. Let’s first integrate the generalized beta and the monomolecular models for the original severity data for the citrus canker experiment.\nFirst, we need a data frame for the interaction between temperature \\(t\\) and hours of wetness \\(hw\\). Then, we obtain the disease value for each combination of \\(t\\) and \\(hw\\).\n\nt &lt;- rep(1:40, 40)\nhw &lt;- rep(1:40, each = 40)\n\n# let's fit the two models again and store the parameters in objects\n# Temperature effects\nfit_temp &lt;- nlsLM(\n  y ~ a * ((t - b) ^ d) * ((c - t) ^ e),\n  start = list(\n    a = 0,\n    b = 10,\n    c = 40,\n    d = 1.5,\n    e = 1\n  ),\n  algorithm = \"port\",\n  data = temp\n)\nfit_temp$m$getAllPars()\n\n          a           b           c           d           e \n 0.00130259 11.99999931 40.13723602  1.76010097  0.83086798 \n\na &lt;- fit_temp$m$getAllPars()[1]\nb &lt;- fit_temp$m$getAllPars()[2]\nc &lt;- fit_temp$m$getAllPars()[3]\nd &lt;- fit_temp$m$getAllPars()[4]\ne &lt;- fit_temp$m$getAllPars()[5]\n\n## Moist duration effects\nfit_wet &lt;- nlsLM(y ~ c1 + (d1 - c1) * (1 - exp(-x / e1)),\n                 start = list(c1 = 0.5,\n                              d1 = 3,\n                              e1 = 1),\n                 data = wet)\n\nc1 &lt;-  fit_wet$m$getAllPars()[1]\nd1 &lt;- fit_wet$m$getAllPars()[2]\ne1 &lt;- fit_wet$m$getAllPars()[3]\n\ndis &lt;-\n  (a * (t - b) ^ d) * ((c - t) ^ e) * (c1 + (d1 - c1) * (1 - exp(- hw / e1)))\nvalidation &lt;- data.frame(t, hw, dis)\n\nNow the contour plot can be visualized using {ggplot2} and {geomtextpath} packages.\n\nlibrary(geomtextpath)\nggplot(validation, aes(t, hw, z = dis)) +\n  geom_contour_filled(bins = 8, alpha = 0.7) +\n  geom_textcontour(bins = 8,\n                   size = 2.5,\n                   padding = unit(0.05, \"in\")) +\n  theme_light(base_size = 10) +\n  theme(legend.position = \"right\") +\n  ylim(0, 40) +\n  labs(y = \"Wetness duration (hours)\",\n       fill = \"Severity\",\n       x = \"Temperature (Celcius)\",\n       title = \"Integrating generalized beta and monomolecular\")\n\n\n\n\n\n\n\n\nIn the second example, let’s integrate the Analytis beta and the Weibull model:\n\nfit_temp2 &lt;- nlsLM(\n  yscaled ~ (a * ((t - Tmin) / (Tmax - Tmin))^b * (1 - ((t - Tmin) / (Tmax - Tmin))))^c,\n  data = temp,\n  start = list(a = 1, b = 2, c = 3), # Initial guesses for parameters\n  algorithm = \"port\" \n)\n\nfit_temp2$m$getAllPars()\n\n       a        b        c \n6.762509 1.964817 1.160702 \n\na2 &lt;- fit_temp2$m$getAllPars()[1]\nb2 &lt;- fit_temp2$m$getAllPars()[2]\nc2 &lt;- fit_temp2$m$getAllPars()[3]\n\n\nfit_wet2 &lt;- nlsLM(\n  yscaled ~ 1 - exp(-(d * x)^e),\n  data = wet,\n  start = list(d = 1, e = 2),  # Initial guesses for parameters a and b\n  )\n\nd2 &lt;-  fit_wet2$m$getAllPars()[1]\ne2 &lt;- fit_wet2$m$getAllPars()[2]\n\nTmin &lt;- 12\nTmax &lt;- 40\ndis2 &lt;-  (a2 * ((t - Tmin) / (Tmax - Tmin))^b2 * (1 - ((t - Tmin) / (Tmax - Tmin))))^c2 * 1 - exp(-(d2 * hw)^e2)\n\nt &lt;- rep(1:40, 40)\nhw &lt;- rep(1:40, each = 40)\nvalidation2 &lt;- data.frame(t, hw, dis2)\n\nvalidation2 &lt;- validation2 |&gt; \n  filter(dis2 != \"NaN\") |&gt; \n  mutate(dis2 = case_when(dis2 &lt; 0 ~ 0,\n                          TRUE ~ dis2))\n\nNow the plot.\n\nggplot(validation2, aes(t, hw, z = dis2)) +\n  geom_contour_filled(bins = 7, alpha = 0.7) +\n  geom_textcontour(bins = 7,\n                   size = 2.5,\n                   padding = unit(0.05, \"in\")) +\n  theme_light(base_size = 10) +\n  theme(legend.position = \"right\") +\n  ylim(0, 40) +\n  labs(y = \"Wetness duration (hours)\",\n       fill = \"Severity\",\n       x = \"Temperature (Celcius)\",\n       title = \"Integrating generalized beta and monomolecular\")\n\n\n\n\n\n\n\n\nWe can create a 3D surface plot to visualize the predictions, as it was used in the original paper. Note that In plot_ly, a 3D surface plot requires a matrix or grid format for the z values, with corresponding vectors for x and y values that define the axes. If the data frame (validation2) has three columns (t, hw, and dis2), we’ll need to convert dis2 into a matrix format that plot_ly can interpret for a surface plot.\n\nlibrary(plotly)\nlibrary(reshape2)  \nz_matrix &lt;- acast(validation2, hw ~ t, value.var = \"dis2\")\nx_vals &lt;- sort(unique(validation2$t))  \ny_vals &lt;- sort(unique(validation2$hw)) \n\nplot_ly(x = ~x_vals, y = ~y_vals, z = ~z_matrix, type = \"surface\") |&gt; \n    config(displayModeBar = FALSE) |&gt; \n  layout(\n    scene = list(\n      xaxis = list(title = \"Temperature (°C)\", nticks = 10),\n      yaxis = list(title = \"Wetness Duration (hours)\", range = c(0, 40)),\n      zaxis = list(title = \"Severity\"),\n      aspectratio = list(x = 1, y = 1, z = 1)  \n    ),\n    title = \"Integrating Generalized Beta and Monomolecular\"\n  )\n\n\n\n\n\n\n\n20.2.1.4 Magarey’s generic infection model\nIn the early 2000s, Magarey and collaborators (Magarey et al. 2005) proposed a generic infection model for foliar fungal pathogens, designed to predict infection periods based on limited data on temperature and wetness requirements. The model uses cardinal temperatures (minimum, optimum, maximum) and the minimum wetness duration (Wmin) necessary for infection. The model can incorporate inputs based on estimated cardinal temperatures and surface wetness duration. These values are available for numerous pathogens and can be consulted in the literature (See table 2 of the paper by Magarey et al. (2005)).\nThe model utilizes a temperature response function, which is adjusted to the pathogen’s minimum and optimum wetness duration needs, allowing it to be broadly applicable even with limited data on specific pathogens. The model was validated with data from 53 studies, showing good accuracy and adaptability, even for pathogens lacking comprehensive data (Magarey et al. 2005).\nThe function is given by\n\\(f(T) = \\left( \\frac{T_{\\text{max}} - T}{T_{\\text{max}} - T_{\\text{opt}}} \\right)^{\\frac{T_{\\text{opt}} - T_{\\text{min}}}{T_{\\text{max}} - T_{\\text{opt}}}} \\times \\left( \\frac{T - T_{\\text{min}}}{T_{\\text{opt}} - T_{\\text{min}}} \\right)^{\\frac{T_{\\text{opt}} - T_{\\text{min}}}{T_{\\text{opt}} - T_{\\text{min}}}}\\)\nwhere \\(T\\) is the temperature, \\(T_{\\text{min}}\\) is the minimum temperature, \\(T_{\\text{opt}}\\) is the optimum temperature, and \\(T_{\\text{max}}\\) is the maximum temperature for infection.\nThe wetness duration requirement is given by\n\\(W(T) = \\frac{W_{\\text{min}}}{f(T)} \\leq W_{\\text{max}}\\)\nwhere \\(W_{\\text{min}}\\) is the minimum wetness duration requirement, and \\(W_{\\text{max}}\\) is an optional upper limit on \\(W(T)\\).\nLet’s write the functions for estimating the required wetness duration at each temperature.\n\ntemp_response &lt;- function(T, Tmin, Topt, Tmax) {\n  if (T &lt; Tmin || T &gt; Tmax) {\n    return(0)\n  } else {\n    ((Tmax - T) / (Tmax - Topt))^((Topt - Tmin) / (Tmax - Topt)) * \n    ((T - Tmin) / (Topt - Tmin))^((Topt - Tmin) / (Topt - Tmin))\n  }\n}\n\n# Define the function to calculate wetness duration requirement W(T)\nwetness_duration &lt;- function(T, Wmin, Tmin, Topt, Tmax, Wmax = Inf) {\n  f_T &lt;- temp_response(T, Tmin, Topt, Tmax)\n  if (f_T == 0) {\n    return(0)  # Infinite duration required if outside temperature range\n  }\n  W &lt;- Wmin / f_T\n  return(min(W, Wmax))  # Apply Wmax as an upper limit if specified\n}\n\nLet’s set the parameters for the fungus Venturia inaequalis, the cause of apple scab.\n\n# Parameters for Venturia inaequalis (apple scab)\nT &lt;- seq(0, 35, by = 0.5) \nWmin &lt;- 6                  \nTmin &lt;- 1                  \nTopt &lt;- 20                 \nTmax &lt;- 35                 \nWmax &lt;- 40.5                \n\n# Calculate wetness duration required at each temperature\nW_T &lt;- sapply(T, wetness_duration, Wmin, Tmin, Topt, Tmax, Wmax)\n\ntemperature_data_applescab &lt;- data.frame(\n  Temperature = T,\n  Wetness_Duration = W_T\n)\n\nAnd now the parameters for the fungus Phakopsora pachyrhizi, the cause of soybean rust in soybean.\n\n# Parameters for Phakposora pachyrhizi\nT &lt;- seq(0, 35, by = 0.5)  \nWmin &lt;- 8                \nTmin &lt;- 10                  \nTopt &lt;- 23                 \nTmax &lt;- 28                \nWmax &lt;- 12                 \n\n# Calculate wetness duration required at each temperature\nW_T &lt;- sapply(T, wetness_duration, Wmin, Tmin, Topt, Tmax, Wmax)\n\ntemperature_data_soyrust &lt;- data.frame(\n  Temperature = T,\n  Wetness_Duration = W_T)\n\nWe can produce the plots for each pathogen.\n\napplescab &lt;- ggplot(temperature_data_applescab, aes(x = Temperature, y = Wetness_Duration)) +\n  geom_line(color = \"black\", linewidth = 1, linetype =1) +\n  theme_r4pde(font_size = 14)+\n  labs(x = \"Temperature (°C)\", y = \"Wetness Duration (hours)\", \n       subtitle = \"Venturia inaequalis\")+\n  theme(plot.subtitle = element_text(face = \"italic\"))\n\nsoyrust &lt;- ggplot(temperature_data_soyrust, aes(x = Temperature, y = Wetness_Duration)) +\n  geom_line(color = \"black\", linewidth = 1, linetype =1) +\n  theme_r4pde(font_size = 14)+\n  labs(x = \"Temperature (°C)\", y = \"Wetness Duration (hours)\", \n       subtitle = \"Phakopsora pachyrizhi\")+ \n  theme(plot.subtitle = element_text(face = \"italic\"))\n\nlibrary(patchwork)\napplescab | soyrust\n\n\n\n\n\n\n\n\n\n\n\n20.2.2 Latency period models\nThe latent period can be defined as “the length of time between the start of the infection process by a unit of inoculum and the start of production of infectious units” (Madden et al. 2007). The latent period, analogous to the reproductive maturity age of nonparasitic organisms, defines the generation time between infections and is a key factor in pathogen development and epidemic progress in plant disease epidemiology (Vanderplank 1963). As a critical trait of aggressiveness, especially in polycyclic diseases, it largely determines the potential number of infection cycles within a season, impacting the overall epidemic intensity (Lannou 2012).\n\n20.2.2.1 Parabolic function\nThe effects of temperature on the length of the incubation and latent periods of hawthorn powdery mildew, caused by Podosphaera clandestina, were studied by Xu and Robinson (2000). In that work, the authors inoculated the leaves and, each day after inoculation, the upper surface of each leaf was examined for mildew colonies and conidiophores using a pen-microscope (×50). Sporulation was recorded at the leaf level, noting the number of colonies and the first appearance dates of colonies and sporulation for each leaf.\nThe latent period (LP) was defined as the time from inoculation to the first day of observed sporulation on the leaf. Due to the skewed distribution of LP across temperatures and inoculations, medians were used to summarize LP rather than means (Xu and Robinson 2000).\nLet’s look at two plots extracted from the paper. The first, on the left-hand side, is the original number of days of the latent period of each evaluated temperature (note: the solid symbol is for constant temperature while the open circle is for fluctuating temperature). On the right-hand side, the relationship between temperature and rates of development of powdery mildew under constant temperature during the latent periods; the solid line indicates the fitted model. The rate of fungal development was calculated as the reciprocal of the corresponding observed incubation (in hours) and latent periods.\n\n\n\n\n\n\nFigure 20.4: Source: Xu and Robinson (2000)\n\n\n\nThe latent period data in days for the solid black circle (constant temperature) above was extracted using the {digitize} R package.\n\nlatent &lt;- tibble::tribble(\n   ~T, ~days,\n  10L,   13L,\n  11L,   16L,\n  13L,    8L,\n  14L,    9L,\n  15L,    7L,\n  16L,    7L,\n  17L,    6L,\n  18L,    6L,\n  19L,    6L,\n  20L,    6L,\n  21L,    5L,\n  22L,    5L,\n  23L,    6L,\n  24L,    6L,\n  25L,    5L,\n  26L,    7L,\n  27L,    7L,\n  28L,   10L\n  )\n\nLet’s reproduce the two plots using the datapoints.\n\n#|fig-width: 10\n#|fig-height: 4\nlibrary(ggplot2)\nlibrary(r4pde)\n\np_latent &lt;- latent |&gt; \n  ggplot(aes(T, days))+\n  geom_point()+\n  theme_r4pde()\n\nlatent_rate &lt;- data.frame(\n  T = latent$T,  # Scale temperature\n  R = 1/latent$days/24\n)\n\np_latent_rate &lt;- latent_rate |&gt; \n  ggplot(aes(T, R))+\n  geom_point()+\n  theme_r4pde()\n\nlibrary(patchwork)\np_latent | p_latent_rate\n\n\n\n\n\n\n\n\nWe will fit the parabolic function proposed by Bernard et al. (2013) which predicts a thermal response curve (developmental rate, R), which is the relationship between the inverse of latent period and temperature. We need to enter the values for optimum temperature (where latent period is shortest) and the minimum latent period. The model is given by:\n\\(R(T) = \\frac{k}{\\text{LP}_{\\text{min}} + \\text{a} \\times (T - T_{\\text{opt}})^2}\\)\n\n# Load necessary package\n#library(minpack.lm)\n\n# Define the model formula\n# model_formula &lt;- R ~ (a + b * T)^(c * T)\nLPmin &lt;- 5 # minimum latent period\nTopt &lt;- 21 # Optimal temperature\nmodel_formula2 &lt;- R ~ k / (LPmin + a * (T - Topt)^2)\n\n# Set initial parameter estimates\n#start_values &lt;- list(a = 0.1, b = 0.01, c = 0.01)\nstart_values2 &lt;- list(a = 0.1, k = 1)\n# Fit the model\n#fit_rate &lt;- nlsLM(model_formula, data = latent_rate, start = start_values)\nfit_rate2 &lt;- nls(model_formula2, data = latent_rate, start = start_values2)\n\n# View the summary of the fit\nsummary(fit_rate2)\n\n\nFormula: R ~ k/(LPmin + a * (T - Topt)^2)\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na 0.060274   0.010705    5.63 3.76e-05 ***\nk 0.039520   0.001464   26.99 9.05e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0006936 on 16 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 3.183e-06\n\nfit_rate2$m$getAllPars()\n\n         a          k \n0.06027417 0.03952035 \n\na &lt;- fit_rate2$m$getAllPars()[1]\nk &lt;- fit_rate2$m$getAllPars()[2]\n\nNow we reproduce the plot with the fitted data. Note that the curve is not the same shown in the paper because we used a different equation.\n\nT &lt;- seq(10, 29, 0.1)\n#R &lt;- (a + b * T)^(c * T)\nR &lt;- k / (LPmin + a * (T - Topt)^2)\ndat2 &lt;- data.frame(T, R)\n\ndat2 |&gt;\n  ggplot(aes(T, R)) +\n  geom_line() +\n  geom_point(data = latent_rate, aes(T, R)) +\n  theme_r4pde(font_size = 16) +\n  labs(x = \"Temperature\", y = \"Inverse of the latent period (hour -1)\", \n       title = \"\")",
    "crumbs": [
      "Disease prediction",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Disease modeling</span>"
    ]
  },
  {
    "objectID": "prediction-disease-modeling.html#field-data",
    "href": "prediction-disease-modeling.html#field-data",
    "title": "20  Disease modeling",
    "section": "20.3 Field data",
    "text": "20.3 Field data\nWhile pathogen inoculum, host resistance, and agronomic factors are sometimes included alongside weather in empirically derived models using field data (Cao et al. 2015; Mehra et al. 2017; Shah et al. 2013), only a few models explicitly incorporate non-weather factors (Mehra et al. 2016; Paul and Munkvold 2004). In most cases, these models primarily rely on weather variables as predictors (González-Domínguez et al. 2023). This focus reflects the critical role of weather in driving key processes in the disease cycle, such as pathogen survival, dispersal, host infection, and reproduction (De Wolf and Isard 2007). Consequently, a primary objective for plant epidemiologists is to identify and quantify the relationships between weather conditions and measures of disease intensity (Coakley et al. 1988; Coakley 1988; Del Ponte et al. 2006; El Jarroudi et al. 2017; Pietravalle et al. 2003; Shah et al. 2013, 2019b).\nIn modeling efforts, the disease variable can be represented either as a continuous measure (e.g., incidence or severity) or as categorical data, which may be binary (e.g., non-epidemic vs. epidemic) or multinomial (e.g., low, moderate, and high severity). This variability in response types informs the selection of suitable modeling techniques, ensuring that the model accurately captures the nature of the data and the relationships between weather variables and disease outcomes.\nIn this section, I will demonstrate several modeling approaches that can be applied when field data is available. These examples will cover a range of techniques, starting with variable construction, which involves transforming raw weather data into summary measures that can effectively represent conditions relevant to disease outcomes. Next, variable selection methods will be explored to identify the most influential predictors, ensuring that models are both accurate and interpretable. The focus will then shift to model fitting, showing how different models, such as linear and logistic regression, can be used to capture relationships between weather variables and disease endpoints. Finally, model evaluation will be addressed, emphasizing metrics like accuracy, sensitivity, and area under the curve (AUC), which are crucial for assessing the predictive performance and reliability of the models developed.\nFollows a typical workflow for developing a disease prediction model, starting with disease data collection and weather data retrieval. The process includes data pre-processing, feature engineering, variable selection, model fitting, cross-validation, model tuning, and evaluation, followed by interpretation. Iterative feedback between model evaluation and variable selection aims to optimize model performance.\n\n\n\n\n\n\nFigure 20.5: Key steps for developing a plant disease prediction model\n\n\n\n\n20.3.1 Variable creation and selection\nVariable construction, particularly for weather-related variables, involves not only data transformation methods but also requires an understanding of how diseases respond to specific weather conditions at particular growth stages (De Cól et al. 2024; De Wolf et al. 2003). This approach ensures that the variables derived accurately capture biologically relevant processes, improving the realism and relevance of the model inputs.\nIn addition, data mining techniques are employed to systematically explore time-series data and identify potential weather-disease relationships (Coakley et al. 1988; Pietravalle et al. 2003; Shah et al. 2019b). These techniques involve creating lagged variables, moving averages, or window-based summaries that capture delayed or cumulative effects of weather on disease outcomes. By integrating system knowledge with data mining, researchers aim to construct variables that are both biologically meaningful and statistically robust, improving the chances of identifying predictors that enhance model accuracy and interpretability.\n\n20.3.1.1 Window-pane\n\n20.3.1.1.1 Variable construction\nWith regards to weather variable creation and selection for data-mining purposes, window-pane analysis, first introduced in the mid-1980s (Coakley 1985), has been widely used in modeling studies in plant pathology (Calvero Jr et al. 1996; Coakley et al. 1988; Coakley 1988; Dalla Lana et al. 2021b; Gouache et al. 2015; Kriss et al. 2010; Pietravalle et al. 2003; Te Beest et al. 2008). This method aids in identifying weather conditions that are most strongly associated with disease outcomes by segmenting a continuous time series (e.g. daily temperature, relative humidity, and rainfall), into discrete, fixed-length windows.\nThe analysis involves summarizing conditions within each window (e.g., mean, sum, count) and correlating these summaries with disease outcomes, which may be expressed as continuous measures (e.g., severity) or as categorical variables (e.g., low vs. high levels). This approach allows users to set specific start and end times, as well as window lengths, enabling the exploration of different temporal relationships between weather and disease. By sliding the start and end points along the series, multiple overlapping windows are generated, making it possible to identify the most informative variables for modeling. The selected optimal fixed-time and fixed-window-length variables derived from this analysis serve as predictors in model development, helping to improve the accuracy and relevance of disease forecasting models.\nHere’s an R code that demonstrates how the windows are defined over a 28-day period using four fixed window lengths (7, 14, 21, and 28 days), generating a total of 46 variables.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Define total days and window lengths\nmax_days &lt;- 28\nwindow_lengths &lt;- c(7, 14, 21, 28)\n\n# Create an empty data frame for all sliding windows\nwindow_data &lt;- data.frame()\n\n# Populate the data frame with start and end points for each window\nvar_id &lt;- 1  # Variable ID for each window\n\nfor (length in sort(window_lengths)) {  # Sort window lengths from shortest to longest\n  for (start_day in 0:(max_days - length)) {\n    end_day &lt;- start_day + length\n    window_data &lt;- rbind(\n      window_data,\n      data.frame(\n        start = start_day,\n        end = end_day,\n        var_id = var_id,\n        window_length = length\n      )\n    )\n    var_id &lt;- var_id + 1  # Increment variable ID\n  }\n}\n\n# Convert window_length to a factor for correct ordering in the legend\nwindow_data$window_length &lt;- factor(window_data$window_length, levels = sort(unique(window_data$window_length)))\n\n\n\nwindow_data |&gt; \n  ggplot(aes(x = start, xend = end, y = var_id, yend = var_id)) +\n  geom_segment(linewidth = 2) +  # Line segments for each window\n  scale_x_continuous(breaks = 0:max_days, limits = c(0, max_days)) +\n  scale_y_continuous(breaks = 1:var_id) +\n  labs(title = \"Window-pane\",\n    subtitle = \"Each variable of 7, 14, 21 and 28 days length over 28 days\",\n    x = \"Days\", y = \"Variable ID\", color = \"Window Length (days)\") +\n  r4pde::theme_r4pde(font_size = 14) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\nThe window-pane analysis requires a spreadsheet program (Kriss et al. 2010) or a specific algorithm that automates the creation of sliding windows at defined starting and ending times relative to a reference date. In the seminal work, software was programmed in the FORTRAN language and named WINDOW (Coakley 1985). It enabled the creation of windows and the calculation of summary statistics, including correlation with disease severity. Building on the original idea, a Genstat 6.1 algorithm was developed in the early 2000s, incorporating further adjustments such as the implementation of bootstrapping analysis to validate correlations and misclassifications identified by window-pane (Pietravalle et al. 2003; Te Beest et al. 2008). More recently, window-pane analysis including variable creation and analysis has been conducted in R using custom-made scripts (Dalla Lana et al. 2021b; Gouache et al. 2015).\nI will demonstrate the windowpane() function of the {r4pde} package developed to facilitate the creation of variables using the window-pane approach. First, let’s load a dataset that contains information on the disease, as well as metadata, including a key date that will be used as the starting point for window creation. The BlastWheat dataset, which is included in the {r4pde} package, was provided by De Cól et al. (2024).\n\nlibrary(r4pde)\nlibrary(dplyr)\ntrials &lt;- BlastWheat\nglimpse(trials)\n\nRows: 143\nColumns: 10\n$ study      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ year       &lt;dbl&gt; 2012, 2012, 2012, 2012, 2012, 2013, 2013, 2013, 2013, 2014,…\n$ location   &lt;chr&gt; \"Dourados\", \"Palotina\", \"Londrina\", \"Planaltina\", \"Itaberá\"…\n$ state      &lt;chr&gt; \"MS\", \"PR\", \"PR\", \"DF\", \"SP\", \"MS\", \"MG\", \"DF\", \"SP\", \"MG\",…\n$ latitude   &lt;dbl&gt; -22.27516, -24.35483, -23.35916, -15.60387, -24.06832, -22.…\n$ longitude  &lt;dbl&gt; -54.81640, -53.75794, -51.16476, -47.71381, -49.15575, -54.…\n$ heading    &lt;chr&gt; \"10-05-2012\", \"09-06-2012\", \"01-06-2012\", \"03-05-2012\", \"15…\n$ inc_mean   &lt;dbl&gt; 93.30, 40.50, 100.00, 35.50, 7.71, 48.70, 22.00, 46.50, 68.…\n$ index_mean &lt;dbl&gt; 68.65, 11.73, 92.86, 6.36, 0.35, 9.17, 2.91, 10.41, 41.86, …\n$ yld_mean   &lt;dbl&gt; 1097.00, 1219.38, 495.12, 1747.44, 2148.31, 506.00, 1562.75…\n\n\nWe can note that the heading date, which will be used as reference date in our analysis, is not defined as date object, which needs correction.\n\ntrials$heading = as.Date(trials$heading, format = \"%d-%m-%Y\")\nglimpse(trials$heading)\n\n Date[1:143], format: \"2012-05-10\" \"2012-06-09\" \"2012-06-01\" \"2012-05-03\" \"2012-04-15\" ...\n\n\nThe weather data for our analysis will be downloaded from NASA POWER. Since we have multiple trials with different heading dates, a wrapper function for the get_power() function from the {nasapower} package was created, included in {r4pde}, and named get_nasapower(). This function enables the download of data for a period “around” the key date, which can be defined by the user. In our case, we will download data from 28 days before and 28 days after the heading date.\n\nweather_data &lt;- get_nasapower(\n  data = trials,\n  days_around = 28,\n  date_col = \"heading\",\n  pars = c(\"T2M\", \"T2M_MAX\", \"T2M_MIN\", \"T2M_RANGE\", \"RH2M\", \n           \"PRECTOTCORR\", \"T2MDEW\", \"WS2M\", \"PS\", \"GWETTOP\", \n           \"GWETPROF\", \"CLRSKY_SFC_PAR_TOT\")\n)\n# See all parameters in the website: https://power.larc.nasa.gov/#resources\n\n# save the data for faster rendering\nwrite_csv(weather_data, \"data/weather_windowpane.csv\")\n\nNow we can see the weather data and join the two dataframes.\n\n# read the data\nweather_data &lt;- readr::read_csv(\"data/weather_windowpane.csv\")\n\nglimpse(weather_data)\n\nRows: 8,151\nColumns: 14\n$ LON         &lt;dbl&gt; -54.8164, -54.8164, -54.8164, -54.8164, -54.8164, -54.8164…\n$ LAT         &lt;dbl&gt; -22.27516, -22.27516, -22.27516, -22.27516, -22.27516, -22…\n$ YEAR        &lt;dbl&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012…\n$ MM          &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5…\n$ DD          &lt;dbl&gt; 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26…\n$ DOY         &lt;dbl&gt; 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114…\n$ YYYYMMDD    &lt;date&gt; 2012-04-12, 2012-04-13, 2012-04-14, 2012-04-15, 2012-04-1…\n$ T2M         &lt;dbl&gt; 24.31, 25.83, 24.99, 24.15, 22.72, 23.33, 23.76, 24.83, 24…\n$ RH2M        &lt;dbl&gt; 83.69, 77.62, 81.69, 82.69, 74.62, 74.44, 70.19, 67.75, 77…\n$ PRECTOTCORR &lt;dbl&gt; 0.02, 1.13, 11.00, 0.06, 0.00, 0.04, 0.00, 0.03, 24.08, 20…\n$ T2M_MAX     &lt;dbl&gt; 28.61, 31.01, 30.01, 29.43, 30.33, 30.69, 31.51, 32.49, 30…\n$ T2M_MIN     &lt;dbl&gt; 20.82, 20.43, 21.11, 19.04, 16.31, 17.37, 17.15, 17.69, 20…\n$ T2MDEW      &lt;dbl&gt; 21.19, 21.23, 21.38, 20.75, 17.47, 18.01, 17.29, 17.85, 20…\n$ study       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n# apply a full join\ntrials_weather &lt;- full_join(trials, weather_data) \n\nWe are now ready to use the windowpane function to create new variables. The function has several arguments. Note the two date variables: end_date, which serves as the reference for sliding the windows, and date_col, which represents the date for each day in the time series. The summary_type specifies the statistic to be calculated, while the direction determines whether the sliding windows will move backward, forward, or in both directions relative to the end date. Lastly, the group_by argument specifies the index variable for the epidemic or study.\nWe will create new variables based on the mean daily temperature (T2M), with each variable representing the mean value over one of the four window lengths (7, 14, 21, and 28 days) defined in the window_length argument. We will only generate variables that cover periods before the heading date, using “backward” in the direction argument.\n\n# Create window variables separated for each weather variable\n\nwp_T2M &lt;- windowpane(\n  data = trials_weather,\n  end_date_col = heading,\n  date_col = YYYYMMDD,\n  variable = T2M,  \n  summary_type = \"mean\",\n  threshold = NULL,\n  window_lengths = c(7, 14, 21, 28),\n  direction = \"backward\",\n  group_by_cols = \"study\", \n)\n\nwpT2M_MIN_15 &lt;- windowpane(\n    data = trials_weather,\n    end_date_col = heading,\n    date_col = YYYYMMDD,\n    variable = T2M_MIN,   \n    summary_type = \"below_threshold\",\n    threshold = 15,\n    window_lengths = c(7, 14, 21, 28),\n    direction = \"backward\",\n    group_by_cols = \"study\", \n  )\n\nwpT2M_MIN &lt;- windowpane(\n    data = trials_weather,\n    end_date_col = heading,\n    date_col = YYYYMMDD,\n    variable = T2M_MIN,   \n    summary_type = \"mean\",\n    threshold = NULL,\n    window_lengths = c(7, 14, 21, 28),\n    direction = \"backward\",\n    group_by_cols = \"study\", \n  )\n\nwpT2M_MAX &lt;- windowpane(\n    data = trials_weather,\n    end_date_col = heading,\n    date_col = YYYYMMDD,\n    variable = T2M_MAX,   \n    summary_type = \"mean\",\n    threshold = NULL,\n    window_lengths = c(7, 14, 21, 28),\n    direction = \"backward\",\n    group_by_cols = \"study\", \n  )\n\nwpPREC &lt;- windowpane(\n    data = trials_weather,\n    end_date_col = heading,\n    date_col = YYYYMMDD,\n    variable = PRECTOTCORR,   \n    summary_type = \"sum\",\n    threshold = NULL,\n    window_lengths = c(7, 14, 21, 28),\n    direction = \"backward\",\n    group_by_cols = \"study\", \n  )\n\nwpRH2M &lt;- windowpane(\n    data = trials_weather,\n    end_date_col = heading,\n    date_col = YYYYMMDD,\n    variable = RH2M,   \n    summary_type = \"mean\",\n    threshold = NULL,\n    window_lengths = c(7, 14, 21, 28),\n    direction = \"backward\",\n    group_by_cols = \"study\", \n  )\n\n# combine all datasets\nwp_all &lt;- cbind(wp_T2M, wpT2M_MIN_15, wpT2M_MIN, wpT2M_MAX, wpPREC, wpRH2M)\n\n\n\n20.3.1.1.2 Correlations and multiple hypothesis test\nThe window-pane analysis begins by quantifying the associations between each summary weather variable and disease response using a specific correlation coefficient (Pearson or Spearman). Usually, Spearman’s rank correlation can be preferred due to its ability to measure monotonic relationships and its robustness to outliers. In other cases, Spearman was used because the disease data was ordinal (Kriss et al. 2010).\nIn a recent study, Dalla Lana et al. (2021b), differing from Kriss et al. (2010), proposed the estimation of the precision of these correlations via bootstrapping, where a high number of samples (e.g. 1000) are randomly drawn (with replacement) from the original dataset. For each bootstrap sample, correlations between weather variables and disease outcomes are calculated, and the average across samples is used as the final measure of association. This approach ensures a more reliable estimation of the correlations by capturing variability and improving statistical robustness.\nThe window-pane analysis involves numerous tests, as each time window generates a separate test statistic. Because many tests are conducted, the global significance level becomes higher than the critical significance level set for individual tests, increasing the risk of false positives (Kriss et al. 2010). Additionally, the correlations among test statistics are influenced by overlapping time windows, shared data, and large-scale climatic patterns. To address this issue, Kriss et al. (2010) proposed the use of the Simes’ method, which tests the global null hypothesis that none of the individual correlations are significant. Simes’ method orders p-values and rejects the global null hypothesis if any adjusted p-value meets a specific threshold.\nWhile this method indicates whether at least one correlation is significant, it does not provide significance for individual correlations. Therefore, the authors proposed that the individual correlation coefficients should be compared against a more stringent significance level (α = 0.005 instead of 0.05), reducing the likelihood of false positives but increasing false negatives. Although this adjustment is independent of the Simes’ method, there was a general alignment: significant global results often corresponded to significant individual correlations, and non-significant global results typically lacked significant individual correlations (Kriss et al. 2010).\nLet’s calculate the bootstrapped correlation coefficients combined with the Sime’s method. First, we need to subset our variables to the specific combination of weather and window.\n\n  T2M_MIN_7 = wp_all |&gt; dplyr::select(starts_with(\"length7_T2M_MIN_mean\"))\n  T2M_MIN_14 = wp_all |&gt; dplyr::select(starts_with(\"length14_T2M_MIN_mean\"))\n  T2M_MIN_21 = wp_all |&gt; dplyr::select(starts_with(\"length21_T2M_MIN_mean\"))\n  T2M_MIN_28 = wp_all |&gt; dplyr::select(starts_with(\"length28_T2M_MIN_mean\"))\n \n  T2M_MAX_7 = wp_all |&gt; dplyr::select(starts_with(\"length7_T2M_MAX_mean\"))\n  T2M_MAX_14 = wp_all |&gt; dplyr::select(starts_with(\"length14_T2M_MAX_mean\"))\n  T2M_MAX_21 = wp_all |&gt; dplyr::select(starts_with(\"length21_T2M_MAX_mean\"))\n  T2M_MAX_28 = wp_all |&gt; dplyr::select(starts_with(\"length28_T2M_MAX_mean\"))\n  \n  T2M_7 = wp_all |&gt; select(starts_with(\"length7_T2M_mean\"))\n  T2M_14 = wp_all |&gt; select(starts_with(\"length14_T2M_mean\"))\n  T2M_21 = wp_all |&gt; select(starts_with(\"length21_T2M_mean\"))\n  T2M_28 = wp_all |&gt; select(starts_with(\"length28_T2M_mean\"))\n  \n  RH2M_7 = wp_all |&gt; select(starts_with(\"length7_RH2M_mean\"))\n  RH2M_14 = wp_all |&gt; select(starts_with(\"length14_RH2M_mean\"))\n  RH2_21 = wp_all |&gt; select(starts_with(\"length21_RH2M_mean\"))\n  RH2_28 = wp_all |&gt; select(starts_with(\"length28_RH2M_mean\"))\n  \n  PRECTOTCORR_7 = wp_all |&gt; select(starts_with(\"length7_PRECTOTCORR\"))\n  PRECTOTCORR_14 = wp_all |&gt; select(starts_with(\"length14_PRECTOTCORR\"))\n  PRECTOTCORR_21 = wp_all |&gt; select(starts_with(\"length21_PRECTOTCORR\"))\n  PRECTOTCORR_28 = wp_all |&gt; select(starts_with(\"length28_PRECTOTCORR\"))\n  \n  T2M_MINb_7 = wp_all |&gt; select(starts_with(\"length7_T2M_MIN_below\"))\n  T2M_MINb_14 = wp_all |&gt; select(starts_with(\"length14_T2M_MIN_below\"))\n  T2M_MINb_21 = wp_all |&gt; select(starts_with(\"length21_T2M_MIN_below\"))\n  T2M_MINb_28 = wp_all |&gt; select(starts_with(\"length28_T2M_MIN_below\"))\n\nNow, we can use the windowpane_tests() function from the {r4pde} package to analyze each of the datasets created above. This function will compute the bootstrapped correlation coefficients for the variables of interest and apply the Simes’ procedure to account for multiple testing, providing adjusted P-values for more robust statistical inference.\n\nlibrary(boot)\ndata &lt;- T2M_MINb_7 \ndata$inc &lt;- trials$inc_mean \nresponse_var &lt;- 'inc'  \nresults &lt;- windowpane_tests(data, response_var, corr_type = \"spearman\", R = 1000)\nresults_TMINb &lt;- results$results\n\nresults\n\n$results\n                                  variable correlation      p_value  mean_corr\n1  length7_T2M_MIN_below_threshold_-18_-24  -0.5295915 1.040432e-11 -0.5249507\n2  length7_T2M_MIN_below_threshold_-17_-23  -0.5136581 5.359609e-11 -0.5130773\n3  length7_T2M_MIN_below_threshold_-15_-21  -0.5116609 6.543248e-11 -0.5063511\n4  length7_T2M_MIN_below_threshold_-10_-16  -0.5063934 1.100732e-10 -0.5008654\n5  length7_T2M_MIN_below_threshold_-11_-17  -0.5054218 1.210402e-10 -0.5013972\n6  length7_T2M_MIN_below_threshold_-16_-22  -0.5038720 1.407514e-10 -0.4988078\n7  length7_T2M_MIN_below_threshold_-12_-18  -0.4972482 2.659657e-10 -0.4921962\n8  length7_T2M_MIN_below_threshold_-19_-25  -0.4951918 3.231718e-10 -0.4943599\n9   length7_T2M_MIN_below_threshold_-9_-15  -0.4913413 4.638201e-10 -0.4853357\n10  length7_T2M_MIN_below_threshold_-7_-13  -0.4882483 6.180090e-10 -0.4853013\n11   length7_T2M_MIN_below_threshold_-3_-9  -0.4793727 1.386299e-09 -0.4775091\n12    length7_T2M_MIN_below_threshold_0_-6  -0.4757057 1.922737e-09 -0.4737155\n13  length7_T2M_MIN_below_threshold_-8_-14  -0.4743890 2.160334e-09 -0.4731947\n14 length7_T2M_MIN_below_threshold_-14_-20  -0.4742346 2.189972e-09 -0.4734724\n15  length7_T2M_MIN_below_threshold_-4_-10  -0.4735015 2.336183e-09 -0.4693049\n16   length7_T2M_MIN_below_threshold_-2_-8  -0.4730773 2.425031e-09 -0.4741065\n17   length7_T2M_MIN_below_threshold_-1_-7  -0.4698400 3.218863e-09 -0.4678987\n18  length7_T2M_MIN_below_threshold_-6_-12  -0.4663226 4.364062e-09 -0.4625572\n19 length7_T2M_MIN_below_threshold_-13_-19  -0.4630281 5.785628e-09 -0.4610996\n20  length7_T2M_MIN_below_threshold_-5_-11  -0.4594875 7.807700e-09 -0.4582524\n21 length7_T2M_MIN_below_threshold_-20_-26  -0.4334293 6.401965e-08 -0.4305574\n22 length7_T2M_MIN_below_threshold_-21_-27  -0.4291061 8.925631e-08 -0.4271509\n      sd_corr median_corr rank  m simes_threshold significant_simes\n1  0.05792860  -0.5281375    1 22     0.002272727              TRUE\n2  0.06219467  -0.5156222    2 22     0.004545455              TRUE\n3  0.06085362  -0.5081182    3 22     0.006818182              TRUE\n4  0.06439511  -0.5032007    4 22     0.009090909              TRUE\n5  0.06317908  -0.5034083    5 22     0.011363636              TRUE\n6  0.06375940  -0.4989540    6 22     0.013636364              TRUE\n7  0.06625604  -0.4921979    7 22     0.015909091              TRUE\n8  0.06260022  -0.4976638    8 22     0.018181818              TRUE\n9  0.06759529  -0.4847309    9 22     0.020454545              TRUE\n10 0.06787472  -0.4880106   10 22     0.022727273              TRUE\n11 0.06497033  -0.4802147   11 22     0.025000000              TRUE\n12 0.06536201  -0.4736488   12 22     0.027272727              TRUE\n13 0.06529500  -0.4757740   13 22     0.029545455              TRUE\n14 0.06578540  -0.4759618   14 22     0.031818182              TRUE\n15 0.06580619  -0.4740651   15 22     0.034090909              TRUE\n16 0.06212626  -0.4755563   16 22     0.036363636              TRUE\n17 0.06178866  -0.4722236   17 22     0.038636364              TRUE\n18 0.06805414  -0.4621550   18 22     0.040909091              TRUE\n19 0.06631706  -0.4606907   19 22     0.043181818              TRUE\n20 0.06542673  -0.4562920   20 22     0.045454545              TRUE\n21 0.06440558  -0.4348640   21 22     0.047727273              TRUE\n22 0.06557457  -0.4305249   22 22     0.050000000              TRUE\n   individual_significant\n1                    TRUE\n2                    TRUE\n3                    TRUE\n4                    TRUE\n5                    TRUE\n6                    TRUE\n7                    TRUE\n8                    TRUE\n9                    TRUE\n10                   TRUE\n11                   TRUE\n12                   TRUE\n13                   TRUE\n14                   TRUE\n15                   TRUE\n16                   TRUE\n17                   TRUE\n18                   TRUE\n19                   TRUE\n20                   TRUE\n21                   TRUE\n22                   TRUE\n\n$summary_table\n               Metric         Value\n1 Global P-value (Pg)  2.288950e-10\n2     Max Correlation -4.271509e-01\n\n$global_significant\n[1] TRUE\n\n\nThe window-pane tests results indicate strong, statistically significant negative correlations between the response variable, the number of days when the TMIN was lower than 15 oC, and the selected predictors over a 7-day period. The correlations range from approximately -0.56 to -0.50, with all P-values below the global significance threshold after Simes’ correction, suggesting that these predictors are robustly associated with the response.\nThe global P-value, which accounts for multiple testing, is exceptionally low, confirming a significant overall relationship, while the highest observed correlation is -0.50. These findings highlight that low-temperature conditions over 7 days are consistently linked with the response variable, emphasizing the importance of temperature variability in this context.\nLet’s apply the test function to 7-day long windows for three other variables: relative humidity, precipitation and maximum temperature.\n\nlibrary(r4pde)\ndata1 &lt;- RH2M_7 \ndata1$inc &lt;- trials$inc_mean \nresponse_var &lt;- 'inc'  \nresults1 &lt;- windowpane_tests(data1, response_var, corr_type = \"spearman\", R = 1000)\nresults_RH &lt;- results1$results\n\n\ndata2 &lt;- PRECTOTCORR_7 \ndata2$inc &lt;- trials$inc_mean \nresponse_var &lt;- 'inc'  \nresults2 &lt;- windowpane_tests(data2, response_var, corr_type = \"spearman\", R = 1000)\nresults_PREC &lt;- results2$results\n\n\ndata3 &lt;- T2M_MAX_7 # enter the dataset\ndata3$inc &lt;- trials$inc_mean \nresponse_var &lt;- 'inc'  \nresults3 &lt;- windowpane_tests(data3, response_var, corr_type = \"spearman\", R = 1000)\nresults_TMAX &lt;- results3$results\n\nThe first variable in the results dataframe contains the name of the variable. We can extract the starting and ending day from each window using the extract function, which allows for regex-based extraction into new columns. In this case, two new columns will be created representing the extreme days of the window.\n\n# Use strcapture to extract components into new columns\ndf_TMINb7 &lt;- results_TMINb |&gt;\n  extract(variable, into = c(\"variable_prefix\", \"low\", \"high\"), \n          regex = \"^(.*)_(-?\\\\d+)_(-?\\\\d+)$\", convert = TRUE)\n\n\ndf_RH7 &lt;- results_RH |&gt;\n  extract(variable, into = c(\"variable_prefix\", \"low\", \"high\"), \n          regex = \"^(.*)_(-?\\\\d+)_(-?\\\\d+)$\", convert = TRUE)\n\n\ndf_PREC7 &lt;- results_PREC |&gt;\n  extract(variable, into = c(\"variable_prefix\", \"low\", \"high\"), \n          regex = \"^(.*)_(-?\\\\d+)_(-?\\\\d+)$\", convert = TRUE)\n\ndf_TMAX7 &lt;- results_TMAX |&gt;\n  extract(variable, into = c(\"variable_prefix\", \"low\", \"high\"), \n          regex = \"^(.*)_(-?\\\\d+)_(-?\\\\d+)$\", convert = TRUE)\n\nNow we can plot the mean correlation for the start day of each window.\n\np_TMIN &lt;- df_TMINb7 |&gt; \n  ggplot(aes(low, mean_corr, fill = p_value))+\n  ylim(-1, 1)+\n  geom_col()+\n  theme_r4pde(font_size = 12)+\n  labs(title = \"N. days TMIN &lt; 15C\",\n       subtitle = \"7-day window\",\n       y = \"Spearman's correlation\",\n       x = \"Day relative to heading date \")\n\np_RH &lt;- df_RH7 |&gt; \n  ggplot(aes(low, mean_corr, fill = p_value))+\n  ylim(-1, 1)+\n  geom_col()+\n  theme_r4pde(font_size = 12)+\n  labs(title = \"Relative humidity\", \n       subtitle = \"7-day window\",\n       y = \"Spearman's correlation\",\n       x = \"Day relative to heading date \")\n\np_TMAX &lt;- df_TMAX7 |&gt; \nggplot(aes(low, mean_corr, fill = p_value))+\n  ylim(-1, 1)+\n  geom_col()+\n  theme_r4pde(font_size = 12)+\n  labs(title = \"Max. temperature\", \n       subtitle = \"7-day window\",\n       y = \"Spearman's correlation\",\n       x = \"Day relative to heading date \")\n\n\np_PREC &lt;- df_PREC7 |&gt; \n  ggplot(aes(low, mean_corr, fill = p_value))+\n  ylim(-1, 1)+\n  geom_col()+\n  theme_r4pde(font_size = 12)+\n  labs(title = \"Cumulative rainfall\", \n       subtitle = \"7-day window\",\n       y = \"Spearman's correlation\",\n       x = \"Day relative to heading date \")\n    \nscale_common &lt;- scale_fill_viridis_c(limits = c(0, 1), \n                                     na.value = \"grey90\")\n\np_TMIN &lt;- p_TMIN + scale_common\np_TMAX &lt;- p_TMAX + scale_common\np_RH &lt;- p_RH + scale_common\np_PREC &lt;- p_PREC + scale_common\n\n# Combine plots using patchwork and collect guides\n(p_TMIN | p_TMAX) / (p_RH | p_PREC) + \n  plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nInterpretation. All 7-day-length variables but maximum temperature were strongly associated with the incidence of wheat blast. These three variables can be used as candidates for developing predictions models.\n\n\n\n20.3.1.2 Functional data analysis\nFunctional Data Analysis (FDA) is a statistical approach used to analyze and model data that varies continuously over time or space. It represents entire data sequences (e.g., time series) as smooth functions, allowing for the exploration of trends, patterns, and relationships in complex, continuous processes rather than relying on discrete, segmented data points (Gertheiss et al. 2024; Ramsay and Silverman 2005).\nIn the FDA framework, scalar-on-function and function-on-scalar models are two types of FDA models, designed to examine relationships between functional predictors and outcomes. Function-on-scalar models use single values (e.g., mean daily relative humidity) as predictors to explain the shape of an entire outcome curve (e.g. disease). Meanwhile, in scalar-on-function models, time series data (e.g., daily temperatures) are used as predictors to identify which parts of the series are linked to a single outcome. Together, these FDA models offer complementary insights, allowing either for predicting a scalar outcome from curves or understanding how scalar predictors influence entire functions (Ramsay and Silverman 2005).\nInitially applied in plant pathology to study the relationship between Fusarium head blight outbreaks and weather variables over time (using the function-on-scalar model) (Shah et al. 2019a), FDA has since been adopted in other plant pathology studies with similar objectives (Alves et al. 2022; Hjelkrem et al. 2021; Shah et al. 2019b).\nIn particular, Shah et al. (2019b) used FDA as a basis to develop logistic regression models for large-scale deployment by utilizing FDA through a scalar-on-function approach to predict plant diseases - in this approach, the model identifies which parts of the time series (e.g., certain weeks or days) are most associated with the disease outcome, helping to predict whether an epidemic is likely to occur based on patterns in the weather series. Moreover, FDA was employed to identify the most relevant temporal regions of weather series associated with disease outbreaks, improving prediction by summarizing key windows across critical growth stages. The authors discussed that , unlike traditional window-pane analysis, FDA reduces statistical testing issues and offers a more refined method for incorporating novel predictors into simple, effective models (Shah et al. 2019b).\nWith the objective of identifying temporal regions of the weather series, Alves et al. (unpublished) proposed the use of a function-on-scalar model to gain insights about the time series related to a binary disease outcome by identifying significant regions in the weather series where the curves diverge. The function-on-scalar approach here compares the average functional trajectories of two groups: epidemic vs. non-epidemic. It identifies time regions where the mean weather curves for the two groups differ significantly. While the model doesn’t directly predict a binary response, it aids in diagnosing critical time points in the series that differentiate the two disease outcomes, supporting model development and refinement for binary classification.\nFor that purpose, the authors used the ITP2bspline() function of the R package {fdatest} that implements the Interval Testing Procedure (ITP) to compare two functional populations (epidemic vs. non-epidemic) evaluated on a uniform grid. It represents data using B-spline basis functions and assesses the significance of each basis coefficient with interval-wise control of the Family Wise Error Rate (FWER) (Pini and Vantini 2016).\nIn this section, I will first demonstrate how to fit the function-on-scalar regression with the objective of visualizing two curves, the difference coefficient function and the overall mean coefficient curve. The former shows how the effect of a weather-related variable on the response variable changes over the days relative to a reference date (e.g. planting date, flowering). It helps to identify critical windows when the weather variations have the most substantial impact. The latter represents the cumulative or overall effect of weather over the time period. It helps assess whether the influence of weather variable accumulates or stabilizes over time.\nThen, I will demonstrate the Interval Testing Procedure (ITP) to compare two functional populations (epidemic vs. non-epidemic) and identify exactly the days in the series that the two populations are significantly different. We will keep analyzing the data on wheat blast epidemics in Brazil.\n\n20.3.1.2.1 Function-on-scalar regression\nThe following analysis uses function-on-scalar regression to explore the relationship between relative humidity at 2 meters (RH2M) and the occurrence of wheat blast epidemics. Let’s first load the necessary libraries.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(r4pde)\nlibrary(refund)\nlibrary(ggplot2)\n\nThe first step involves reading and merging two datasets:\n- trials: Contains disease data with heading dates.\n- weather_data: Contains weather data for specific time windows. It was created in the previous section on window-pane and can be downloaded here.\nThe datasets are merged into trials_weather, and the resulting data is transformed into a wide format where each column represents time points (days), and rows correspond to individual studies. The binary epidemic status is created, where:\n\n1 indicates an epidemic (if inc_mean &gt; 20).\n0 indicates no epidemic (if inc_mean &lt;= 20).\n\nThe weather variable (RH2M) is used as the predictor, and time points (days) are calculated relative to the heading date.\n\n# load the disease datase\ntrials &lt;- r4pde::BlastWheat\ntrials$heading = as.Date(trials$heading, format = \"%d-%m-%Y\")\n\n# load the weather data constructed in previous section\nweather_data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/emdelponte/epidemiology-R/refs/heads/main/data/weather_windowpane.csv\")\ntrials_weather &lt;- full_join(trials, weather_data)\n\n# create epidemic variable and number of days relative to heading\ndat_wide_RH &lt;- trials_weather |&gt;\n    mutate(epidemic = if_else(inc_mean &gt; 20, 1, 0),\n           days = as.numeric(-(heading - YYYYMMDD))) |&gt;\n    dplyr::select(study, epidemic, days, RH2M) |&gt;\n    pivot_wider(id_cols = c(study, epidemic), \n                names_from = days, values_from = RH2M)\n\nThe response vector is the epidemic status (0 or 1), while the predictor matrix contains the values of the specified weather variable across time. The model fits smooth functions to represent the effects of time and the epidemic status on the weather variable using penalized splines. The function-on-scalar regression model is given by:\n\\(y_{ij} = \\beta_0(t_j) + \\beta_1(t_j) x_i + \\epsilon_{ij}\\)\nwhere: \\(y_{ij}\\) is the value of the weather variable for study \\(i\\) at time \\(j\\); \\(\\beta_0(t_j)\\) is the smooth function for the overall mean effect over time; \\(\\beta_1(t_j)\\) is the smooth function representing the effect of epidemic status over time; \\(x_i\\) is the binary epidemic status (0 or 1); and \\(\\epsilon{ij}\\) is the error term.\nWe can now fit the model using the pffr() function of the {refund} package.\n\nx &lt;- dat_wide_RH$epidemic  # Response vector\ny &lt;- dat_wide_RH |&gt; \n  select(-study, -epidemic) |&gt; \n  as.matrix()  # Matrix of predictors\n\nyind &lt;- as.numeric(colnames(y))\n\nFOSR_RH &lt;- pffr(y ~ x, \n             yind = yind,\n             bs.yindex = list(bs = \"ps\", k = 30, m = c(2, 1)), \n             bs.int = list(bs = \"ps\", k = 30, m = c(2, 1)),\n                algorithm = \"gam\")\n\nLet’s extract the coefficients of the model fit object and prepare the data and plot using gpplot.\n\n  coef_list &lt;- coef(FOSR_RH)$smterms\n\nusing seWithMean for  s(yind.vec) .\n\n  mean_df &lt;- data.frame(\n    time = coef_list[[1]]$coef[, \"yind.vec\"],\n    value = coef_list[[1]]$coef[, \"value\"],\n    lower = coef_list[[1]]$coef[, \"value\"] - 1.96 * coef_list[[1]]$coef[, \"se\"],\n    upper = coef_list[[1]]$coef[, \"value\"] + 1.96 * coef_list[[1]]$coef[, \"se\"],\n    term = \"Overall Mean\"\n  )\n  \n  x_df &lt;- data.frame(\n    time = coef_list[[2]]$coef[, \"yind.vec\"],\n    value = coef_list[[2]]$coef[, \"value\"],\n    lower = coef_list[[2]]$coef[, \"value\"] - 1.96 * coef_list[[2]]$coef[, \"se\"],\n    upper = coef_list[[2]]$coef[, \"value\"] + 1.96 * coef_list[[2]]$coef[, \"se\"],\n    term = \"Difference\"\n  )\n  \n  plot_df &lt;- bind_rows(mean_df, x_df)\n  \n\n  ggplot(plot_df, aes(x = time, y = value)) +\n    geom_ribbon(aes(ymin = lower, ymax = upper), \n                alpha = 0.2) +\n    geom_line(linewidth = 1.2) +\n    facet_grid(rows = vars(term), \n               scales = \"free_y\") +\n    theme_r4pde(font_size = 12) +\n    geom_hline(aes(yintercept = 0), \n               color = \"grey\", \n               linetype = \"dashed\") +\n    geom_vline(aes(xintercept = 0), \n               color = \"grey\", \n               linetype = \"dashed\") +\n    labs(x = \"Day relative to wheat heading\", y = \"Coefficient Function\",\n         title = \"RH2M\") +\n    theme(strip.text = element_text(face = \"bold\", size = rel(1.0)),\n          axis.title = element_text(face = \"bold\", size = 12))\n\n\n\n\n\n\n\n\nBecause we have several weather variables, we can facilitate the process by creating a function named fosr() that can be applied for the other variables.\n\n# Define the function for function-on-scalar regression\nfosr &lt;- function(data, weather_var) {\n  \n  # Step 1: Prepare the dataset and transform to wide format\n  dat_wide &lt;- data %&gt;%\n    mutate(epidemic = if_else(inc_mean &gt; 20, 1, 0),\n           days = as.numeric(-(heading - YYYYMMDD))) %&gt;%\n      dplyr::select(study, epidemic, days, !!sym(weather_var)) %&gt;%\n    pivot_wider(id_cols = c(study, epidemic), names_from = days, values_from = !!sym(weather_var))\n  \n\n  x &lt;- dat_wide$epidemic  # Response vector\n  y &lt;- dat_wide |&gt;\n    dplyr::select(-study, -epidemic) |&gt;\n    as.matrix()  # Matrix of predictors\n  \n  m2 &lt;- pffr(y ~ x, \n             yind = yind,\n             bs.yindex = list(bs = \"ps\", k = 30, m = c(2, 1)), \n             bs.int = list(bs = \"ps\", k = 30, m = c(2, 1)),\n                algorithm = \"gam\")\n\n  coef_list &lt;- coef(m2)$smterms\n  \n  mean_df &lt;- data.frame(\n    time = coef_list[[1]]$coef[, \"yind.vec\"],\n    value = coef_list[[1]]$coef[, \"value\"],\n    lower = coef_list[[1]]$coef[, \"value\"] - 1.96 * coef_list[[1]]$coef[, \"se\"],\n    upper = coef_list[[1]]$coef[, \"value\"] + 1.96 * coef_list[[1]]$coef[, \"se\"],\n    term = \"Overall Mean\"\n  )\n  \n  x_df &lt;- data.frame(\n    time = coef_list[[2]]$coef[, \"yind.vec\"],\n    value = coef_list[[2]]$coef[, \"value\"],\n    lower = coef_list[[2]]$coef[, \"value\"] - 1.96 * coef_list[[2]]$coef[, \"se\"],\n    upper = coef_list[[2]]$coef[, \"value\"] + 1.96 * coef_list[[2]]$coef[, \"se\"],\n    term = \"Difference\"\n  )\n  \n  plot_df &lt;- bind_rows(mean_df, x_df)\n  \n   ggplot(plot_df, aes(x = time, y = value)) +\n    geom_ribbon(aes(ymin = lower, ymax = upper), \n                alpha = 0.2) +\n    geom_line(linewidth = 1.2) +\n    facet_grid(rows = vars(term), \n               scales = \"free_y\") +\n    theme_r4pde(font_size = 10) +\n    geom_hline(aes(yintercept = 0), \n               color = \"grey\", \n               linetype = \"dashed\") +\n    geom_vline(aes(xintercept = 0), \n               color = \"grey\", \n               linetype = \"dashed\") +\n    labs(x = \"Day relative to wheat heading\", y = \"Coefficient Function\",\n         title = weather_var) +\n    theme(strip.text = element_text(face = \"bold\", size = rel(1.0)),\n          axis.title = element_text(face = \"bold\", size = 10))\n}\n\nNow we can apply the function to the other variables and obtain plots for each variable.\n\ntmin &lt;- fosr(trials_weather, \"T2M_MIN\")\n\nusing seWithMean for  s(yind.vec) .\n\ntmax &lt;- fosr(trials_weather, \"T2M_MAX\")\n\nusing seWithMean for  s(yind.vec) .\n\nrh &lt;- fosr(trials_weather, \"RH2M\")\n\nusing seWithMean for  s(yind.vec) .\n\nprec &lt;- fosr(trials_weather, \"PRECTOTCORR\")\n\nusing seWithMean for  s(yind.vec) .\n\nlibrary(patchwork)\n\n(tmin |tmax)/(rh | prec)\n\n\n\n\n\n\n\n\n\n\n20.3.1.2.2 Interval test procedure\nFor this test, we need to subset the data into two epidemic conditions, transform from long to wide format and create a matrix object.\n\ndf_epidemic &lt;- trials_weather |&gt; \n  mutate(epidemic = if_else(inc_mean &gt; 20, 1, 0),\n           days = as.numeric(-(heading - YYYYMMDD))) |&gt;\n  filter(epidemic == 1) |&gt; \n  dplyr::select(RH2M, days, study)\n\ndf_non_epidemic &lt;- trials_weather |&gt; \n   mutate(epidemic = if_else(inc_mean &gt; 20, 1, 0),\n           days = as.numeric(-(heading - YYYYMMDD))) |&gt;\n  filter(epidemic == 0) |&gt; \n  dplyr::select(RH2M, days, study)\n\n# Pivot data to wide format for FDA\ndf_epidemic_wide &lt;- df_epidemic |&gt;\n  group_by(study) |&gt;\n  pivot_wider(names_from = days, values_from = RH2M) |&gt;\n  ungroup() |&gt;\n  dplyr::select(-study)\n\ndf_non_epidemic_wide &lt;- df_non_epidemic |&gt;\n  group_by(study) |&gt;\n  pivot_wider(names_from = days, values_from = RH2M) |&gt;\n  ungroup() |&gt;\n  dplyr::select(-study)\n\n# Convert to matrix\ndata_epidemic &lt;- as.matrix(df_epidemic_wide)\ndata_non_epidemic &lt;- as.matrix(df_non_epidemic_wide)\n\nNow we can now apply the function specifying each new data in each data argument.\n\nlibrary(fdatest)\n\n# Perform FDA test\nitp_result &lt;- ITP2bspline(data1 = data_epidemic, \n                          data2 = data_non_epidemic, \n                          B = 100)\n\n[1] \"First step: basis expansion\"\nSwapping 'y' and 'argvals', because 'y' is  simpler,\n  and 'argvals' should be;  now  dim(argvals) =  57 ;  dim(y) =  57 x 143 \n[1] \"Second step: joint univariate tests\"\n[1] \"Third step: interval-wise combination and correction\"\n[1] \"creating the p-value matrix: end of row 2 out of 57\"\n[1] \"creating the p-value matrix: end of row 3 out of 57\"\n[1] \"creating the p-value matrix: end of row 4 out of 57\"\n[1] \"creating the p-value matrix: end of row 5 out of 57\"\n[1] \"creating the p-value matrix: end of row 6 out of 57\"\n[1] \"creating the p-value matrix: end of row 7 out of 57\"\n[1] \"creating the p-value matrix: end of row 8 out of 57\"\n[1] \"creating the p-value matrix: end of row 9 out of 57\"\n[1] \"creating the p-value matrix: end of row 10 out of 57\"\n[1] \"creating the p-value matrix: end of row 11 out of 57\"\n[1] \"creating the p-value matrix: end of row 12 out of 57\"\n[1] \"creating the p-value matrix: end of row 13 out of 57\"\n[1] \"creating the p-value matrix: end of row 14 out of 57\"\n[1] \"creating the p-value matrix: end of row 15 out of 57\"\n[1] \"creating the p-value matrix: end of row 16 out of 57\"\n[1] \"creating the p-value matrix: end of row 17 out of 57\"\n[1] \"creating the p-value matrix: end of row 18 out of 57\"\n[1] \"creating the p-value matrix: end of row 19 out of 57\"\n[1] \"creating the p-value matrix: end of row 20 out of 57\"\n[1] \"creating the p-value matrix: end of row 21 out of 57\"\n[1] \"creating the p-value matrix: end of row 22 out of 57\"\n[1] \"creating the p-value matrix: end of row 23 out of 57\"\n[1] \"creating the p-value matrix: end of row 24 out of 57\"\n[1] \"creating the p-value matrix: end of row 25 out of 57\"\n[1] \"creating the p-value matrix: end of row 26 out of 57\"\n[1] \"creating the p-value matrix: end of row 27 out of 57\"\n[1] \"creating the p-value matrix: end of row 28 out of 57\"\n[1] \"creating the p-value matrix: end of row 29 out of 57\"\n[1] \"creating the p-value matrix: end of row 30 out of 57\"\n[1] \"creating the p-value matrix: end of row 31 out of 57\"\n[1] \"creating the p-value matrix: end of row 32 out of 57\"\n[1] \"creating the p-value matrix: end of row 33 out of 57\"\n[1] \"creating the p-value matrix: end of row 34 out of 57\"\n[1] \"creating the p-value matrix: end of row 35 out of 57\"\n[1] \"creating the p-value matrix: end of row 36 out of 57\"\n[1] \"creating the p-value matrix: end of row 37 out of 57\"\n[1] \"creating the p-value matrix: end of row 38 out of 57\"\n[1] \"creating the p-value matrix: end of row 39 out of 57\"\n[1] \"creating the p-value matrix: end of row 40 out of 57\"\n[1] \"creating the p-value matrix: end of row 41 out of 57\"\n[1] \"creating the p-value matrix: end of row 42 out of 57\"\n[1] \"creating the p-value matrix: end of row 43 out of 57\"\n[1] \"creating the p-value matrix: end of row 44 out of 57\"\n[1] \"creating the p-value matrix: end of row 45 out of 57\"\n[1] \"creating the p-value matrix: end of row 46 out of 57\"\n[1] \"creating the p-value matrix: end of row 47 out of 57\"\n[1] \"creating the p-value matrix: end of row 48 out of 57\"\n[1] \"creating the p-value matrix: end of row 49 out of 57\"\n[1] \"creating the p-value matrix: end of row 50 out of 57\"\n[1] \"creating the p-value matrix: end of row 51 out of 57\"\n[1] \"creating the p-value matrix: end of row 52 out of 57\"\n[1] \"creating the p-value matrix: end of row 53 out of 57\"\n[1] \"creating the p-value matrix: end of row 54 out of 57\"\n[1] \"creating the p-value matrix: end of row 55 out of 57\"\n[1] \"creating the p-value matrix: end of row 56 out of 57\"\n[1] \"creating the p-value matrix: end of row 57 out of 57\"\n[1] \"Interval Testing Procedure completed\"\n\n\nHere, we can see the p-values for each time point as well as which time points were significant considering p &lt; 0.05.\n\n# global p-values\nitp_result$corrected.pval\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n# significant components\nwhich(itp_result$corrected.pval &lt; 0.05)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n[51] 51 52 53 54 55 56 57\n\n\nThe plot function produces two graphs, one for the curves and another for the p-values where the shaded area indicate the significance.\n\n# Plot FDA results\nplot(itp_result, main = \"RH2M\", \n     xrange = c(-28, 28), \n     xlab = 'Day', xaxt = 'n')\n\n\n\n\n\n\n\naxis(1, at = seq(-28, 28, by = 2), \n     labels = seq(-28, 28, by = 2))\n\n\n\n\n\n\n\n\nBecause we may have several weather variables, we can avoid repeating the code for each variable by creating a function. The arguments are the data and weather variable. Optionally, the user can define the threshold for disease.\n\nrun_ITP_test &lt;- function(data, weather_var, threshold = 20, B = 100) {\n  \n  # Convert the weather_var input to symbol for dplyr's select function\n  weather_var_sym &lt;- sym(weather_var)\n  \n  # Filter and prepare epidemic data\n  df_epidemic &lt;- data |&gt; \n    mutate(epidemic = if_else(inc_mean &gt; threshold, 1, 0),\n           days = as.numeric(-(heading - YYYYMMDD))) |&gt; \n    filter(epidemic == 1) |&gt; \n    dplyr::select(!!weather_var_sym, days, study)\n  \n  # Filter and prepare non-epidemic data\n  df_non_epidemic &lt;- data |&gt; \n    mutate(epidemic = if_else(inc_mean &gt; threshold, 1, 0),\n           days = as.numeric(-(heading - YYYYMMDD))) |&gt; \n    filter(epidemic == 0) |&gt; \n    dplyr::select(!!weather_var_sym, days, study)\n  \n  # Pivot epidemic data to wide format\n  df_epidemic_wide &lt;- df_epidemic |&gt; \n    group_by(study) |&gt; \n    pivot_wider(names_from = days, values_from = !!weather_var_sym) |&gt; \n    ungroup() |&gt; \n    dplyr::select(-study)\n  \n  # Pivot non-epidemic data to wide format\n  df_non_epidemic_wide &lt;- df_non_epidemic |&gt; \n    group_by(study) |&gt; \n    pivot_wider(names_from = days, values_from = !!weather_var_sym) |&gt; \n    ungroup() |&gt; \n    dplyr::select(-study)\n  \n  # Convert to matrix\n  data_epidemic &lt;- as.matrix(df_epidemic_wide)\n  data_non_epidemic &lt;- as.matrix(df_non_epidemic_wide)\n  \n  # Perform FDA test\n  itp_result &lt;- ITP2bspline(data1 = data_epidemic, \n                            data2 = data_non_epidemic, \n                            B = B)\n  \n  return(itp_result)\n}\n\nApplying the function to each variable.\n\nitp_tmin &lt;- run_ITP_test(data = trials_weather, weather_var = \"T2M_MIN\")\n\n[1] \"First step: basis expansion\"\nSwapping 'y' and 'argvals', because 'y' is  simpler,\n  and 'argvals' should be;  now  dim(argvals) =  57 ;  dim(y) =  57 x 143 \n[1] \"Second step: joint univariate tests\"\n[1] \"Third step: interval-wise combination and correction\"\n[1] \"creating the p-value matrix: end of row 2 out of 57\"\n[1] \"creating the p-value matrix: end of row 3 out of 57\"\n[1] \"creating the p-value matrix: end of row 4 out of 57\"\n[1] \"creating the p-value matrix: end of row 5 out of 57\"\n[1] \"creating the p-value matrix: end of row 6 out of 57\"\n[1] \"creating the p-value matrix: end of row 7 out of 57\"\n[1] \"creating the p-value matrix: end of row 8 out of 57\"\n[1] \"creating the p-value matrix: end of row 9 out of 57\"\n[1] \"creating the p-value matrix: end of row 10 out of 57\"\n[1] \"creating the p-value matrix: end of row 11 out of 57\"\n[1] \"creating the p-value matrix: end of row 12 out of 57\"\n[1] \"creating the p-value matrix: end of row 13 out of 57\"\n[1] \"creating the p-value matrix: end of row 14 out of 57\"\n[1] \"creating the p-value matrix: end of row 15 out of 57\"\n[1] \"creating the p-value matrix: end of row 16 out of 57\"\n[1] \"creating the p-value matrix: end of row 17 out of 57\"\n[1] \"creating the p-value matrix: end of row 18 out of 57\"\n[1] \"creating the p-value matrix: end of row 19 out of 57\"\n[1] \"creating the p-value matrix: end of row 20 out of 57\"\n[1] \"creating the p-value matrix: end of row 21 out of 57\"\n[1] \"creating the p-value matrix: end of row 22 out of 57\"\n[1] \"creating the p-value matrix: end of row 23 out of 57\"\n[1] \"creating the p-value matrix: end of row 24 out of 57\"\n[1] \"creating the p-value matrix: end of row 25 out of 57\"\n[1] \"creating the p-value matrix: end of row 26 out of 57\"\n[1] \"creating the p-value matrix: end of row 27 out of 57\"\n[1] \"creating the p-value matrix: end of row 28 out of 57\"\n[1] \"creating the p-value matrix: end of row 29 out of 57\"\n[1] \"creating the p-value matrix: end of row 30 out of 57\"\n[1] \"creating the p-value matrix: end of row 31 out of 57\"\n[1] \"creating the p-value matrix: end of row 32 out of 57\"\n[1] \"creating the p-value matrix: end of row 33 out of 57\"\n[1] \"creating the p-value matrix: end of row 34 out of 57\"\n[1] \"creating the p-value matrix: end of row 35 out of 57\"\n[1] \"creating the p-value matrix: end of row 36 out of 57\"\n[1] \"creating the p-value matrix: end of row 37 out of 57\"\n[1] \"creating the p-value matrix: end of row 38 out of 57\"\n[1] \"creating the p-value matrix: end of row 39 out of 57\"\n[1] \"creating the p-value matrix: end of row 40 out of 57\"\n[1] \"creating the p-value matrix: end of row 41 out of 57\"\n[1] \"creating the p-value matrix: end of row 42 out of 57\"\n[1] \"creating the p-value matrix: end of row 43 out of 57\"\n[1] \"creating the p-value matrix: end of row 44 out of 57\"\n[1] \"creating the p-value matrix: end of row 45 out of 57\"\n[1] \"creating the p-value matrix: end of row 46 out of 57\"\n[1] \"creating the p-value matrix: end of row 47 out of 57\"\n[1] \"creating the p-value matrix: end of row 48 out of 57\"\n[1] \"creating the p-value matrix: end of row 49 out of 57\"\n[1] \"creating the p-value matrix: end of row 50 out of 57\"\n[1] \"creating the p-value matrix: end of row 51 out of 57\"\n[1] \"creating the p-value matrix: end of row 52 out of 57\"\n[1] \"creating the p-value matrix: end of row 53 out of 57\"\n[1] \"creating the p-value matrix: end of row 54 out of 57\"\n[1] \"creating the p-value matrix: end of row 55 out of 57\"\n[1] \"creating the p-value matrix: end of row 56 out of 57\"\n[1] \"creating the p-value matrix: end of row 57 out of 57\"\n[1] \"Interval Testing Procedure completed\"\n\nitp_tmax &lt;- run_ITP_test(data = trials_weather, weather_var = \"T2M_MAX\")\n\n[1] \"First step: basis expansion\"\nSwapping 'y' and 'argvals', because 'y' is  simpler,\n  and 'argvals' should be;  now  dim(argvals) =  57 ;  dim(y) =  57 x 143 \n[1] \"Second step: joint univariate tests\"\n[1] \"Third step: interval-wise combination and correction\"\n[1] \"creating the p-value matrix: end of row 2 out of 57\"\n[1] \"creating the p-value matrix: end of row 3 out of 57\"\n[1] \"creating the p-value matrix: end of row 4 out of 57\"\n[1] \"creating the p-value matrix: end of row 5 out of 57\"\n[1] \"creating the p-value matrix: end of row 6 out of 57\"\n[1] \"creating the p-value matrix: end of row 7 out of 57\"\n[1] \"creating the p-value matrix: end of row 8 out of 57\"\n[1] \"creating the p-value matrix: end of row 9 out of 57\"\n[1] \"creating the p-value matrix: end of row 10 out of 57\"\n[1] \"creating the p-value matrix: end of row 11 out of 57\"\n[1] \"creating the p-value matrix: end of row 12 out of 57\"\n[1] \"creating the p-value matrix: end of row 13 out of 57\"\n[1] \"creating the p-value matrix: end of row 14 out of 57\"\n[1] \"creating the p-value matrix: end of row 15 out of 57\"\n[1] \"creating the p-value matrix: end of row 16 out of 57\"\n[1] \"creating the p-value matrix: end of row 17 out of 57\"\n[1] \"creating the p-value matrix: end of row 18 out of 57\"\n[1] \"creating the p-value matrix: end of row 19 out of 57\"\n[1] \"creating the p-value matrix: end of row 20 out of 57\"\n[1] \"creating the p-value matrix: end of row 21 out of 57\"\n[1] \"creating the p-value matrix: end of row 22 out of 57\"\n[1] \"creating the p-value matrix: end of row 23 out of 57\"\n[1] \"creating the p-value matrix: end of row 24 out of 57\"\n[1] \"creating the p-value matrix: end of row 25 out of 57\"\n[1] \"creating the p-value matrix: end of row 26 out of 57\"\n[1] \"creating the p-value matrix: end of row 27 out of 57\"\n[1] \"creating the p-value matrix: end of row 28 out of 57\"\n[1] \"creating the p-value matrix: end of row 29 out of 57\"\n[1] \"creating the p-value matrix: end of row 30 out of 57\"\n[1] \"creating the p-value matrix: end of row 31 out of 57\"\n[1] \"creating the p-value matrix: end of row 32 out of 57\"\n[1] \"creating the p-value matrix: end of row 33 out of 57\"\n[1] \"creating the p-value matrix: end of row 34 out of 57\"\n[1] \"creating the p-value matrix: end of row 35 out of 57\"\n[1] \"creating the p-value matrix: end of row 36 out of 57\"\n[1] \"creating the p-value matrix: end of row 37 out of 57\"\n[1] \"creating the p-value matrix: end of row 38 out of 57\"\n[1] \"creating the p-value matrix: end of row 39 out of 57\"\n[1] \"creating the p-value matrix: end of row 40 out of 57\"\n[1] \"creating the p-value matrix: end of row 41 out of 57\"\n[1] \"creating the p-value matrix: end of row 42 out of 57\"\n[1] \"creating the p-value matrix: end of row 43 out of 57\"\n[1] \"creating the p-value matrix: end of row 44 out of 57\"\n[1] \"creating the p-value matrix: end of row 45 out of 57\"\n[1] \"creating the p-value matrix: end of row 46 out of 57\"\n[1] \"creating the p-value matrix: end of row 47 out of 57\"\n[1] \"creating the p-value matrix: end of row 48 out of 57\"\n[1] \"creating the p-value matrix: end of row 49 out of 57\"\n[1] \"creating the p-value matrix: end of row 50 out of 57\"\n[1] \"creating the p-value matrix: end of row 51 out of 57\"\n[1] \"creating the p-value matrix: end of row 52 out of 57\"\n[1] \"creating the p-value matrix: end of row 53 out of 57\"\n[1] \"creating the p-value matrix: end of row 54 out of 57\"\n[1] \"creating the p-value matrix: end of row 55 out of 57\"\n[1] \"creating the p-value matrix: end of row 56 out of 57\"\n[1] \"creating the p-value matrix: end of row 57 out of 57\"\n[1] \"Interval Testing Procedure completed\"\n\nitp_prec &lt;- run_ITP_test(data = trials_weather, weather_var = \"PRECTOTCORR\")\n\n[1] \"First step: basis expansion\"\nSwapping 'y' and 'argvals', because 'y' is  simpler,\n  and 'argvals' should be;  now  dim(argvals) =  57 ;  dim(y) =  57 x 143 \n[1] \"Second step: joint univariate tests\"\n[1] \"Third step: interval-wise combination and correction\"\n[1] \"creating the p-value matrix: end of row 2 out of 57\"\n[1] \"creating the p-value matrix: end of row 3 out of 57\"\n[1] \"creating the p-value matrix: end of row 4 out of 57\"\n[1] \"creating the p-value matrix: end of row 5 out of 57\"\n[1] \"creating the p-value matrix: end of row 6 out of 57\"\n[1] \"creating the p-value matrix: end of row 7 out of 57\"\n[1] \"creating the p-value matrix: end of row 8 out of 57\"\n[1] \"creating the p-value matrix: end of row 9 out of 57\"\n[1] \"creating the p-value matrix: end of row 10 out of 57\"\n[1] \"creating the p-value matrix: end of row 11 out of 57\"\n[1] \"creating the p-value matrix: end of row 12 out of 57\"\n[1] \"creating the p-value matrix: end of row 13 out of 57\"\n[1] \"creating the p-value matrix: end of row 14 out of 57\"\n[1] \"creating the p-value matrix: end of row 15 out of 57\"\n[1] \"creating the p-value matrix: end of row 16 out of 57\"\n[1] \"creating the p-value matrix: end of row 17 out of 57\"\n[1] \"creating the p-value matrix: end of row 18 out of 57\"\n[1] \"creating the p-value matrix: end of row 19 out of 57\"\n[1] \"creating the p-value matrix: end of row 20 out of 57\"\n[1] \"creating the p-value matrix: end of row 21 out of 57\"\n[1] \"creating the p-value matrix: end of row 22 out of 57\"\n[1] \"creating the p-value matrix: end of row 23 out of 57\"\n[1] \"creating the p-value matrix: end of row 24 out of 57\"\n[1] \"creating the p-value matrix: end of row 25 out of 57\"\n[1] \"creating the p-value matrix: end of row 26 out of 57\"\n[1] \"creating the p-value matrix: end of row 27 out of 57\"\n[1] \"creating the p-value matrix: end of row 28 out of 57\"\n[1] \"creating the p-value matrix: end of row 29 out of 57\"\n[1] \"creating the p-value matrix: end of row 30 out of 57\"\n[1] \"creating the p-value matrix: end of row 31 out of 57\"\n[1] \"creating the p-value matrix: end of row 32 out of 57\"\n[1] \"creating the p-value matrix: end of row 33 out of 57\"\n[1] \"creating the p-value matrix: end of row 34 out of 57\"\n[1] \"creating the p-value matrix: end of row 35 out of 57\"\n[1] \"creating the p-value matrix: end of row 36 out of 57\"\n[1] \"creating the p-value matrix: end of row 37 out of 57\"\n[1] \"creating the p-value matrix: end of row 38 out of 57\"\n[1] \"creating the p-value matrix: end of row 39 out of 57\"\n[1] \"creating the p-value matrix: end of row 40 out of 57\"\n[1] \"creating the p-value matrix: end of row 41 out of 57\"\n[1] \"creating the p-value matrix: end of row 42 out of 57\"\n[1] \"creating the p-value matrix: end of row 43 out of 57\"\n[1] \"creating the p-value matrix: end of row 44 out of 57\"\n[1] \"creating the p-value matrix: end of row 45 out of 57\"\n[1] \"creating the p-value matrix: end of row 46 out of 57\"\n[1] \"creating the p-value matrix: end of row 47 out of 57\"\n[1] \"creating the p-value matrix: end of row 48 out of 57\"\n[1] \"creating the p-value matrix: end of row 49 out of 57\"\n[1] \"creating the p-value matrix: end of row 50 out of 57\"\n[1] \"creating the p-value matrix: end of row 51 out of 57\"\n[1] \"creating the p-value matrix: end of row 52 out of 57\"\n[1] \"creating the p-value matrix: end of row 53 out of 57\"\n[1] \"creating the p-value matrix: end of row 54 out of 57\"\n[1] \"creating the p-value matrix: end of row 55 out of 57\"\n[1] \"creating the p-value matrix: end of row 56 out of 57\"\n[1] \"creating the p-value matrix: end of row 57 out of 57\"\n[1] \"Interval Testing Procedure completed\"\n\nitp_tmin$corrected.pval\n\n [1] 0.00 0.02 0.04 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00\n[16] 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.01 0.00 0.00 0.00 0.00 0.01 0.07 0.00\n[31] 0.00 0.02 0.01 0.02 0.01 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.18 0.06 0.03\n[46] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.04 0.04 0.09\n\nwhich(itp_tmin$corrected.pval &lt; 0.05)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 30 31 32 33 34 35 36 37 38 39 40 41 42 45 46 47 48 49 50 51 52 53\n[51] 54 55 56\n\nitp_tmax$corrected.pval\n\n [1] 0.05 0.05 0.08 0.22 0.09 0.09 0.08 0.09 0.13 0.56 0.28 0.18 0.22 0.38 0.47\n[16] 0.59 0.52 0.52 0.63 0.36 0.18 0.24 0.20 0.30 0.56 0.57 0.37 0.18 0.13 0.11\n[31] 0.11 0.15 0.16 0.16 0.18 0.20 0.22 0.53 0.32 0.32 0.28 0.23 0.30 0.42 0.41\n[46] 0.30 0.52 0.79 0.91 0.61 0.58 0.58 0.46 0.33 0.00 0.05 0.07\n\nwhich(itp_tmax$corrected.pval &lt; 0.05)\n\n[1] 55\n\nitp_prec$corrected.pval\n\n [1] 0.00 0.18 0.42 0.10 0.37 0.05 0.00 0.00 0.94 0.08 0.02 0.06 0.00 0.00 0.25\n[16] 0.00 0.00 0.00 0.01 0.49 0.00 0.03 0.01 0.01 0.19 0.07 0.01 0.01 0.00 0.00\n[31] 0.00 0.00 0.00 0.30 0.00 0.00 0.04 0.03 0.00 0.00 0.03 0.19 0.83 0.62 0.52\n[46] 0.41 0.77 0.92 0.92 0.92 0.41 0.18 0.00 0.00 0.00 0.06 0.01\n\nwhich(itp_prec$corrected.pval &lt; 0.05)\n\n [1]  1  7  8 11 13 14 16 17 18 19 21 22 23 24 27 28 29 30 31 32 33 35 36 37 38\n[26] 39 40 41 53 54 55 57\n\nplot(itp_tmin, main = \"TMIN\", \n     xrange = c(-28, 28), \n     xlab = 'Day', xaxt = 'n')\n\n\n\n\n\n\n\naxis(1, at = seq(-28, 28, by = 2), \n     labels = seq(-28, 28, by = 2))\n\n\n\n\n\n\n\nplot(itp_tmax, main = \"TMAX\", \n     xrange = c(-28, 28), \n     xlab = 'Day', xaxt = 'n')\n\n\n\n\n\n\n\naxis(1, at = seq(-28, 28, by = 2), \n     labels = seq(-28, 28, by = 2))\n\n\n\n\n\n\n\nplot(itp_prec, main = \"PREC\", \n     xrange = c(-28, 28), \n     xlab = 'Day', xaxt = 'n')\n\n\n\n\n\n\n\naxis(1, at = seq(-28, 28, by = 2), \n     labels = seq(-28, 28, by = 2))\n\n\n\n\n\n\n\n\nInterpretation: Based on the results above, minimum temperature and relative humidity had a significant influence across the entire range of days, particularly during the pre-heading period. Maximum temperature did not affect the epidemics at any time. Lastly, precipitation influenced wheat blast during several short intervals, with the most notable impact occurring over a longer seven-day period around the heading stage. Therefore, TMIN, RH and PREC can be tested further as candidate predictors in prediction models.\n\n\n\n\n20.3.2 Model fitting\n\n20.3.2.1 Logistic regression models\nLogistic regression is a statistical modeling technique used to predict the probability of a binary outcome based on one or more predictor variables (James et al. 2021). In the context of plant disease prediction, the logistic model relates factors like environmental conditions (usually weather), host characteristics (e.g., cultivar resistance), and pathogen presence to the likelihood of disease occurrence. This occurrence can be expressed as presence or absence (Mila et al. 2004) or as a classification of incidence/severity based on an epidemic threshold (De Cól et al. 2024; De Wolf et al. 2003). By integrating these variables, the model enables researchers to assess disease risk under varying conditions and to develop decision support systems. This modeling approach has been used successfully in several studies in the field (De Cól et al. 2024; De Wolf et al. 2003; Harikrishnan and Río 2008; Mila et al. 2004; Romero et al. 2021; Shah et al. 2013; Willbur et al. 2018; Xu et al. 2013), demonstrating its efficacy in predicting disease outbreaks and informing early intervention efforts.\nThe logistic model uses the logistic function to transform a linear combination of predictors into a probability between 0 and 1. This transformation ensures that the predicted values are valid probabilities, facilitating meaningful interpretations. By estimating the influence of each predictor on the odds of disease, logistic regression helps identify significant risk factors. Additionally, the model’s coefficients provide insights into the strength and direction of the association between predictors and disease risk, offering valuable information for understanding the underlying mechanisms of disease development and progression.\nThe logistic regression model predicts the probability \\(P\\) of the binary outcome \\(Y = 1 \\mid X\\) (e.g., presence or absence of disease) given a set of predictor variables \\(X\\). The model uses the logistic function to ensure the predicted probabilities lie between 0 and 1:\n\\(P(Y = 1 \\mid X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p)}}\\)\nAlternatively, the model can be expressed in terms of the log-odds (also known as the logit function):\n\\(\\log\\left( \\frac{P(Y = 1 \\mid X)}{1 - P(Y = 1 \\mid X)} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\\)\nCompared to more complex machine learning models, logistic regression offers a high degree of interpretability, which is crucial in many scientific and practical applications. While models like neural networks, random forests, or support vector machines can achieve high predictive accuracy, they often function as “black boxes” where the relationship between predictors and the outcome is not easily understood (Molnar 2019). In contrast, logistic regression provides clear and direct interpretations of model parameters, allowing researchers and practitioners to understand how each predictor variable influences the probability of disease occurrence. This transparency makes logistic regression a preferred choice in contexts where explaining the underlying relationships is as important as prediction accuracy. The balance between interpretability and predictive performance justifies the choice of logistic regression in several contexts, especially when insights into the data are essential for decision-making, which is the case in plant disease prediction contexts.\nIn this section, we will keep working with the wheat blast disease data and expand on the previous sections on variable construction and selection, a usual step prior to testing the prediction variables in the logistic regression model.\n\n20.3.2.1.1 Data preparation\n\nlibrary(r4pde)\nlibrary(tidyverse)\n\n# load the disease datase\ntrials &lt;- r4pde::BlastWheat\ntrials$heading = as.Date(trials$heading, format = \"%d-%m-%Y\")\n\n# load the weather data \nweather_data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/emdelponte/epidemiology-R/refs/heads/main/data/weather_windowpane.csv\")\n\n# join the two datasets\ntrials_weather &lt;- full_join(trials, weather_data)\n\n# create epidemic variable and number of days relative to heading\ntrials_weather &lt;- trials_weather |&gt;\n    mutate(epidemic = if_else(inc_mean &gt; 20, 1, 0),\n           days = as.numeric(-(heading - YYYYMMDD)))\n\nLet’s create a few weather-related variables based on the original variables available from the NASA POWER. First, we need to group by study and epidemic and then filter the days to only the pre-heading (28 days) since will attempt to develop a model using only pre-heading variables. We will not include mean daily maximum temperature given its weak association based on window-pane and not significant influence based on the interval test procedure of the FDA (see previous section).\n\nepi_weather &lt;- trials_weather |&gt; \n  group_by(study, epidemic) |&gt; \n  filter(days &gt;= -28 & days &lt;= 7) |&gt; \n  summarise(\n    tmin_pre = mean(T2M_MIN, na.rm = TRUE),\n    rh_pre = mean(RH2M, na.rm = TRUE),\n    prec_pre = sum(PRECTOTCORR, na.rm = TRUE))|&gt; \n  ungroup() |&gt; \n  dplyr::select(- study)\n\n\n## create rainfall around heading\nepi_rain &lt;- trials_weather |&gt; \n  group_by(study, epidemic) |&gt; \n  filter(days &gt;= -3 & days &lt;= 6) |&gt; \n  summarise(\n    prec_pos = sum(PRECTOTCORR, na.rm = TRUE))|&gt; \n  ungroup() |&gt; \n  dplyr::select(- study, - epidemic)\n\nepi_weather2 &lt;- cbind(epi_weather, epi_rain)\n\n\n\n20.3.2.1.2 Model fitting\nNow we can fit the model and look at the summary.\n\n# model fitting\nm_logistic &lt;- glm(epidemic ~ tmin_pre + rh_pre + prec_pre, \n                      data = epi_weather, \n                      family = binomial)\n\n# Model summary\nsummary(m_logistic)\n\n\nCall:\nglm(formula = epidemic ~ tmin_pre + rh_pre + prec_pre, family = binomial, \n    data = epi_weather)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1986  -0.5938   0.3459   0.6545   2.1343  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -20.16698    3.87801  -5.200 1.99e-07 ***\ntmin_pre      0.27391    0.08610   3.181  0.00147 ** \nrh_pre        0.22020    0.04734   4.652 3.29e-06 ***\nprec_pre     -0.12680    0.20907  -0.606  0.54420    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 197.06  on 142  degrees of freedom\nResidual deviance: 125.11  on 139  degrees of freedom\nAIC: 133.11\n\nNumber of Fisher Scoring iterations: 5\n\n\ninterpretation of the coefficients:\n\nIntercept: Estimate of -18.96, indicating the baseline log-odds of an epidemic when all predictors are zero.\ntmin_pre: Positive estimate (0.27, p = 0.002), indicating that higher minimum temperatures are significantly associated with an increased probability of an epidemic.\nrh_pre: Positive estimate (0.20, p &lt; 0.001), suggesting that higher relative humidity is strongly associated with an increased probability of an epidemic.\nprec_pre: Negative but non-significant estimate (-0.002, p = 0.70), indicating no significant relationship between precipitation and epidemic occurrence.\n\nModel fit\n\nThe null deviance (197.06) reflects the fit of a model without predictors, while the residual deviance (121.55) indicates the fit of the full model with predictors. A significant reduction in deviance suggests that the predictors improve model fit.\nThe AIC (129.55) helps evaluate model performance, with lower values indicating better fit.\n\nLet’s fit again the model using only the two significant variables and include the prec_pos variable.\n\nm_logistic &lt;- glm(epidemic ~ tmin_pre + rh_pre + prec_pos, \n                      data = epi_weather2, \n                      family = binomial)\n\nsummary(m_logistic)\n\n\nCall:\nglm(formula = epidemic ~ tmin_pre + rh_pre + prec_pos, family = binomial, \n    data = epi_weather2)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -19.31170    3.47903  -5.551 2.84e-08 ***\ntmin_pre      0.34620    0.10023   3.454 0.000552 ***\nrh_pre        0.18418    0.03746   4.917 8.80e-07 ***\nprec_pos      0.06393    0.02222   2.878 0.004003 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 197.06  on 142  degrees of freedom\nResidual deviance: 109.99  on 139  degrees of freedom\nAIC: 117.99\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe can notice that including rainfall accumulation around flowering, the AIC is reduced to 117.99, suggesting that its inclusion improved prediction.\nWe can check the accuracy of the new model in predicting the training data.\n\nlibrary(caret)\n\n# Obtain the predicted probability\npred_probs &lt;- predict(m_logistic, epi_weather2, type = \"response\")\n\n# Convert probabilities to binary outcomes using 0.5 as the threshold\npred_classes &lt;- ifelse(pred_probs &gt;= 0.5, 1, 0)\n\n# Calculate accuracy\nactual_classes &lt;- epi_weather$epidemic\naccuracy &lt;- mean(pred_classes == actual_classes)\naccuracy\n\n[1] 0.8181818\n\n\nThe confusionMatrix() function of the {caret} package gives a more complete information about the perfomrance of the model on the training set.\n\n# create the confusion matrix\nconfusionMatrix(data = as.factor(as.numeric(pred_probs &gt; 0.50)),  mode= \"everything\",  reference = as.factor(epi_weather$epidemic))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 49 10\n         1 16 68\n                                          \n               Accuracy : 0.8182          \n                 95% CI : (0.7451, 0.8777)\n    No Information Rate : 0.5455          \n    P-Value [Acc &gt; NIR] : 6.459e-12       \n                                          \n                  Kappa : 0.6305          \n                                          \n Mcnemar's Test P-Value : 0.3268          \n                                          \n            Sensitivity : 0.7538          \n            Specificity : 0.8718          \n         Pos Pred Value : 0.8305          \n         Neg Pred Value : 0.8095          \n              Precision : 0.8305          \n                 Recall : 0.7538          \n                     F1 : 0.7903          \n             Prevalence : 0.4545          \n         Detection Rate : 0.3427          \n   Detection Prevalence : 0.4126          \n      Balanced Accuracy : 0.8128          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\n20.3.2.1.3 Model evaluation\nIt is highly desirable to evaluate model performance metrics using data that was not “seen” by the model during training. If the dataset is sufficiently large (e.g., more than 300 observations), it can be split into training and test sets, typically using an 80:20 ratio. For smaller datasets, alternatives like leave-one-out cross-validation (LOOCV) can be applied, as was done in the original wheat blast model study, which had 143 cases available (De Cól et al. 2024).\nThe Leave-One-Out Cross-Validation (LOOCV) is a resampling method used to evaluate model performance, particularly with small datasets. In LOOCV, the model is trained repeatedly, each time using all but one observation, which is set aside for testing. This process is repeated until every observation has served as the test case once. The overall performance metric is calculated by averaging the results from all iterations, providing an unbiased estimate of model accuracy (Hastie et al. 2009).\nLet’s first create the training and test data.\n\ntrain_index &lt;- createDataPartition(epi_weather2$epidemic, p = 0.8, list = FALSE)\ntrain_data &lt;- epi_weather2[train_index, ]\ntest_data &lt;- epi_weather2[-train_index, ]\n\nNow we fit the logistic model on the training data and obtain the predictions.\n\n# fit the model on training data\nm_logistic2 &lt;- glm(epidemic ~ tmin_pre + rh_pre + prec_pos, \n                   data = train_data, \n                   family = binomial)\n\n# Predict on the test data\npred &lt;- predict(m_logistic2, newdata = test_data, type = \"response\")\n\nLet’s build now the confusion matrix for the test data.\n\nconfusion_matrix &lt;- confusionMatrix(data = as.factor(as.numeric(pred &gt; 0.5)), reference = as.factor(test_data$epidemic), mode = \"everything\")\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 10  0\n         1  6 12\n                                         \n               Accuracy : 0.7857         \n                 95% CI : (0.5905, 0.917)\n    No Information Rate : 0.5714         \n    P-Value [Acc &gt; NIR] : 0.01543        \n                                         \n                  Kappa : 0.5882         \n                                         \n Mcnemar's Test P-Value : 0.04123        \n                                         \n            Sensitivity : 0.6250         \n            Specificity : 1.0000         \n         Pos Pred Value : 1.0000         \n         Neg Pred Value : 0.6667         \n              Precision : 1.0000         \n                 Recall : 0.6250         \n                     F1 : 0.7692         \n             Prevalence : 0.5714         \n         Detection Rate : 0.3571         \n   Detection Prevalence : 0.3571         \n      Balanced Accuracy : 0.8125         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nThe code below provides a comparative evaluation of model accuracy using two different cross-validation techniques, the LOOCV and 10-fold cross validation. The latter is similar to the LOOCV code, but with the addition of K = 10, which specifies 10-fold cross-validation. Here, the dataset is divided into 10 subsets (folds), with the model trained on 9 folds and tested on the remaining fold. This process repeats for all 10 folds, averaging the results. For both we need a custom cost function that evaluates the performance of a binary classifier. Note that we will use the model 1 built with all observations, not the training data.\n\nlibrary(boot)\n\ncost &lt;- function(r, pi = 0) mean(abs(r - pi) &gt; 0.5)\n\n# LOOCV (accuracy)\n1 - cv.glm(epi_weather2, m_logistic, cost = cost)$delta[1]\n\n[1] 0.8041958\n\n# K-fold CV K=10 (accuracy)\n1 - cv.glm(epi_weather2, m_logistic, K = 10, cost = cost)$delta[1]\n\n[1] 0.8041958\n\n\nWe can conclude that using two variables from relatively long periods before heading, and the precipitation accumulation variable around heading (all defined based on FDA analysis) the model can reasonably predict wheat blast. We can further explore the effect of other variables from the post-heading period, as well as variables from shorter intervals around heading and flowering—periods when infections are likely to occur.\nVariable selection is a critical process that combines statistical techniques with biological knowledge to identify the most relevant predictors for a given model. Approaches can range from simple methods, like selecting the most correlated variables at periods of time know as most critical for infection or epidemic development (De Wolf et al. 2003), to more advanced algorithms designed to optimize predictive power, such as regularization techniques like lasso or elastic-net (Dalla Lana et al. 2021a; De Cól et al. 2024; De Wolf et al. 2023).\nRegularization methods not only help in selecting significant predictors but also in handling multicollinearity and reducing model complexity (Hastie et al. 2009). These methods will be demonstrated further in the following sections, where Lasso and similar techniques are discussed in detail.\n\n\n\n\nAlves, K. S., Shah, D. A., Dillard, H. R., Del Ponte, E. M., and Pethybridge, S. J. 2022. From reanalysis data to inference: A framework for linking environment to plant disease epidemics at the regional scale.\n\n\nAnalytis, S. 1977. Über die Relation zwischen biologischer Entwicklung und Temperatur bei phytopathogenen Pilzen. Journal of Phytopathology 90:64–76. https://doi.org/10.1111/j.1439-0434.1977.tb02886.x.\n\n\nBailey, J. E. 1999. Integrated Method of Organizing, Computing, and Deploying Weather-Based Disease Advisories for Selected Peanut Disease. Peanut Science 26:74–80. https://doi.org/10.3146/i0095-3679-26-2-3.\n\n\nBernard, F., Sache, I., Suffert, F., and Chelle, M. 2013. The development of a foliar fungal pathogen does react to leaf temperature! New Phytologist 198:232–240. https://doi.org/10.1111/nph.12134.\n\n\nBulger, M. A. 1987. Influence of temperature and wetness duration on infection of strawberry flowers byBotrytis cinereaand disease incidence of fruit originating from infected flowers. Phytopathology 77:1225. https://doi.org/10.1094/phyto-77-1225.\n\n\nCalvero Jr, S. B., Coakley, S. M., and Teng, P. S. 1996. Development of empirical forecasting models for rice blast based on weather factors. Plant Pathology 45:667–678. https://doi.org/10.1046/j.1365-3059.1996.d01-168.x.\n\n\nCao, X., Yao, D., Xu, X., Zhou, Y., Ding, K., Duan, X., Fan, J., and Luo, Y. 2015. Development of Weather- and Airborne Inoculum-Based Models to Describe Disease Severity of Wheat Powdery Mildew. Plant Disease 99:395–400. https://doi.org/10.1094/pdis-02-14-0201-re.\n\n\nCoakley, S. M. 1985. Model for predicting severity of septoria tritici blotch on winter wheat. Phytopathology 75:1245. https://doi.org/10.1094/phyto-75-1245.\n\n\nCoakley, S. M. 1988. Predicting stripe rust severity on winter wheat using an improved method for analyzing meteorological and rust data. Phytopathology 78:543. https://doi.org/10.1094/phyto-78-543.\n\n\nCoakley, S. M., McDaniel, L. R., and Line, R. F. 1988. Quantifying how climatic factors affect variation in plant disease severity: A general method using a new way to analyze meteorological data. Climatic Change 12:57–75. https://doi.org/10.1007/bf00140264.\n\n\nDalla Lana, F., Madden, L. V., and Paul, P. A. 2021a. Logistic Models Derived via LASSO Methods for Quantifying the Risk of Natural Contamination of Maize Grain with Deoxynivalenol. Phytopathology® 111:2250–2267. https://doi.org/10.1094/phyto-03-21-0104-r.\n\n\nDalla Lana, F., Madden, L. V., and Paul, P. A. 2021b. Natural Occurrence of Maize Gibberella Ear Rot and Contamination of Grain with Mycotoxins in Association with Weather Variables. Plant Disease 105:114–126. https://doi.org/10.1094/pdis-05-20-0952-re.\n\n\nDalla Pria, M., Christiano, R. C. S., Furtado, E. L., Amorim, L., and Bergamin Filho, A. 2006. Effect of temperature and leaf wetness duration on infection of sweet oranges by Asiatic citrus canker. Plant Pathology 55:657–663. https://doi.org/10.1111/j.1365-3059.2006.01393.x.\n\n\nDe Cól, M., Coelho, M., and Del Ponte, E. M. 2024. Weather-Based Logistic Regression Models for Predicting Wheat Head Blast Epidemics. Plant Disease 108:2206–2213. https://doi.org/10.1094/pdis-11-23-2513-re.\n\n\nDe Wolf, E. D., Andersen Onofre, K. F., and Lollato, R. P. 2023. Early Season Environmental Indicators of Wheat Stripe Rust Epidemics in Kansas and the Central Great Plains Region of the United States. Plant Disease 107:2119–2125. https://doi.org/10.1094/pdis-08-22-1873-re.\n\n\nDe Wolf, E. D., and Isard, S. A. 2007. Disease Cycle Approach to Plant Disease Prediction. Annual Review of Phytopathology 45:203–220. https://doi.org/10.1146/annurev.phyto.44.070505.143329.\n\n\nDe Wolf, E. D., Madden, L. V., and Lipps, P. E. 2003. Risk Assessment Models for Wheat Fusarium Head Blight Epidemics Based on Within-Season Weather Data. Phytopathology® 93:428–435. https://doi.org/10.1094/phyto.2003.93.4.428.\n\n\nDel Ponte, E. M., Godoy, C. V., Li, X., and Yang, X. B. 2006. Predicting Severity of Asian Soybean Rust Epidemics with Empirical Rainfall Models. Phytopathology® 96:797–803. https://doi.org/10.1094/phyto-96-0797.\n\n\nDuthie, J. A. 1997. Models of the Response of Foliar Parasites to the Combined Effects of Temperature and Duration of Wetness. Phytopathology® 87:1088–1095. https://doi.org/10.1094/phyto.1997.87.11.1088.\n\n\nEl Jarroudi, M., Kouadio, L., Bock, C. H., El Jarroudi, M., Junk, J., Pasquali, M., Maraite, H., and Delfosse, P. 2017. A Threshold-Based Weather Model for Predicting Stripe Rust Infection in Winter Wheat. Plant Disease 101:693–703. https://doi.org/10.1094/pdis-12-16-1766-re.\n\n\nEvans, K. J. 1992. A model based on temperature and leaf wetness duration for establishment of alternaria leaf blight of muskmelon. Phytopathology 82:890. https://doi.org/10.1094/phyto-82-890.\n\n\nFedele, G., Brischetto, C., Rossi, V., and Gonzalez-Dominguez, E. 2022. A Systematic Map of the Research on Disease Modelling for Agricultural Crops Worldwide. Plants 11:724. https://doi.org/10.3390/plants11060724.\n\n\nGertheiss, J., Rügamer, D., Liew, B. X. W., and Greven, S. 2024. Functional Data Analysis: An Introduction and Recent Developments. Biometrical Journal 66. https://doi.org/10.1002/bimj.202300363.\n\n\nGonzález-Domínguez, E., Caffi, T., Rossi, V., Salotti, I., and Fedele, G. 2023. Plant Disease Models and Forecasting: Changes in Principles and Applications over the Last 50 Years. Phytopathology® 113:678–693. https://doi.org/10.1094/phyto-10-22-0362-kd.\n\n\nGouache, D., Léon, M. S., Duyme, F., and Braun, P. 2015. A novel solution to the variable selection problem in Window Pane approaches of plant pathogen  Climate models: Development, evaluation and application of a climatological model for brown rust of wheat. Agricultural and Forest Meteorology 205:51–59. https://doi.org/10.1016/j.agrformet.2015.02.013.\n\n\nHarikrishnan, R., and Río, L. E. del. 2008. A Logistic Regression Model for Predicting Risk of White Mold Incidence on Dry Bean in North Dakota. Plant Disease 92:42–46. https://doi.org/10.1094/pdis-92-1-0042.\n\n\nHastie, T., Tibshirani, R., and Friedman, J. 2009. The elements of statistical learning. Springer New York. https://doi.org/10.1007/978-0-387-84858-7.\n\n\nHau, B., and Kranz, J. 1990. Mathematics and statistics for analyses in epidemiology. In Springer Berlin Heidelberg, pp. 12–52. https://doi.org/10.1007/978-3-642-75398-5_2.\n\n\nHjelkrem, A.-G. R., Aamot, H. U., Lillemo, M., Sørensen, E. S., Brodal, G., Russenes, A. L., Edwards, S. G., and Hofgaard, I. S. 2021. Weather Patterns Associated with DON Levels in Norwegian Spring Oat Grain: A Functional Data Approach. Plants 11:73. https://doi.org/10.3390/plants11010073.\n\n\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. 2021. An introduction to statistical learning with applications in r. 2nd ed. New York: Springer.\n\n\nJi, T., Languasco, L., Li, M., and Rossi, V. 2021. Effects of Temperature and Wetness Duration on Infection by Coniella diplodiella, the Fungus Causing White Rot of Grape Berries. Plants 10:1696. https://doi.org/10.3390/plants10081696.\n\n\nJi, T., Salotti, I., Altieri, V., Li, M., and Rossi, V. 2023. Temperature-Dependent Growth and Spore Germination of Fungi Causing Grapevine Trunk Diseases: Quantitative Analysis of Literature Data. Plant Disease 107:1386–1398. https://doi.org/10.1094/pdis-09-22-2249-re.\n\n\nKrause, R. A., and Massie, L. B. 1975. Predictive Systems: Modern Approaches to Disease Control. Annual Review of Phytopathology 13:31–47. https://doi.org/10.1146/annurev.py.13.090175.000335.\n\n\nKriss, A. B., Paul, P. A., and Madden, L. V. 2010. Relationship Between Yearly Fluctuations in Fusarium Head Blight Intensity and Environmental Variables: A Window-Pane Analysis. Phytopathology® 100:784–797. https://doi.org/10.1094/phyto-100-8-0784.\n\n\nLannou, C. 2012. Variation and Selection of Quantitative Traits in Plant Pathogens. Annual Review of Phytopathology 50:319–338. https://doi.org/10.1146/annurev-phyto-081211-173031.\n\n\nMacHardy, W. E. 1989. A revision of mills’s criteria for predicting apple scab infection periods. Phytopathology 79:304. https://doi.org/10.1094/phyto-79-304.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. The study of plant disease epidemics. The American Phytopathological Society. https://doi.org/10.1094/9780890545058.\n\n\nMagarey, R. D., and Sutton, T. B. 2007. How to Create and Deploy Infection Models for Plant Pathogens. In Springer Netherlands, pp. 3–25. https://doi.org/10.1007/978-1-4020-6061-8_1.\n\n\nMagarey, R. D., Sutton, T. B., and Thayer, C. L. 2005. A Simple Generic Infection Model for Foliar Fungal Plant Pathogens. Phytopathology® 95:92–100. https://doi.org/10.1094/phyto-95-0092.\n\n\nMagarey, R. D., Travis, J. W., Russo, J. M., Seem, R. C., and Magarey, P. A. 2002. Decision Support Systems: Quenching the Thirst. Plant Disease 86:4–14. https://doi.org/10.1094/pdis.2002.86.1.4.\n\n\nMehra, L. K., Cowger, C., Gross, K., and Ojiambo, P. S. 2016. Predicting pre-planting risk of stagonospora nodorum blotch in winter wheat using machine learning models. Frontiers in Plant Science 7. https://doi.org/10.3389/fpls.2016.00390.\n\n\nMehra, L. K., Cowger, C., and Ojiambo, P. S. 2017. A Model for Predicting Onset of Stagonospora nodorum Blotch in Winter Wheat Based on Preplanting and Weather Factors. Phytopathology® 107:635–644. https://doi.org/10.1094/phyto-03-16-0133-r.\n\n\nMila, A. L., Carriquiry, A. L., and Yang, X. B. 2004. Logistic Regression Modeling of Prevalence of Soybean Sclerotinia Stem Rot in the North-Central Region of the United States. Phytopathology® 94:102–110. https://doi.org/10.1094/phyto.2004.94.1.102.\n\n\nMills, W. D. 1944. Efficient use of sulfur dusts and sprays during rain to control apple scab. Cornell Extension Bulletin 630.\n\n\nMolnar, C. 2019. Interpretable machine learning. Leanpub.\n\n\nPaul, P. A., and Munkvold, G. P. 2004. A Model-Based Approach to Preplanting Risk Assessment for Gray Leaf Spot of Maize. Phytopathology® 94:1350–1357. https://doi.org/10.1094/phyto.2004.94.12.1350.\n\n\nPfender, W. F. 2003. Prediction of Stem Rust Infection Favorability, by Means of Degree-Hour Wetness Duration, for Perennial Ryegrass Seed Crops. Phytopathology® 93:467–477. https://doi.org/10.1094/phyto.2003.93.4.467.\n\n\nPietravalle, S., Shaw, M. W., Parker, S. R., and Bosch, F. van den. 2003. Modeling of Relationships Between Weather and Septoria tritici Epidemics on Winter Wheat: A Critical Approach. Phytopathology® 93:1329–1339. https://doi.org/10.1094/phyto.2003.93.10.1329.\n\n\nPini, A., and Vantini, S. 2016. The Interval Testing Procedure: A General Framework for Inference in Functional Data Analysis. Biometrics 72:835–845. https://doi.org/10.1111/biom.12476.\n\n\nRamsay, J. O., and Silverman, B. W. 2005. Functional data analysis. 2nd ed. New York: Springer.\n\n\nRomero, J., Moral, J., Gonzalez-Dominguez, E., Agustí-Brisach, C., Roca, L. F., Rossi, V., and Trapero, A. 2021. Logistic models to predict olive anthracnose under field conditions. Crop Protection 148:105714. https://doi.org/10.1016/j.cropro.2021.105714.\n\n\nRossi, V., Caffi, T., Giosuè, S., and Bugiani, R. 2008. A mechanistic model simulating primary infections of downy mildew in grapevine. Ecological Modelling 212:480–491. https://doi.org/10.1016/j.ecolmodel.2007.10.046.\n\n\nRossi, V., Giosuè, S., and Caffi, T. 2010. Modelling Plant Diseases for Decision Making in Crop Protection. In Springer Netherlands, pp. 241–258. https://doi.org/10.1007/978-90-481-9277-9_15.\n\n\nRossi, V., Onesti, G., Legler, S. E., and Caffi, T. 2014. Use of systems analysis to develop plant disease models based on literature data: grape black-rot as a case-study. European Journal of Plant Pathology 141:427–444. https://doi.org/10.1007/s10658-014-0553-z.\n\n\nRotem, J. 1988. Techniques of Controlled-Condition Experiments. In Springer Berlin Heidelberg, pp. 19–31. https://doi.org/10.1007/978-3-642-95534-1_3.\n\n\nSalotti, I., Bove, F., and Rossi, V. 2022. Development and validation of a mechanistic, weather-based model for predicting puccinia graminis f. Sp. Tritici infections and stem rust progress in wheat. Frontiers in Plant Science 13. https://doi.org/10.3389/fpls.2022.897680.\n\n\nSalotti, I., and Rossi, V. 2023. A Mechanistic Model Accounting for the Effect of Soil Moisture, Weather, and Host Growth Stage on the Development of Sclerotinia sclerotiorum. Plant Disease 107:514–533. https://doi.org/10.1094/pdis-12-21-2743-re.\n\n\nSavary, S., Nelson, A. D., Djurle, A., Esker, P. D., Sparks, A., Amorim, L., Bergamin Filho, A., Caffi, T., Castilla, N., Garrett, K., McRoberts, N., Rossi, V., Yuen, J., and Willocquet, L. 2018. Concepts, approaches, and avenues for modelling crop health and crop losses. European Journal of Agronomy 100:4–18. https://doi.org/10.1016/j.eja.2018.04.003.\n\n\nSeem, R. C. 1984. Simple decision aids for practical control of pests. Plant Disease 68:656. https://doi.org/10.1094/pd-69-656.\n\n\nShah, D. A., De Wolf, E. D., Paul, P. A., and Madden, L. V. 2019a. Functional Data Analysis of Weather Variables Linked to Fusarium Head Blight Epidemics in the United States. Phytopathology® 109:96–110. https://doi.org/10.1094/phyto-11-17-0386-r.\n\n\nShah, D. A., Molineros, J. E., Paul, P. A., Willyerd, K. T., Madden, L. V., and De Wolf, E. D. 2013. Predicting Fusarium Head Blight Epidemics With Weather-Driven Pre- and Post-Anthesis Logistic Regression Models. Phytopathology® 103:906–919. https://doi.org/10.1094/phyto-11-12-0304-r.\n\n\nShah, D. A., Paul, P. A., De Wolf, E. D., and Madden, L. V. 2019b. Predicting plant disease epidemics from functionally represented weather series. Philosophical Transactions of the Royal Society B: Biological Sciences 374:20180273. https://doi.org/10.1098/rstb.2018.0273.\n\n\nTe Beest, D. E., Paveley, N. D., Shaw, M. W., and Bosch, F. van den. 2008. DiseaseWeather Relationships for Powdery Mildew and Yellow Rust on Winter Wheat. Phytopathology® 98:609–617. https://doi.org/10.1094/phyto-98-5-0609.\n\n\nVanderplank, J. 1963. Plant disease epidemics and control. Elsevier. https://doi.org/10.1016/c2013-0-11642-x.\n\n\nWillbur, J. F., Fall, M. L., Bloomingdale, C., Byrne, A. M., Chapman, S. A., Isard, S. A., Magarey, R. D., McCaghey, M. M., Mueller, B. D., Russo, J. M., Schlegel, J., Chilvers, M. I., Mueller, D. S., Kabbage, M., and Smith, D. L. 2018. Weather-Based Models for Assessing the Risk of Sclerotinia sclerotiorum Apothecial Presence in Soybean (Glycine max) Fields. Plant Disease 102:73–84. https://doi.org/10.1094/pdis-04-17-0504-re.\n\n\nWindels, C. E., Lamey, H. A., Hilde, D., Widner, J., and Knudsen, T. 1998. A Cerospora Leaf Spot Model for Sugar Beet: In Practice by an Industry. Plant Disease 82:716–726. https://doi.org/10.1094/pdis.1998.82.7.716.\n\n\nXu, X., Madden, L. V., Edwards, S. G., Doohan, F. M., Moretti, A., Hornok, L., Nicholson, P., and Ritieni, A. 2013. Developing logistic models to relate the accumulation of DON associated with Fusarium head blight to climatic conditions in Europe. European Journal of Plant Pathology 137:689–706. https://doi.org/10.1007/s10658-013-0280-x.\n\n\nXu, X.-M., and Robinson, J. D. 2000. Effects of temperature on the incubation and latent periods of hawthorn powdery mildew (Podosphaera clandestina). Plant Pathology 49:791–797. https://doi.org/10.1046/j.1365-3059.2000.00520.x.",
    "crumbs": [
      "Disease prediction",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Disease modeling</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agrios, G. N. 2005a. INTRODUCTION. In Elsevier, pp. 3–75. https://doi.org/10.1016/b978-0-08-047378-9.50007-5.\n\n\nAgrios, G. N. 2005b. Plant disease epidemiology. In Elsevier, pp.\n265–291. https://doi.org/10.1016/b978-0-08-047378-9.50014-2.\n\n\nAlves, K. S., and Del Ponte, E. M. 2021. Analysis and simulation of\nplant disease progress curves in R: introducing the epifitter package.\nPhytopathology Research 3. https://doi.org/10.1186/s42483-021-00098-7.\n\n\nAlves, K. S., Guimarães, M., Ascari, J. P., Queiroz, M. F., Alfenas, R.\nF., Mizubuti, E. S. G., and Del Ponte, E. M. 2021. RGB-based phenotyping\nof foliar disease severity under controlled conditions. Tropical Plant\nPathology 47:105–117. https://doi.org/10.1007/s40858-021-00448-y.\n\n\nAlves, K. S., Shah, D. A., Dillard, H. R., Del Ponte, E. M., and\nPethybridge, S. J. 2022. From reanalysis data to\ninference: A framework for linking environment to plant disease\nepidemics at the regional scale.\n\n\nAnalytis, S. 1977. Über die Relation zwischen biologischer Entwicklung\nund Temperatur bei phytopathogenen Pilzen. Journal of Phytopathology\n90:64–76. https://doi.org/10.1111/j.1439-0434.1977.tb02886.x.\n\n\nBaddeley, A., Diggle, P. J., Hardegen, A., Lawrence, T., Milne, R. K.,\nand Nair, G. 2014. On tests of spatial pattern based on simulation\nenvelopes. Ecological Monographs 84:477–489. https://doi.org/10.1890/13-2042.1.\n\n\nBaddeley, A., Turner, R., Moller, J., and Hazelton, M. 2005. Residual\nanalysis for spatial point processes (with discussion). Journal of the\nRoyal Statistical Society: Series B (Statistical Methodology)\n67:617–666. https://doi.org/10.1111/j.1467-9868.2005.00519.x.\n\n\nBailey, J. E. 1999. Integrated Method of Organizing, Computing, and\nDeploying Weather-Based Disease Advisories for Selected Peanut Disease.\nPeanut Science 26:74–80. https://doi.org/10.3146/i0095-3679-26-2-3.\n\n\nBarnhart, H. X., Haber, M., and Song, J. 2002. Overall Concordance\nCorrelation Coefficient for Evaluating Agreement Among Multiple\nObservers. Biometrics 58:1020–1027. https://doi.org/10.1111/j.0006-341x.2002.01020.x.\n\n\nBates, D., Mächler, M., Bolker, B., and Walker, S. 2015. Fitting Linear\nMixed-Effects Models Usinglme4. Journal of\nStatistical Software 67. https://doi.org/10.18637/jss.v067.i01.\n\n\nBernard, F., Sache, I., Suffert, F., and Chelle, M. 2013. The\ndevelopment of a foliar fungal pathogen does react to leaf temperature!\nNew Phytologist 198:232–240. https://doi.org/10.1111/nph.12134.\n\n\nBock, C. H., Chiang, K.-S., and Del Ponte, E. M. 2021a. Plant disease\nseverity estimated visually: a century of research, best practices, and\nopportunities for improving methods and practices to maximize accuracy.\nTropical Plant Pathology 47:25–42. https://doi.org/10.1007/s40858-021-00439-z.\n\n\nBock, C. H., Pethybridge, S. J., Barbedo, J. G. A., Esker, P. D.,\nMahlein, A.-K., and Del Ponte, E. M. 2021b. A phytopathometry glossary\nfor the twenty-first century: towards consistency and precision in\nintra- and inter-disciplinary dialogues. Tropical Plant Pathology\n47:14–24. https://doi.org/10.1007/s40858-021-00454-0.\n\n\nBourke, P. M. A. 1970. Use of Weather Information in the Prediction of\nPlant Disease Epiphytotics. Annual Review of Phytopathology 8:345–370.\nhttps://doi.org/10.1146/annurev.py.08.090170.002021.\n\n\nBrooks, M. E., Kristensen, K., van Benthem, K. J., Magnusson, A., Berg,\nC. W., Nielsen, A., Skaug, H. J., Maechler, M., and Bolker, B. M. 2017.\nglmmTMB balances speed and flexibility among\npackages for zero-inflated generalized linear mixed modeling. The R\nJournal 9:378–400. https://doi.org/10.32614/RJ-2017-066.\n\n\nBrown, V. A. 2021. An Introduction to Linear Mixed-Effects Modeling in\nR. Advances in Methods and Practices in Psychological Science\n4:251524592096035. https://doi.org/10.1177/2515245920960351.\n\n\nBulger, M. A. 1987. Influence of temperature and wetness duration on\ninfection of strawberry flowers byBotrytis\ncinereaand disease incidence of fruit originating from\ninfected flowers. Phytopathology 77:1225. https://doi.org/10.1094/phyto-77-1225.\n\n\nCafé-Filho, A. C., Santos, G. R., and Laranjeira, F. F. 2010. Temporal\nand spatial dynamics of watermelon gummy stem blight epidemics. European\nJournal of Plant Pathology 128:473–482. https://doi.org/10.1007/s10658-010-9674-1.\n\n\nCaffi, T., Rossi, V., Cossu, A., and Fronteddu, F. 2007. Empirical vs.\nmechanistic models for primary infections of  Plasmopara\nviticola*. EPPO Bulletin 37:261–271. https://doi.org/10.1111/j.1365-2338.2007.01120.x.\n\n\nCaffi, T., Rossi, V., Legler, S. E., and Bugiani, R. 2010. A mechanistic\nmodel simulating ascosporic infections by Erysiphe\nnecator, the powdery mildew fungus of grapevine. Plant\nPathology 60:522–531. https://doi.org/10.1111/j.1365-3059.2010.02395.x.\n\n\nCalvero Jr, S. B., Coakley, S. M., and Teng, P. S. 1996. Development of\nempirical forecasting models for rice blast based on weather factors.\nPlant Pathology 45:667–678. https://doi.org/10.1046/j.1365-3059.1996.d01-168.x.\n\n\nCampbell, C. L., and Madden. L., V. 1990. Introduction to plant\ndisease epidemiology. Wiley.\n\n\nCao, X., Yao, D., Xu, X., Zhou, Y., Ding, K., Duan, X., Fan, J., and\nLuo, Y. 2015. Development of Weather- and Airborne Inoculum-Based Models\nto Describe Disease Severity of Wheat Powdery Mildew. Plant Disease\n99:395–400. https://doi.org/10.1094/pdis-02-14-0201-re.\n\n\nChester, K. S. 1950. Plant disease losses : Their appraisal and\ninterpretation /. https://doi.org/10.5962/bhl.title.86198.\n\n\nChiang, K.-S., and Bock, C. H. 2021. Understanding the ramifications of\nquantitative ordinal scales on accuracy of estimates of disease severity\nand data analysis in plant pathology. Tropical Plant Pathology 47:58–73.\nhttps://doi.org/10.1007/s40858-021-00446-0.\n\n\nChiang, K.-S., Chang, Y. M., Liu, H. I., Lee, J. Y., El Jarroudi, M.,\nand Bock, C. 2023. Survival Analysis as a Basis to Test Hypotheses When\nUsing Quantitative Ordinal Scale Disease Severity Data. Phytopathology®.\nhttps://doi.org/10.1094/phyto-02-23-0055-r.\n\n\nChiang, K.-S., Liu, S.-C., Bock, C. H., and Gottwald, T. R. 2014. What\nInterval Characteristics Make a Good Categorical Disease Assessment\nScale? Phytopathology® 104:575–585. https://doi.org/10.1094/phyto-10-13-0279-r.\n\n\nCoakley, S. M. 1985. Model for predicting severity of septoria tritici\nblotch on winter wheat. Phytopathology 75:1245. https://doi.org/10.1094/phyto-75-1245.\n\n\nCoakley, S. M. 1988. Predicting stripe rust severity on winter wheat\nusing an improved method for analyzing meteorological and rust data.\nPhytopathology 78:543. https://doi.org/10.1094/phyto-78-543.\n\n\nCoakley, S. M., McDaniel, L. R., and Line, R. F. 1988. Quantifying how\nclimatic factors affect variation in plant disease severity: A general\nmethod using a new way to analyze meteorological data. Climatic Change\n12:57–75. https://doi.org/10.1007/bf00140264.\n\n\nCruz, C. D., and Valent, B. 2017. Wheat blast disease: danger on the\nmove. Tropical Plant Pathology 42:210–222. https://doi.org/10.1007/s40858-017-0159-z.\n\n\nDalla Lana, F., Madden, L. V., and Paul, P. A. 2021a. Logistic Models\nDerived via LASSO Methods for Quantifying the Risk of Natural\nContamination of Maize Grain with Deoxynivalenol. Phytopathology®\n111:2250–2267. https://doi.org/10.1094/phyto-03-21-0104-r.\n\n\nDalla Lana, F., Madden, L. V., and Paul, P. A. 2021b. Natural Occurrence\nof Maize Gibberella Ear Rot and Contamination of Grain with Mycotoxins\nin Association with Weather Variables. Plant Disease 105:114–126. https://doi.org/10.1094/pdis-05-20-0952-re.\n\n\nDalla Lana, F., Ziegelmann, P. K., Maia, A. de H. N., Godoy, C. V., and\nDel Ponte, E. M. 2015. Meta-Analysis of the Relationship Between Crop\nYield and Soybean Rust Severity. Phytopathology® 105:307–315. https://doi.org/10.1094/phyto-06-14-0157-r.\n\n\nDalla Pria, M., Christiano, R. C. S., Furtado, E. L., Amorim, L., and\nBergamin Filho, A. 2006. Effect of temperature and leaf wetness duration\non infection of sweet oranges by Asiatic citrus canker. Plant Pathology\n55:657–663. https://doi.org/10.1111/j.1365-3059.2006.01393.x.\n\n\nDe Cól, M., Coelho, M., and Del Ponte, E. M. 2024. Weather-Based\nLogistic Regression Models for Predicting Wheat Head Blast Epidemics.\nPlant Disease 108:2206–2213. https://doi.org/10.1094/pdis-11-23-2513-re.\n\n\nDe Rossi, R. L., Guerra, F. A., Plazas, M. C., Vuletic, E. E., Brücher,\nE., Guerra, G. D., and Reis, E. M. 2022. Crop damage, economic losses,\nand the economic damage threshold for northern corn leaf blight. Crop\nProtection 154:105901. https://doi.org/10.1016/j.cropro.2021.105901.\n\n\nDe Wolf, E. D., Andersen Onofre, K. F., and Lollato, R. P. 2023. Early\nSeason Environmental Indicators of Wheat Stripe Rust Epidemics in Kansas\nand the Central Great Plains Region of the United States. Plant Disease\n107:2119–2125. https://doi.org/10.1094/pdis-08-22-1873-re.\n\n\nDe Wolf, E. D., and Isard, S. A. 2007. Disease Cycle Approach to Plant\nDisease Prediction. Annual Review of Phytopathology 45:203–220. https://doi.org/10.1146/annurev.phyto.44.070505.143329.\n\n\nDe Wolf, E. D., Madden, L. V., and Lipps, P. E. 2003. Risk Assessment\nModels for Wheat Fusarium Head Blight Epidemics Based on Within-Season\nWeather Data. Phytopathology® 93:428–435. https://doi.org/10.1094/phyto.2003.93.4.428.\n\n\nDel Ponte, E. M., Cazón, L. I., Alves, K. S., Pethybridge, S. J., and\nBock, C. H. 2022. How much do standard area diagrams improve accuracy of\nvisual estimates of the percentage area diseased? A systematic review\nand meta-analysis. Tropical Plant Pathology 47:43–57. https://doi.org/10.1007/s40858-021-00479-5.\n\n\nDel Ponte, E. M., Fernandes, J. M. C., and Pavan, W. 2005. A risk\ninfection simulation model for fusarium head blight of wheat.\nFitopatologia Brasileira 30:634–642. https://doi.org/10.1590/s0100-41582005000600011.\n\n\nDel Ponte, E. M., Godoy, C. V., Li, X., and Yang, X. B. 2006. Predicting\nSeverity of Asian Soybean Rust Epidemics with Empirical Rainfall Models.\nPhytopathology® 96:797–803. https://doi.org/10.1094/phyto-96-0797.\n\n\nDel Ponte, E. M., Mahlein, A.-K., and Bock, C. H. 2024. Plant disease\nquantification. In Elsevier, pp. 211–225. https://doi.org/10.1016/b978-0-12-822429-8.00006-6.\n\n\nDel Ponte, E. M., Nelson, S. C., and Pethybridge, S. J. 2019. Evaluation\nof App-Embedded Disease Scales for Aiding Visual Severity Estimation of\nCercospora Leaf Spot of Table Beet. Plant Disease 103:1347–1356. https://doi.org/10.1094/pdis-10-18-1718-re.\n\n\nDel Ponte, E. M., Pethybridge, S. J., Bock, C. H., Michereff, S. J.,\nMachado, F. J., and Spolti, P. 2017. Standard Area Diagrams for Aiding\nSeverity Estimation: Scientometrics, Pathosystems, and Methodological\nTrends in the Last 25 Years. Phytopathology® 107:1161–1174. https://doi.org/10.1094/phyto-02-17-0069-fi.\n\n\nDuffeck, M. R., Santos Alves, K. dos, Machado, F. J., Esker, P. D., and\nDel Ponte, E. M. 2020. Modeling Yield Losses and Fungicide Profitability\nfor Managing Fusarium Head Blight in Brazilian Spring Wheat.\nPhytopathology® 110:370–378. https://doi.org/10.1094/phyto-04-19-0122-r.\n\n\nDuthie, J. A. 1997. Models of the Response of Foliar Parasites to the\nCombined Effects of Temperature and Duration of Wetness. Phytopathology®\n87:1088–1095. https://doi.org/10.1094/phyto.1997.87.11.1088.\n\n\nEl Jarroudi, M., Kouadio, L., Bock, C. H., El Jarroudi, M., Junk, J.,\nPasquali, M., Maraite, H., and Delfosse, P. 2017. A Threshold-Based\nWeather Model for Predicting Stripe Rust Infection in Winter Wheat.\nPlant Disease 101:693–703. https://doi.org/10.1094/pdis-12-16-1766-re.\n\n\nEsser, D. S., Leveau, J. H. J., Meyer, K. M., and Wiegand, K. 2014.\nSpatial scales of interactions among bacteria and between bacteria and\nthe leaf surface. FEMS Microbiology Ecology 91. https://doi.org/10.1093/femsec/fiu034.\n\n\nEvans, K. J. 1992. A model based on temperature and leaf wetness\nduration for establishment of alternaria leaf blight of muskmelon.\nPhytopathology 82:890. https://doi.org/10.1094/phyto-82-890.\n\n\nFedele, G., Brischetto, C., Rossi, V., and Gonzalez-Dominguez, E. 2022.\nA Systematic Map of the Research on Disease Modelling for Agricultural\nCrops Worldwide. Plants 11:724. https://doi.org/10.3390/plants11060724.\n\n\nFranceschi, V. T., Alves, K. S., Mazaro, S. M., Godoy, C. V., Duarte, H.\nS. S., and Del Ponte, E. M. 2020. A new standard area diagram set for\nassessment of severity of soybean rust improves accuracy of estimates\nand optimizes resource use. Plant Pathology 69:495–505. https://doi.org/10.1111/ppa.13148.\n\n\nFrancl, L. J. 2001. The..disease triangle: A plant pathological paradigm\nrevisited. The Plant Health Instructor. https://doi.org/10.1094/phi-t-2001-0517-01.\n\n\nGertheiss, J., Rügamer, D., Liew, B. X. W., and Greven, S. 2024.\nFunctional Data Analysis: An Introduction and Recent Developments.\nBiometrical Journal 66. https://doi.org/10.1002/bimj.202300363.\n\n\nGigot, C. 2018. Epiphy: Analysis of plant disease epidemics.\n\n\nGodoy, C. V., Seixas, C. D. S., Soares, R. M., Marcelino-Guimarães, F.\nC., Meyer, M. C., and Costamilan, L. M. 2016. Asian soybean rust in\nbrazil: Past, present, and future. Pesquisa Agropecuária Brasileira\n51:407–421. https://doi.org/10.1590/s0100-204x2016000500002.\n\n\nGonzález-Domínguez, E., Caffi, T., Rossi, V., Salotti, I., and Fedele,\nG. 2023. Plant Disease Models and Forecasting: Changes in Principles and\nApplications over the Last 50 Years. Phytopathology® 113:678–693. https://doi.org/10.1094/phyto-10-22-0362-kd.\n\n\nGonzález-Domínguez, E., Martins, R. B., Del Ponte, E. M., Michereff, S.\nJ., García-Jiménez, J., and Armengol, J. 2014. Development and\nvalidation of a standard area diagram set to aid assessment of severity\nof loquat scab on fruit. European Journal of Plant Pathology. https://doi.org/10.1007/s10658-014-0400-2.\n\n\nGouache, D., Léon, M. S., Duyme, F., and Braun, P. 2015. A novel\nsolution to the variable selection problem in Window Pane approaches of\nplant pathogen  Climate models: Development, evaluation and\napplication of a climatological model for brown rust of wheat.\nAgricultural and Forest Meteorology 205:51–59. https://doi.org/10.1016/j.agrformet.2015.02.013.\n\n\nHank, T. B., Berger, K., Bach, H., Clevers, J. G. P. W., Gitelson, A.,\nZarco-Tejada, P., and Mauser, W. 2018. Spaceborne Imaging Spectroscopy\nfor Sustainable Agriculture: Contributions and Challenges. Surveys in\nGeophysics 40:515–551. https://doi.org/10.1007/s10712-018-9492-0.\n\n\nHarikrishnan, R., and Río, L. E. del. 2008. A Logistic Regression Model\nfor Predicting Risk of White Mold Incidence on Dry Bean in North Dakota.\nPlant Disease 92:42–46. https://doi.org/10.1094/pdis-92-1-0042.\n\n\nHastie, T., Tibshirani, R., and Friedman, J. 2009. The elements of\nstatistical learning. Springer New York. https://doi.org/10.1007/978-0-387-84858-7.\n\n\nHau, B., and Kranz, J. 1990. Mathematics and statistics for analyses in\nepidemiology. In Springer Berlin Heidelberg, pp. 12–52. https://doi.org/10.1007/978-3-642-75398-5_2.\n\n\nHebert, T. T. 1982. The rationale for the horsfall-barratt plant disease\nassessment scale. Phytopathology 72:1269. https://doi.org/10.1094/phyto-72-1269.\n\n\nHjelkrem, A.-G. R., Aamot, H. U., Lillemo, M., Sørensen, E. S., Brodal,\nG., Russenes, A. L., Edwards, S. G., and Hofgaard, I. S. 2021. Weather\nPatterns Associated with DON Levels in Norwegian Spring Oat Grain: A\nFunctional Data Approach. Plants 11:73. https://doi.org/10.3390/plants11010073.\n\n\nHughes, G., and Madden, L. V. 1992. Aggregation and incidence of\ndisease. Plant Pathology 41:657–660. https://doi.org/10.1111/j.1365-3059.1992.tb02549.x.\n\n\nHuichun YE, Senzheng CHEN, Anting GUO, Chaojia NIE, and Jingjing WANG.\n2022. A dataset of UAV multispectral images for banana Fusarium wilt\nsurvey. https://doi.org/10.57760/SCIENCEDB.07000.\n\n\nIslam, M. T., Kim, K.-H., and Choi, J. 2019. Wheat Blast in Bangladesh:\nThe Current Situation and Future Impacts. The Plant Pathology Journal\n35:1–10. https://doi.org/10.5423/ppj.rw.08.2018.0168.\n\n\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. 2021. An\nintroduction to statistical learning with applications in r. 2nd\ned. New York: Springer.\n\n\nJeger, M. J., and Viljanen-Rollinson, S. L. H. 2001. The use of the area\nunder the disease-progress curve (AUDPC) to assess quantitative disease\nresistance in crop cultivars. Theoretical and Applied Genetics\n102:32–40. https://doi.org/10.1007/s001220051615.\n\n\nJesus Junior, W. C. de, and Bassanezi, R. B. 2004. Análise da dinâmica e\nestrutura de focos da morte súbita dos citros. Fitopatologia Brasileira\n29:399–405. https://doi.org/10.1590/s0100-41582004000400007.\n\n\nJi, T., Languasco, L., Li, M., and Rossi, V. 2021. Effects of\nTemperature and Wetness Duration on Infection by Coniella diplodiella,\nthe Fungus Causing White Rot of Grape Berries. Plants 10:1696. https://doi.org/10.3390/plants10081696.\n\n\nJi, T., Salotti, I., Altieri, V., Li, M., and Rossi, V. 2023.\nTemperature-Dependent Growth and Spore Germination of Fungi Causing\nGrapevine Trunk Diseases: Quantitative Analysis of Literature Data.\nPlant Disease 107:1386–1398. https://doi.org/10.1094/pdis-09-22-2249-re.\n\n\nJones, H. G., and Vaughan, R. A. 2010. Remote sensing of vegetation:\nPrinciples, techniques, and applications. Oxford, United Kingdom:\nOxford University Press.\n\n\nKouadio, L., El Jarroudi, M., Belabess, Z., Laasli, S.-E., Roni, M. Z.\nK., Amine, I. D. I., Mokhtari, N., Mokrini, F., Junk, J., and Lahlali,\nR. 2023. A Review on UAV-Based Applications for Plant Disease Detection\nand Monitoring. Remote Sensing 15:4273. https://doi.org/10.3390/rs15174273.\n\n\nKrause, R. A., and Massie, L. B. 1975. Predictive Systems: Modern\nApproaches to Disease Control. Annual Review of Phytopathology 13:31–47.\nhttps://doi.org/10.1146/annurev.py.13.090175.000335.\n\n\nKrause, R. A., Massie, L. B., and Hyre, R. A. 1975. BLITECAST: A\ncomputerized forecast of potato late blight. The Plant Disease Reporter\n59:95.\n\n\nKriss, A. B., Paul, P. A., and Madden, L. V. 2010. Relationship Between\nYearly Fluctuations in Fusarium Head Blight Intensity and Environmental\nVariables: A Window-Pane Analysis. Phytopathology® 100:784–797. https://doi.org/10.1094/phyto-100-8-0784.\n\n\nLannou, C. 2012. Variation and Selection of Quantitative Traits in Plant\nPathogens. Annual Review of Phytopathology 50:319–338. https://doi.org/10.1146/annurev-phyto-081211-173031.\n\n\nLaranjeira, F. F., Bergamin Filho, A. R., and Amorim, L. I. 1998.\nDinâmica e estrutura de focos da clorose variegada dos\ncitros (CVC). Fitopatologia Brasileira 23:36–41.\n\n\nLaranjeira, F. F., Bergamin Filho, A., Amorim, L., and Gottwald, T. R.\n2004. Dinâmica espacial da clorose variegada dos citros em três regiões\ndo estado de são paulo. Fitopatologia Brasileira 29:56–65. https://doi.org/10.1590/s0100-41582004000100009.\n\n\nLehner, M. S., Pethybridge, S. J., Meyer, M. C., and Del Ponte, E. M.\n2016. Meta-analytic modelling of the\nincidenceyield and incidencesclerotial\nproduction relationships in soybean white mould epidemics. Plant\nPathology 66:460–468. https://doi.org/10.1111/ppa.12590.\n\n\nLeiminger, J. H., and Hausladen, H. 2012. Early Blight Control in Potato\nUsing Disease-Orientated Threshold Values. Plant Disease 96:124–130. https://doi.org/10.1094/pdis-05-11-0431.\n\n\nLi, B., Madden, L. V., and Xu, X. 2011. Spatial analysis by distance\nindices: an alternative local clustering index for studying spatial\npatterns. Methods in Ecology and Evolution 3:368–377. https://doi.org/10.1111/j.2041-210x.2011.00165.x.\n\n\nLi, F., Upadhyaya, N. M., Sperschneider, J., Matny, O., Nguyen-Phuc, H.,\nMago, R., Raley, C., Miller, M. E., Silverstein, K. A. T., Henningsen,\nE., Hirsch, C. D., Visser, B., Pretorius, Z. A., Steffenson, B. J.,\nSchwessinger, B., Dodds, P. N., and Figueroa, M. 2019. Emergence of the\nUg99 lineage of the wheat stem rust pathogen through somatic\nhybridisation. Nature Communications 10. https://doi.org/10.1038/s41467-019-12927-7.\n\n\nLin, L. I.-K. 1989. A concordance correlation coefficient to evaluate\nreproducibility. Biometrics 45:255. https://doi.org/10.2307/2532051.\n\n\nLiu, H. I., Tsai, J. R., Chung, W. H., Bock, C. H., and Chiang, K. S.\n2019. Effects of Quantitative Ordinal Scale Design on the Accuracy of\nEstimates of Mean Disease Severity. Agronomy 9:565. https://doi.org/10.3390/agronomy9090565.\n\n\nLowe, A., Harrison, N., and French, A. P. 2017. Hyperspectral image\nanalysis techniques for the detection and classification of the early\nonset of plant disease and stress. Plant Methods 13. https://doi.org/10.1186/s13007-017-0233-z.\n\n\nMacHardy, W. E. 1989. A revision of mills’s criteria for predicting\napple scab infection periods. Phytopathology 79:304. https://doi.org/10.1094/phyto-79-304.\n\n\nMadden, L. 1978. FAST, a forecast system for alternaria solani on\ntomato. Phytopathology 68:1354. https://doi.org/10.1094/phyto-68-1354.\n\n\nMadden, L. V. 1982. Evaluation of tests for randomness of infected\nplants. Phytopathology 72:195. https://doi.org/10.1094/phyto-72-195.\n\n\nMadden, L. V., Esker, P. D., and Pethybridge, S. J. 2021. Forrest W.\nNutter, Jr.: a career in phytopathometry. Tropical Plant Pathology\n47:5–13. https://doi.org/10.1007/s40858-021-00469-7.\n\n\nMadden, L. V., Hughes, G., and Bosch, F. van den, eds. 2007a. CHAPTER\n12: Epidemics and crop yield. In The American Phytopathological Society,\npp. 353–388. https://doi.org/10.1094/9780890545058.012.\n\n\nMadden, L. V., Hughes, G., Moraes, W. B., Xu, X.-M., and Turechek, W. W.\n2018. Twenty-Five Years of the Binary Power Law for Characterizing\nHeterogeneity of Disease Incidence. Phytopathology® 108:656–680. https://doi.org/10.1094/phyto-07-17-0234-rvw.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007b. Spatial aspects\nof epidemicsIII: Patterns of plant disease. In The American\nPhytopathological Society, pp. 235–278. https://doi.org/10.1094/9780890545058.009.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007c. Temporal\nanalysis i: Quantifying and comparing epidemics. In The American\nPhytopathological Society, pp. 63–116. https://doi.org/10.1094/9780890545058.004.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007d. The study of\nplant disease epidemics. The American Phytopathological Society. https://doi.org/10.1094/9780890545058.\n\n\nMadden, L. V., and Paul, P. A. 2009. Assessing Heterogeneity in the\nRelationship Between Wheat Yield and Fusarium Head Blight Intensity\nUsing Random-Coefficient Mixed Models. Phytopathology® 99:850–860. https://doi.org/10.1094/phyto-99-7-0850.\n\n\nMadden, L. V., and Paul, P. A. 2011. Meta-Analysis for Evidence\nSynthesis in Plant Pathology: An Overview. Phytopathology® 101:16–30. https://doi.org/10.1094/phyto-03-10-0069.\n\n\nMagarey, R. D., and Sutton, T. B. 2007. How to Create and Deploy\nInfection Models for Plant Pathogens. In Springer Netherlands, pp. 3–25.\nhttps://doi.org/10.1007/978-1-4020-6061-8_1.\n\n\nMagarey, R. D., Sutton, T. B., and Thayer, C. L. 2005. A Simple Generic\nInfection Model for Foliar Fungal Plant Pathogens. Phytopathology®\n95:92–100. https://doi.org/10.1094/phyto-95-0092.\n\n\nMagarey, R. D., Travis, J. W., Russo, J. M., Seem, R. C., and Magarey,\nP. A. 2002. Decision Support Systems: Quenching the Thirst. Plant\nDisease 86:4–14. https://doi.org/10.1094/pdis.2002.86.1.4.\n\n\nMalaker, P. K., Barma, N. C. D., Tiwari, T. P., Collis, W. J.,\nDuveiller, E., Singh, P. K., Joshi, A. K., Singh, R. P., Braun, H.-J.,\nPeterson, G. L., Pedley, K. F., Farman, M. L., and Valent, B. 2016.\nFirst Report of Wheat Blast Caused by Magnaporthe\noryzae Pathotype triticum in\nBangladesh. Plant Disease 100:2330–2330. https://doi.org/10.1094/pdis-05-16-0666-pdn.\n\n\nMehra, L. K., Cowger, C., Gross, K., and Ojiambo, P. S. 2016. Predicting\npre-planting risk of stagonospora nodorum blotch in winter wheat using\nmachine learning models. Frontiers in Plant Science 7. https://doi.org/10.3389/fpls.2016.00390.\n\n\nMehra, L. K., Cowger, C., and Ojiambo, P. S. 2017. A Model for\nPredicting Onset of Stagonospora nodorum Blotch in Winter Wheat Based on\nPreplanting and Weather Factors. Phytopathology® 107:635–644. https://doi.org/10.1094/phyto-03-16-0133-r.\n\n\nMikaberidze, A., Mundt, C. C., and Bonhoeffer, S. 2015. Data from:\nInvasiveness of plant pathogens depends on the spatial scale of host\ndistribution. https://doi.org/10.5061/DRYAD.F2J8S.\n\n\nMila, A. L., Carriquiry, A. L., and Yang, X. B. 2004. Logistic\nRegression Modeling of Prevalence of Soybean Sclerotinia Stem Rot in the\nNorth-Central Region of the United States. Phytopathology® 94:102–110.\nhttps://doi.org/10.1094/phyto.2004.94.1.102.\n\n\nMills, W. D. 1944. Efficient use of sulfur dusts and sprays during rain\nto control apple scab. Cornell Extension Bulletin 630.\n\n\nMolnar, C. 2019. Interpretable machine learning. Leanpub.\n\n\nMoran, P. A. P. 1950. Notes on continuous stochastic phenomena.\nBiometrika 37:17. https://doi.org/10.2307/2332142.\n\n\nMoreira, R. R., Silva Silveira Duarte, H. da, and De Mio, L. L. M. 2018.\nImproving accuracy, precision and reliability of severity estimates of\nGlomerella leaf spot on apple leaves using a new standard area diagram\nset. European Journal of Plant Pathology 153:975–982. https://doi.org/10.1007/s10658-018-01610-0.\n\n\nMumford, J. D., and Norton, G. A. 1984. Economics of Decision Making in\nPest Management. Annual Review of Entomology 29:157–174. https://doi.org/10.1146/annurev.en.29.010184.001105.\n\n\nMundt, C. C., Ahmed, H. U., Finckh, M. R., Nieva, L. P., and Alfonso, R.\nF. 1999. Primary Disease Gradients of Bacterial Blight of Rice.\nPhytopathology® 89:64–67. https://doi.org/10.1094/phyto.1999.89.1.64.\n\n\nNelson, S. C. 1996. A simple analysis of disease foci. Phytopathology\n86:432–439.\n\n\nNutter, F. W., and Esker, P. D. 2006. The Role of Psychophysics in\nPhytopathology: The WeberFechner Law Revisited. European\nJournal of Plant Pathology 114:199–213. https://doi.org/10.1007/s10658-005-4732-9.\n\n\nNutter, F. W., Esker, P. D., and Netto, R. A. C. 2006. Disease\nAssessment Concepts and the Advancements Made in Improving the Accuracy\nand Precision of Plant Disease Data. European Journal of Plant Pathology\n115:95–103. https://doi.org/10.1007/s10658-005-1230-z.\n\n\nNutter, F., Teng, P., and Royer, M. 1993. Terms and concepts for yield,\ncrop loss, and disease thresholds. Plant Disease 77:193–211.\n\n\nOerke, E.-C. 2020. Remote Sensing of Diseases. Annual Review of\nPhytopathology 58:225–252. https://doi.org/10.1146/annurev-phyto-010820-012832.\n\n\nOlivoto, T. 2022. Lights, camera, pliman! An R package for plant image\nanalysis. Methods in Ecology and Evolution 13:789–798. https://doi.org/10.1111/2041-210x.13803.\n\n\nOlivoto, T., Andrade, S. M. P., and M. Del Ponte, E. 2022. Measuring\nplant disease severity in R: introducing and evaluating the pliman\npackage. Tropical Plant Pathology 47:95–104. https://doi.org/10.1007/s40858-021-00487-5.\n\n\nOnofri, A., Piepho, H.-P., and Kozak, M. 2018. Analysing censored data\nin agricultural research: A review with examples and software tips.\nAnnals of Applied Biology 174:3–13. https://doi.org/10.1111/aab.12477.\n\n\nParker, S. K., Nutter, F. W., and Gleason, M. L. 1997. Directional\nSpread of Septoria Leaf Spot in Tomato Rows. Plant Disease 81:272–276.\nhttps://doi.org/10.1094/pdis.1997.81.3.272.\n\n\nPaul, P. A., and Munkvold, G. P. 2004. A Model-Based Approach to\nPreplanting Risk Assessment for Gray Leaf Spot of Maize. Phytopathology®\n94:1350–1357. https://doi.org/10.1094/phyto.2004.94.12.1350.\n\n\nPedigo, L. P., Hutchins, S. H., and Higley, L. G. 1986. Economic Injury\nLevels in Theory and Practice. Annual Review of Entomology 31:341–368.\nhttps://doi.org/10.1146/annurev.en.31.010186.002013.\n\n\nPegg, K. G., Coates, L. M., O’Neill, W. T., and Turner, D. W. 2019. The\nepidemiology of fusarium wilt of banana. Frontiers in Plant Science 10.\nhttps://doi.org/10.3389/fpls.2019.01395.\n\n\nPereira, W. E. L., Andrade, S. M. P. de, Del Ponte, E. M., Esteves, M.\nB., Canale, M. C., Takita, M. A., Coletta-Filho, H. D., and De Souza, A.\nA. 2020. Severity assessment in the Nicotiana tabacum-Xylella fastidiosa\nsubsp. pauca pathosystem: design and interlaboratory validation of a\nstandard area diagram set. Tropical Plant Pathology 45:710–722. https://doi.org/10.1007/s40858-020-00401-5.\n\n\nPfender, W. F. 2003. Prediction of Stem Rust Infection Favorability, by\nMeans of Degree-Hour Wetness Duration, for Perennial Ryegrass Seed\nCrops. Phytopathology® 93:467–477. https://doi.org/10.1094/phyto.2003.93.4.467.\n\n\nPietravalle, S., Shaw, M. W., Parker, S. R., and Bosch, F. van den.\n2003. Modeling of Relationships Between Weather and Septoria\ntritici Epidemics on Winter Wheat: A Critical Approach.\nPhytopathology® 93:1329–1339. https://doi.org/10.1094/phyto.2003.93.10.1329.\n\n\nPini, A., and Vantini, S. 2016. The Interval Testing Procedure: A\nGeneral Framework for Inference in Functional Data Analysis. Biometrics\n72:835–845. https://doi.org/10.1111/biom.12476.\n\n\nRamsay, J. O., and Silverman, B. W. 2005. Functional data\nanalysis. 2nd ed. New York: Springer.\n\n\nReis, E. M., Hoffmann, L. L., and Blum, M. 2002. Modelo de ponto crítico\npara estimar os danos causados pelo oídio em cevada. Fitopatologia\nBrasileira 27:644–646.\n\n\nRomero, J., Moral, J., Gonzalez-Dominguez, E., Agustí-Brisach, C., Roca,\nL. F., Rossi, V., and Trapero, A. 2021. Logistic models to predict olive\nanthracnose under field conditions. Crop Protection 148:105714. https://doi.org/10.1016/j.cropro.2021.105714.\n\n\nRossi, V., Caffi, T., Giosuè, S., and Bugiani, R. 2008. A mechanistic\nmodel simulating primary infections of downy mildew in grapevine.\nEcological Modelling 212:480–491. https://doi.org/10.1016/j.ecolmodel.2007.10.046.\n\n\nRossi, V., Giosuè, S., and Caffi, T. 2010. Modelling Plant Diseases for\nDecision Making in Crop Protection. In Springer Netherlands, pp.\n241–258. https://doi.org/10.1007/978-90-481-9277-9_15.\n\n\nRossi, V., Onesti, G., Legler, S. E., and Caffi, T. 2014. Use of systems\nanalysis to develop plant disease models based on literature data: grape\nblack-rot as a case-study. European Journal of Plant Pathology\n141:427–444. https://doi.org/10.1007/s10658-014-0553-z.\n\n\nRotem, J. 1988. Techniques of Controlled-Condition Experiments. In\nSpringer Berlin Heidelberg, pp. 19–31. https://doi.org/10.1007/978-3-642-95534-1_3.\n\n\nSackett, K. E., and Mundt, C. C. 2005. Primary Disease Gradients of\nWheat Stripe Rust in Large Field Plots. Phytopathology® 95:983–991. https://doi.org/10.1094/phyto-95-0983.\n\n\nSaif, M. S., Chancia, R., Pethybridge, S., Murphy, S. P., Hassanzadeh,\nA., and Aardt, J. van. 2023. Forecasting Table Beet Root Yield Using\nSpectral and Textural Features from Hyperspectral UAS Imagery. Remote\nSensing 15:794. https://doi.org/10.3390/rs15030794.\n\n\nSaif, M. S., Chancia, R., Sharma, P., Murphy, S., Raqueno, N., Bauch,\nT., Pethybridge, S., and Aardt, J. van. 2024. Data for: Estimation of\ncercospora leaf spot disease severity in table beets from UAS\nmultispectral images. https://doi.org/10.17632/V9B7RWRWX9.1.\n\n\nSalotti, I., Bove, F., and Rossi, V. 2022. Development and validation of\na mechanistic, weather-based model for predicting puccinia graminis f.\nSp. Tritici infections and stem rust progress in wheat. Frontiers in\nPlant Science 13. https://doi.org/10.3389/fpls.2022.897680.\n\n\nSalotti, I., and Rossi, V. 2023. A Mechanistic Model Accounting for the\nEffect of Soil Moisture, Weather, and Host Growth Stage on the\nDevelopment of Sclerotinia sclerotiorum. Plant\nDisease 107:514–533. https://doi.org/10.1094/pdis-12-21-2743-re.\n\n\nSavary, S., Nelson, A. D., Djurle, A., Esker, P. D., Sparks, A., Amorim,\nL., Bergamin Filho, A., Caffi, T., Castilla, N., Garrett, K., McRoberts,\nN., Rossi, V., Yuen, J., and Willocquet, L. 2018. Concepts, approaches,\nand avenues for modelling crop health and crop losses. European Journal\nof Agronomy 100:4–18. https://doi.org/10.1016/j.eja.2018.04.003.\n\n\nSavary, S., Teng, P. S., Willocquet, L., and Nutter, F. W. 2006.\nQuantification and Modeling of Crop Losses: A Review of Purposes. Annual\nReview of Phytopathology 44:89–112. https://doi.org/10.1146/annurev.phyto.44.070505.143342.\n\n\nSavary, S., Willocquet, L., Pethybridge, S. J., Esker, P., McRoberts,\nN., and Nelson, A. 2019. The global burden of pathogens and pests on\nmajor food crops. Nature Ecology & Evolution 3:430–439. https://doi.org/10.1038/s41559-018-0793-y.\n\n\nScott, P. R., and Hollins, T. W. 1974. Effects of eyespot on the yield\nof winter wheat. Annals of Applied Biology 78:269–279. https://doi.org/10.1111/j.1744-7348.1974.tb01506.x.\n\n\nSeem, R. C. 1984. Simple decision aids for practical control of pests.\nPlant Disease 68:656. https://doi.org/10.1094/pd-69-656.\n\n\nShah, D. A., De Wolf, E. D., Paul, P. A., and Madden, L. V. 2019a.\nFunctional Data Analysis of Weather Variables Linked to Fusarium Head\nBlight Epidemics in the United States. Phytopathology® 109:96–110. https://doi.org/10.1094/phyto-11-17-0386-r.\n\n\nShah, D. A., and Madden, L. V. 2004. Nonparametric Analysis of Ordinal\nData in Designed Factorial Experiments. Phytopathology® 94:33–43. https://doi.org/10.1094/phyto.2004.94.1.33.\n\n\nShah, D. A., Molineros, J. E., Paul, P. A., Willyerd, K. T., Madden, L.\nV., and De Wolf, E. D. 2013. Predicting Fusarium Head Blight Epidemics\nWith Weather-Driven Pre- and Post-Anthesis Logistic Regression Models.\nPhytopathology® 103:906–919. https://doi.org/10.1094/phyto-11-12-0304-r.\n\n\nShah, D. A., Paul, P. A., De Wolf, E. D., and Madden, L. V. 2019b.\nPredicting plant disease epidemics from functionally represented weather\nseries. Philosophical Transactions of the Royal Society B: Biological\nSciences 374:20180273. https://doi.org/10.1098/rstb.2018.0273.\n\n\nShrout, P. E., and Fleiss, J. L. 1979. Intraclass correlations: Uses in\nassessing rater reliability. Psychological Bulletin 86:420–428. https://doi.org/10.1037/0033-2909.86.2.420.\n\n\nSimko, I., and Piepho, H.-P. 2012. The Area Under the Disease Progress\nStairs: Calculation, Advantage, and Application. Phytopathology®\n102:381–389. https://doi.org/10.1094/phyto-07-11-0216.\n\n\nSkaracis, G. N., Pavli, O. I., and Biancardi, E. 2010. Cercospora Leaf\nSpot Disease of Sugar Beet. Sugar Tech 12:220–228. https://doi.org/10.1007/s12355-010-0055-z.\n\n\nTan, W., Li, K., Liu, D., and Xing, W. 2023. Cercospora leaf spot\ndisease of sugar beet. Plant Signaling & Behavior 18. https://doi.org/10.1080/15592324.2023.2214765.\n\n\nTe Beest, D. E., Paveley, N. D., Shaw, M. W., and Bosch, F. van den.\n2008. DiseaseWeather Relationships for Powdery Mildew and\nYellow Rust on Winter Wheat. Phytopathology® 98:609–617. https://doi.org/10.1094/phyto-98-5-0609.\n\n\nTembo, B., Mulenga, R. M., Sichilima, S., M’siska, K. K., Mwale, M.,\nChikoti, P. C., Singh, P. K., He, X., Pedley, K. F., Peterson, G. L.,\nSingh, R. P., and Braun, H. J. 2020. Detection and characterization of\nfungus (Magnaporthe oryzae pathotype Triticum) causing wheat blast\ndisease on rain-fed grown wheat (Triticum aestivum L.) in Zambia ed.\nZonghua Wang. PLOS ONE 15:e0238724. https://doi.org/10.1371/journal.pone.0238724.\n\n\nThresh, J. M. 1998. In memory of James Edward Vanderplank\n19091997. Plant Pathology 47:114–115. https://doi.org/10.1046/j.1365-3059.2998.00220.x.\n\n\nVanderplank, J. 1963. Plant disease epidemics and control.\nElsevier. https://doi.org/10.1016/c2013-0-11642-x.\n\n\nViechtbauer, W. 2010. Conducting Meta-Analyses\ninRwith\nthemetaforPackage. Journal of Statistical\nSoftware 36. https://doi.org/10.18637/jss.v036.i03.\n\n\nWallin, J. R. 1962. Summary of recent progress in predicting late blight\nepidemics in United States and Canada. American Potato Journal\n39:306–312. https://doi.org/10.1007/bf02862155.\n\n\nWiegand, T., and A. Moloney, K. 2004. Rings, circles, and null-models\nfor point pattern analysis in ecology. Oikos 104:209–229. https://doi.org/10.1111/j.0030-1299.2004.12497.x.\n\n\nWillbur, J. F., Fall, M. L., Bloomingdale, C., Byrne, A. M., Chapman, S.\nA., Isard, S. A., Magarey, R. D., McCaghey, M. M., Mueller, B. D.,\nRusso, J. M., Schlegel, J., Chilvers, M. I., Mueller, D. S., Kabbage,\nM., and Smith, D. L. 2018. Weather-Based Models for Assessing the Risk\nof Sclerotinia sclerotiorum Apothecial Presence in\nSoybean (Glycine max) Fields. Plant Disease\n102:73–84. https://doi.org/10.1094/pdis-04-17-0504-re.\n\n\nWindels, C. E., Lamey, H. A., Hilde, D., Widner, J., and Knudsen, T.\n1998. A Cerospora Leaf Spot Model for Sugar Beet: In Practice by an\nIndustry. Plant Disease 82:716–726. https://doi.org/10.1094/pdis.1998.82.7.716.\n\n\nXu, X., Madden, L. V., Edwards, S. G., Doohan, F. M., Moretti, A.,\nHornok, L., Nicholson, P., and Ritieni, A. 2013. Developing logistic\nmodels to relate the accumulation of DON associated with Fusarium head\nblight to climatic conditions in Europe. European Journal of Plant\nPathology 137:689–706. https://doi.org/10.1007/s10658-013-0280-x.\n\n\nXu, X.-M., and Madden, L. V. 2004. Use of SADIE statistics\nto study spatial dynamics of plant disease epidemics. Plant Pathology\n53:38–49. https://doi.org/10.1111/j.1365-3059.2004.00949.x.\n\n\nXu, X.-M., and Robinson, J. D. 2000. Effects of temperature on the\nincubation and latent periods of hawthorn powdery mildew\n(Podosphaera clandestina). Plant Pathology\n49:791–797. https://doi.org/10.1046/j.1365-3059.2000.00520.x.\n\n\nYadav, N. V. S., Vos, S. M. de, Bock, C. H., and Wood, B. W. 2012.\nDevelopment and validation of standard area diagrams to aid assessment\nof pecan scab symptoms on fruit. Plant Pathology 62:325–335. https://doi.org/10.1111/j.1365-3059.2012.02641.x.\n\n\nYe, H., Huang, W., Huang, S., Cui, B., Dong, Y., Guo, A., Ren, Y., and\nJin, Y. 2020. Recognition of Banana Fusarium Wilt Based on UAV Remote\nSensing. Remote Sensing 12:938. https://doi.org/10.3390/rs12060938.\n\n\nYorinori, J. T., Paiva, W. M., Frederick, R. D., Costamilan, L. M.,\nBertagnolli, P. F., Hartman, G. E., Godoy, C. V., and Nunes, J. 2005.\nEpidemics of Soybean Rust (Phakopsora pachyrhizi)\nin Brazil and Paraguay from 2001 to 2003. Plant Disease 89:675–677. https://doi.org/10.1094/pd-89-0675.\n\n\nZadoks, J. C., and Schein, R. D. 1988. James Edward Vanderplank:\nMaverick* and Innovator. Annual Review of Phytopathology 26:31–37. https://doi.org/10.1146/annurev.py.26.090188.000335.",
    "crumbs": [
      "References"
    ]
  }
]