[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R4PDE.net",
    "section": "",
    "text": "Welcome\nR for Plant Disease Epidemiology (R4PDE) is a dynamic online book rooted in the teachings of the annual graduate course, FIP 602 - Plant Disease Epidemiology, a key part of the curriculum in the Graduate Program in Plant Pathology at Universidade Federal de Viçosa.\nDesigned for those passionate about studying and modeling plant disease epidemics with R, the book offers an exploration of diverse methods for describing, visualizing, and analyzing epidemic data collected over time and space. Readers should ideally have a foundational knowledge of R to best utilize the examples.\nHowever, R4PDE is not a resource for learning data science through R, as there are already well-established books such as R for data science for that purpose. This book draws on multiple sources, but in certain sections, it utilizes data and replicates (with permission) some of the analyses (presented in SAS codes) from The Study of Plant Disease Epidemics (Madden et al. 2007), a highly recommended textbook for anyone wishing to delve deeply into plant disease epidemiology.\nA mix of general and specific R packages are utilized to conduct common plant disease epidemiology data analysis, notably {epifitter} and {epiphy}, both designed by plant pathologists. In conjunction with this book, a new R package {r4pde} has been developed and can be installed from GitHub using:\n#install.packages(\"remotes\")\nremotes::install_github(\"emdelponte/r4pde\")\nThis online book is frequently updated and edited. It content is free to use, licensed under a Creative Commons licence, and the code for all analyses can be found on GitHub. Contributions are subject to a Contributor Code of Conduct, and by contributing, you agree to adhere to its terms.\nR for Plant Disease Epidemiology © 2023 by Emerson Medeiros Del Ponte is licensed under CC BY-NC 4.0 \n\n\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. The study of plant disease epidemics. The American Phytopathological Society. Available at: http://dx.doi.org/10.1094/9780890545058."
  },
  {
    "objectID": "cite.html",
    "href": "cite.html",
    "title": "How to cite",
    "section": "",
    "text": "The author has opted for self-publishing this book to ensure its accessibility and updatability, reflecting the dynamic nature of the field and the R programming environment. Readers are encouraged to cite this work when referencing or utilizing the methodologies and insights provided in the book. Ensure the accuracy of the details while citing, especially the URL, to direct readers to the correct online source.\nBelow one suggestion of citation style:\n\nDel Ponte, E. M. (2023). R for Plant Disease Epidemiology (R4PDE). Author. https://r4pde.net\n\nCataloguing-in-Publication data prepared by the library of the Federal University of Viçosa, Brazil."
  },
  {
    "objectID": "author.html",
    "href": "author.html",
    "title": "About the author",
    "section": "",
    "text": "Born in 1973 in the culturally rich city of Pelotas, in the southernmost part of Brazil, I found my passion rooted in the lifestyle of some relatives and their friends working as scientists in agricultural sciences. My high school years, spent in a rural setting, were instrumental in forming my connection to agriculture. Graduating with a major in agronomy in 1996 was just the beginning of my academic journey. I delved deeper into the realm of agronomy and plant pathology during my graduate studies at the Federal University of Pelotas, one of the most prominent schools of Agriculture in the country. My engagement with plant diseases solidified during my doctoral project, where I tackled the complexities of modeling Fusarium head blight epidemics, a disease significantly affecting wheat crops.\nThe subsequent stages of my career took me to renowned institutions. I spent one year at Cornell University during my doctoral study and then ventured into a postdoctoral role at Iowa State University, experiences that broadened my research scope and professional network. In 2006, my career took another exciting turn when I embraced a role as an assistant professor at the Federal University of Rio Grande do Sul. For eight years, I dedicated myself to teaching, research and mentoring students with a focus on epidemiology. Seeking new challenges, in 2014 I transitioned to the Universidade Federal de Viçosa (UFV) to take a prestigious position in plant disease epidemiology as associate professor. Here, now as a full professor, beyond my research, I found immense gratification in mentoring, educating, and guiding aspiring scientists during their master’s and doctoral journeys.\nThroughout my career, I have strongly advocated for the principles of open science. Within my lab, transparency isn’t a mere concept but an integral part of our operational ethos. We harness the power of the R language for our statistical analyses and data processes, and we openly share our computational methodologies, codes, and findings.\nMy dedication to disseminating knowledge freely inspired me to take the route of open education, leading me to self-publish this book entitled “R for Plant Disease Epidemiology” which is written entirely using open source tools. I chose this approach to guarantee the book’s accessibility and updatability, mirroring the ever-evolving nature of both the field of plant pathology and the R programming environment.\nI dedicate this book to my wife, Isabel, and our son, Vitor, who have been my pillars of strength and support. Their love and understanding have been unwavering, especially during the times I am distant, immersed in the demanding process of writing this book."
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Embarking on the journey of authoring “R for Plant Disease Epidemiology (R4PDE)” has been an enlightening voyage. This endeavor would not have been possible without the unwavering support and intellectual generosity of many individuals and institutions.\nFirst, my heartfelt gratitude goes to Helen Pennington, who generously granted permission to use her beautiful painting of coffee leaf rust for the book cover. Helen’s artwork encapsulates the essence of plant disease epidemiology, providing a visual narrative that complements the academic discourse.\nI am equally indebted to Prof. Laurence V. Madden, whose generosity in allowing the use of data showcased in the book “The Study of Plant Disease Epidemics” has enriched the analytical content of this publication.\nThe collaborative spirit of the open-source community has been a cornerstone in refining and augmenting the content of “R for Plant Disease Epidemiology (R4PDE)”. I am immensely thankful to those who took the time and effort to contribute fixes and improvements. Their engagements, whether through pull requests or other forms of contact, have been invaluable: Adam Sparks (@adamhsparks), Remco Stam (@remco-stam), Tiago Olivoto (@TiagoOlivoto), Monalisa De Cól (@Monalisacdc), and Juan Edwards Molina (@juanchiem)."
  },
  {
    "objectID": "intro.html#defining-plant-disease",
    "href": "intro.html#defining-plant-disease",
    "title": "1  Introduction",
    "section": "1.1 Defining plant disease",
    "text": "1.1 Defining plant disease\nDisease in plants can be defined as any malfunctioning of host cells and tissues that results from continuous irritation by a pathogenic agent or environmental factor and leads to development of symptoms (Agrios 2005a). When caused by pathogenic agent, the disease results from the combination of three elements: susceptible host plant, a virulent pathogen, and favorable environmental conditions - the famous disease triangle. When a pathogen population establishes and causes disease in a host population, the phenomenon is called an epidemic, or the disease in populations. Among several definitions of epidemic, a comprehensive one is the change in disease intensity in a host population over time and space (Madden et al. 2007).\nThere exist numerous iterations of the disease triangle, incorporating additional elements (e.g., human intervention and time) as points and/or dimensions to provide a more comprehensive representation of an epidemic (Agrios 2005b). We find the disease prism particularly illustrative, where a sequence of stacked triangles represent the evolution of a plant disease through time (Francl 2001).\n\n\n\nFigure 1.1: The plant disease prism as a model of plant disease epidemics"
  },
  {
    "objectID": "intro.html#importance-of-epidemics",
    "href": "intro.html#importance-of-epidemics",
    "title": "1  Introduction",
    "section": "1.2 Importance of epidemics",
    "text": "1.2 Importance of epidemics\nEpidemics bear significant economic importance due to their potential to decrease crop yields, diminish product quality, and escalate control costs, contingent on their intensity level. Numerous historical examples of widespread epidemics, reaching pandemic levels and resulting in catastrophic effects on crops, have been documented (Agrios 2005b). The Irish potato famine of 1845–1847, caused by the late blight pathogen (Phytophthora infestans), is a famous example of a well-documented pandemic. This disease notably altered the course of history in Europe and the United States, and was pivotal in the evolution of the science of plant pathology. During the 1840s, the pathogen ravaged potato crops, which were a dietary staple for the Irish. The disease outbreak was triggered by the introduction of a novel, virulent pathogen population that found suitable environmental conditions (cool and wet weather) for infection and development within a dense population of susceptible hosts.\nHowever, there are several reasons why devastating epidemics may continue to unfold. Recent history has seen severe epidemics reaching pandemic levels due to the incursion of pathogens into regions where they had previously been absent (refer to Box 1). Alternatively, new pathogenic strains might emerge as a result of factors driving genetic diversity within the local pathogen population. A case in point is the Ug99 strain of the wheat stem rust, which poses a significant threat to global wheat production. First identified in Uganda in 1998, an asexual lineage has propagated through Africa and the Middle East, causing catastrophic epidemics. Research suggests that Ug99 emerged via somatic hybridization and nuclear exchange between isolates from different lineages (Li et al. 2019). Finally, disease emergence or re-emergence can be influenced by shifts in climatic patterns. For instance, the Fusarium head blight of wheat caused by the fungus Fusarium graminearum. In Southern Brazil, the increased frequency of severe epidemics resulting in greater yield loss since the early 1990s has been linked to alterations in rainfall patterns across decades (Duffeck et al. 2020).\n\n\n\n\n\n\nBox 1: Diseases on the move\n\n\n\nIn Brazil, the soybean rust pathogen (Phakopsora pachyrhizi) first reached southern Brazil in 2002 (Yorinori et al. 2005). The disease spread to all production regions of the country in the following few years, severely reducing yields. To overcome the problem, farmers have relied on massive applications of fungicides on soybeans, which dramatically increased the production costs with the need for sequential fungicide sprays to combat the disease. Total economic loss have been estimated at around US$ 2 billion yearly (Godoy et al. 2016). More recently, wheat blast, a disease that originated in the south of Brazil in 1984, and have been restricted to South America, was firstly spotted in South Asia, Bangladesh, in 2016. Blast epidemics in that occasion devastated more than 15,000 ha of wheat and reduced yield of wheat in the affected field up to 100% (Malaker et al. 2016; Islam et al. 2019). The disease was later found in Zambia, thus also becoming a threat to wheat production in Africa (Tembo et al. 2020). In Brazil, the wheat blast disease is a current threat to expansion of wheat cultivation in the tropics(Cruz and Valent 2017)."
  },
  {
    "objectID": "intro.html#history-of-epidemiology",
    "href": "intro.html#history-of-epidemiology",
    "title": "1  Introduction",
    "section": "1.3 History of Epidemiology",
    "text": "1.3 History of Epidemiology\nBotanical epidemiology, or the study of plant disease epidemics, is a discipline with roots tracing back to the early 1960s. However, its origins can be linked to events from centuries and decades prior. For instance, in 1728, Duhamel de Monceau presented the earliest known epidemiological work on a disease, referred to as ‘Death,’ that afflicted saffron crocus (Rhizoctonia violacea). Fast forward to 1858, a textbook detailing plant diseases, written by Julius Kuhn, made its debut, introducing the concept of an epidemic as illustrated by the Irish late blight epidemics of 1845-46. Subsequently, in 1901, H.M. Ward adopted an ecological perspective to the study of plant diseases in his seminal book, Disease in Plants. By 1946, Gäumann penned the first book exclusively devoted to plant disease epidemiology.\nFurther evolution of this field was marked by the publication of a chapter titled “Analysis of Epidemics” by J.E. Vanderplank in Plant Pathology, vol. 3, edited by Horsfall and Dimond, in 1960. Vanderplank elaborated on his pioneering ideas in his 1963 book, “Plant Diseases: Epidemics and Control”(Vanderplank 1963). He is universally recognized as the foundational figure of plant disease epidemiology (Zadoks and Schein 1988; Thresh 1998), his landmark book being the first to comprehensively describe and quantify plant disease epidemics, and offering a theoretical framework for epidemic analysis.\nIn the same year, the first International Epidemiology Workshop was convened in Pau, France. This event constitutes an important milestone in the historical narrative, significantly contributing to the molding of this emergent discipline.\n\n\n\nFigure 1.2: Group photo of the First International Epidemiology Workshop\n\n\nThe International Epidemiology Workshop (IEW) is the principal working group of plant disease epidemiology. This is an organization with a rich history whose members have met approximately every 5 years since 1963. Thus far, 13 meetings have been organized/planned:\n1963 - Pau, France\n1971 - Wageningen, The Netherlands\n1979 - Penn State, United States\n1983 - NC State, Raleigh, United States\n1986 - Jerusalem, Israel\n1990 - Giessen, Germany\n1994 - Papendal, The Netherlands\n2001 - Ouro Preto, Brazil\n2005 - Landerneau, France\n2009 - Cornell, Geneva, United States\n2013 - Beijing, China\n2018 - Lillehammer, Norway\n2024 - Iguassu Falls, Brazil"
  },
  {
    "objectID": "intro.html#other-resources",
    "href": "intro.html#other-resources",
    "title": "1  Introduction",
    "section": "1.4 Other resources",
    "text": "1.4 Other resources\n\n1.4.1 Books\n2006 - The Epidemiology of Plant Diseases\n2007 - The Study of Plant Disease Epidemics\n2017 - Exercises in Plant Disease Epidemiology\n2017 - Application of Information Theory to Epidemiology\n2020 - Emerging Plant Diseases and Global Security\n\n\n\n1.4.2 Online tutorials\nEcology and Epidemiology in R\nPlant Disease Epidemiology - Temporal aspects\nSimulation Modeling in Plant Disease Epidemiology and Crop Loss Analysis\n\n\n1.4.3 Software\nEpicrop - Simulation Modeling of Crop Diseases using a SEIR model\n\n\n\n\nAgrios, G. N. 2005a. INTRODUCTION. In Elsevier, p. 3–75. Available at: http://dx.doi.org/10.1016/b978-0-08-047378-9.50007-5.\n\n\nAgrios, G. N. 2005b. Plant disease epidemiology. In Elsevier, p. 265–291. Available at: http://dx.doi.org/10.1016/b978-0-08-047378-9.50014-2.\n\n\nCruz, C. D., and Valent, B. 2017. Wheat blast disease: danger on the move. Tropical Plant Pathology. 42:210–222 Available at: http://dx.doi.org/10.1007/s40858-017-0159-z.\n\n\nDuffeck, M. R., Santos Alves, K. dos, Machado, F. J., Esker, P. D., and Del Ponte, E. M. 2020. Modeling Yield Losses and Fungicide Profitability for Managing Fusarium Head Blight in Brazilian Spring Wheat. Phytopathology®. 110:370–378 Available at: http://dx.doi.org/10.1094/PHYTO-04-19-0122-R.\n\n\nFrancl, L. J. 2001. The..disease triangle: A plant pathological paradigm revisited. The Plant Health Instructor. Available at: http://dx.doi.org/10.1094/PHI-T-2001-0517-01.\n\n\nGodoy, C. V., Seixas, C. D. S., Soares, R. M., Marcelino-Guimarães, F. C., Meyer, M. C., and Costamilan, L. M. 2016. Asian soybean rust in brazil: Past, present, and future. Pesquisa Agropecuária Brasileira. 51:407–421 Available at: http://dx.doi.org/10.1590/S0100-204X2016000500002.\n\n\nIslam, M. T., Kim, K.-H., and Choi, J. 2019. Wheat Blast in Bangladesh: The Current Situation and Future Impacts. The Plant Pathology Journal. 35:1–10 Available at: http://dx.doi.org/10.5423/ppj.rw.08.2018.0168.\n\n\nLi, F., Upadhyaya, N. M., Sperschneider, J., Matny, O., Nguyen-Phuc, H., Mago, R., et al. 2019. Emergence of the Ug99 lineage of the wheat stem rust pathogen through somatic hybridisation. Nature Communications. 10 Available at: http://dx.doi.org/10.1038/s41467-019-12927-7.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. The study of plant disease epidemics. The American Phytopathological Society. Available at: http://dx.doi.org/10.1094/9780890545058.\n\n\nMalaker, P. K., Barma, N. C. D., Tiwari, T. P., Collis, W. J., Duveiller, E., Singh, P. K., et al. 2016. First Report of Wheat Blast Caused by Magnaporthe oryzae Pathotype triticum in Bangladesh. Plant Disease. 100:2330–2330 Available at: http://dx.doi.org/10.1094/pdis-05-16-0666-pdn.\n\n\nTembo, B., Mulenga, R. M., Sichilima, S., M’siska, K. K., Mwale, M., Chikoti, P. C., et al. 2020. Detection and characterization of fungus (Magnaporthe oryzae pathotype Triticum) causing wheat blast disease on rain-fed grown wheat (Triticum aestivum L.) in Zambia ed. Zonghua Wang. PLOS ONE. 15:e0238724 Available at: http://dx.doi.org/10.1371/journal.pone.0238724.\n\n\nThresh, J. M. 1998. In memory of James Edward Vanderplank 19091997. Plant Pathology. 47:114–115 Available at: http://dx.doi.org/10.1046/j.1365-3059.2998.00220.x.\n\n\nVanderplank, J. 1963. Plant disease epidemics and control. Elsevier. Available at: http://dx.doi.org/10.1016/C2013-0-11642-X.\n\n\nYorinori, J. T., Paiva, W. M., Frederick, R. D., Costamilan, L. M., Bertagnolli, P. F., Hartman, G. E., et al. 2005. Epidemics of Soybean Rust (Phakopsora pachyrhizi) in Brazil and Paraguay from 2001 to 2003. Plant Disease. 89:675–677 Available at: http://dx.doi.org/10.1094/PD-89-0675.\n\n\nZadoks, J. C., and Schein, R. D. 1988. James Edward Vanderplank: Maverick* and Innovator. Annual Review of Phytopathology. 26:31–37 Available at: http://dx.doi.org/10.1146/annurev.py.26.090188.000335."
  },
  {
    "objectID": "data-terminology.html#disease-quantification",
    "href": "data-terminology.html#disease-quantification",
    "title": "2  Disease variables",
    "section": "2.1 Disease quantification",
    "text": "2.1 Disease quantification\nStudies on the temporal progression or spatial spread of epidemics cannot be conducted without field-collected data, or, in some cases, simulated data. The study of plant disease quantification, termed Phytopathometry, is a subdivision of plant pathology concerned with the science of disease measurement. It has strong ties to the field of epidemiology (Bock et al. 2021).\nTraditionally, disease quantification has been executed through visual evaluation. However, the past few decades have witnessed significant advancements in imaging and remote sensing technologies (which don’t necessitate contact with the object), leaving a profound impact on this field. As such, disease quantity can now be gauged through estimation (visually, by the human eye) or measurement (through remote sensing technologies such as RGB, MSI, and HSI) Figure 2.1.\nWhile the utilization of digital or remote sensing technology for disease measurement or estimation provides a more objective approach, visual assessment is largely subjective. It is known to vary among human raters, as these raters differ in their innate abilities, training, and how they are influenced by the chosen method (e.g., scales). Disease is estimated or measured on a specimen within a population, or on a sample of specimens drawn from that population. The specimen in question can be a plant organ, an individual plant, a group of plants, a field, or a farm. The specific specimen type also determines the terminology used to describe disease quantity.\n\n\n\nFigure 2.1: Different approaches used to obtain estimates or measures of plant disease. RGB = red, green, blue; MSI = multispectral imaging; HSI = hyperspectral imaging.\n\n\ninally, while developing new or refining existing disease assessment methods, it is crucial to evaluate the reliability of the assessments made by different raters or instruments, as well as their accuracy—specifically, how close the estimations or measurements are to the reference (or gold standard) values. Several methods are available for assessing the reliability, precision, and accuracy of these estimates or measurements (see definitions). The choice of methods depends on the objective of the work, but largely on the type or nature of the data. These considerations will be further discussed."
  },
  {
    "objectID": "data-terminology.html#disease-variables",
    "href": "data-terminology.html#disease-variables",
    "title": "2  Disease variables",
    "section": "2.2 Disease variables",
    "text": "2.2 Disease variables\nA common term used to reference the quantity of disease, irrespective of how it is expressed, is ‘disease intensity’. This term, however, has minimal practical value as it only implies that the disease is more or less “intense”. We require more specific terminology to standardize the reference to disease quantity and methodology. One of the primary tasks in disease assessment is classifying each specimen, often in a sample or within a population, as diseased or not diseased. This binary (yes/no or 1/0) evaluation may sufficiently express disease intensity if the goal is to ascertain the number or proportion of diseased specimens in a sample or a population.\nThis discussion brings us to two terms: disease incidence and prevalence. Incidence is typically used to denote the proportion or number (count) of plants (or their organs) deemed as observational units at the field scale or below. On the other hand, prevalence refers to the proportion or number of fields or farms with diseased plants within a larger production area or region (Nutter et al. 2006) Figure 2.2. Therefore, prevalence is analogous to incidence, with the only difference being the spatial scale of the sampling unit.\n\n\n\nFigure 2.2: Schematic representation of how prevalence and incidence of plant diseases are calculated depending on the spatial scale of the assessment\n\n\nIn many instances, it’s necessary to determine the degree to which a specimen is diseased, a concept defined as disease severity. In certain contexts, severity is narrowly defined as the proportion of the unit that exhibits symptoms (Nutter et al. 2006). However, a more expansive view of severity includes additional metrics such as nominal or ordinal scores, lesion count, and percent area affected (ratio scale). Ordinal scales are broken down into rank-ordered classes (see specific section), defined based on either a percentage scale or descriptions of symptoms (Bock et al. 2021). Occasionally, disease is expressed in terms of (average) lesion size or area, which could be regarded as a measure of severity. These variables represent different levels of measurements that provide varying degrees of information about the disease quantity - from low (nominal scale) to high (ratio scale) Figure 2.3.\n\n\n\nFigure 2.3: Scales and associated levels of measurement used to describe severity of plant diseases"
  },
  {
    "objectID": "data-terminology.html#data-types",
    "href": "data-terminology.html#data-types",
    "title": "2  Disease variables",
    "section": "2.3 Data types",
    "text": "2.3 Data types\nThe data used to express disease as incidence or any form of severity measurements can be discrete or continuous in nature.\nDiscrete variables are countable (involving integers) at a particular point in time. In other words, only a finite number of values (nominal or ordinal) is possible, and these cannot be subdivided. For instance, a plant or plant part can be either diseased or not diseased (nominal data). It’s not possible to count 1.5 diseased plants. Furthermore, a plant classified as diseased may exhibit a certain number of lesions (count data), or be categorized into a specific severity class (ordinal data, common in ordinal scales, e.g., 1-9). Disease data in the form of counts often relates to the number of infections per sampling units. Most commonly, these counts refer to the assessed pathogen population, such as the number of airborne or soilborne propagules.\nIn contrast to discrete variables, continuous variables can be measured on a scale and can assume any numeric value between two points. For example, the size of a lesion on a plant can be measured at a very precise scale (cm or mm). An estimate of severity on a percent scale (% diseased area) can take any value between non-zero and 100%. Although incidence at the individual level is discrete, at the sample level it can be treated as continuous, as it can assume any value in proportion or percentage.\nDisease variables can also be characterized by a statistical distribution, which are models that provide the probability of a specific value (or a range of values) being drawn from a particular distribution. Understanding statistical or mathematical distributions is a crucial step in improving our grasp of data collection methods, experiment design, and data analysis processes such as data summarization or hypothesis testing."
  },
  {
    "objectID": "data-terminology.html#statistical-distributions-and-simulation",
    "href": "data-terminology.html#statistical-distributions-and-simulation",
    "title": "2  Disease variables",
    "section": "2.4 Statistical distributions and simulation",
    "text": "2.4 Statistical distributions and simulation\n\n2.4.1 Binomial distribution\nFor incidence (and prevalence), the data is binary at the individual level, as there are only two possible outcomes in a trial: the plant or plant part is disease or not diseased. The statistical distribution that best describe the incidence data at the individual level is the binomial distribution.\nLet’s simulate the binomial outcomes for a range of probabilities in a sample of 100 units, using the rbinom() function in R. For a single trial (e.g., status of plants in a single plant row), the size argument is set to 1.\n\nlibrary(tidyverse)\nlibrary(r4pde)\n\n\nset.seed(123) # for reproducibility\nP.1 &lt;- rbinom(100, size = 1, prob = 0.1)\nP.3 &lt;- rbinom(100, size = 1, prob = 0.3)\nP.7 &lt;- rbinom(100, size = 1, prob = 0.7)\nP.9 &lt;- rbinom(100, size = 1, prob = 0.9)\nbinomial_data &lt;- data.frame(P.1, P.3, P.7, P.9)\n\nWe can then visualize the plots.\n\nbinomial_data |&gt;\n  pivot_longer(1:4, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 bins = 10) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\nFigure 2.4: Binomial distribution to describe binary data\n\n\n\n\n\n\n2.4.2 Beta distribution\nIn many studies, it’s often useful to express these quantities as a proportion of the total population or sample size, rather than absolute numbers. This helps standardize the data, making it easier to compare between different populations or different time periods.\nFor example, if we’re studying a plant disease, we could express disease incidence as the proportion of plants that are newly diseased during a given time period. Similarly, disease severity could be expressed as the proportion of each plant’s organ area that is affected by the disease. These proportions are ratio variables, as they can take on any value between 0 and 1, and ratios of these variables are meaningful.\nThe Beta distribution is a probability distribution that is defined between 0 and 1, which makes it ideal for modeling data that represents proportions. It’s a flexible distribution, as its shape can take many forms depending on the values of its two parameters, often denoted as alpha and beta.\nLet’s simulate some data using the rbeta() function.\n\nbeta1.5 &lt;- rbeta(n = 1000, shape1 = 1, shape2 = 5)\nbeta5.5 &lt;- rbeta(n = 1000, shape1 = 5, shape2 = 5)\nbeta_data &lt;- data.frame(beta1.5, beta5.5)\n\nNotice that there are two shape parameters in the beta distribution: shape1 and shape2 to be defined. This makes the distribution very flexible and with different potential shapes as we can see below.\n\nbeta_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\",\n                 bins = 15) +\n  scale_x_continuous(limits = c(0, 1)) +\n  facet_wrap( ~ P)+\n  theme_r4pde()\n\n\n\n\nFigure 2.5: Binomial distribution to describe proportion data\n\n\n\n\n\n\n2.4.3 Beta-binomial distribution\nThe Beta-Binomial distribution is a mixture of the Binomial distribution with the Beta distribution acting as a prior on the probability parameter of the binomial. Disease probabilities can vary across trials due to a number of unobserved or unmeasured factors. This variability can result in overdispersion, a phenomenon where the observed variance in the data is greater than what the binomial distribution expects.\nThis is where the Beta-Binomial distribution comes in handy. By combining the Beta distribution’s flexibility in modeling probabilities with the Binomial distribution’s discrete event modeling, it provides an extra layer of variability to account for overdispersion. The Beta-Binomial distribution treats the probability of success (disease occurrence in this context) as a random variable itself, following a Beta distribution. This means the probability can vary from trial to trial.\nTherefore, when we observe data that shows more variance than the Beta distribution can account for, or when we believe there are underlying factors causing variability in the probability of disease occurrence, the Beta-Binomial distribution is a more appropriate model. It captures both the variability in success probability as well as the occurrence of the discrete event (disease incidence).\nWhen combined with the Binomial distribution, which handles discrete events (e.g. whether an individual is diseased or not), the Beta-Binomial distribution allows us to make probabilistic predictions about these events. For example, based on prior data (the Beta distribution), we can estimate the likelihood of a particular individual being diseased (the Binomial distribution).\nIn R, the rBetaBin function of the FlexReg package generates random values from the beta-binomial distribution. The arguments of the function are n, or the number of values to generate; if length(n) &gt; 1, the length is taken to be the number required. size is he total number of trials. mu is the mean parameter. It must lie in (0, 1). theta is the overdispersion parameter. It must lie in (0, 1). phi the precision parameter. It is an alternative way to specify the theta parameter. It must be a positive real value.\n\nlibrary(FlexReg) \nbetabin3.6 &lt;- rBetaBin(n = 100, size = 40, mu = .3, theta = .6)\nbetabin7.3 &lt;- rBetaBin(n = 100, size = 40, mu = .7, theta = .3)\nbetabin_data &lt;- data.frame(betabin3.6, betabin7.3)\n\n\nbetabin_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\",\n                 bins = 15) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\nFigure 2.6: Beta-binomial distribution to describe proportion data\n\n\n\n\n\n\n2.4.4 Poisson distribution\nWhen conducting studies in epidemiology, specifically plant diseases, researchers often collect data on the number of diseased plants, infected plant parts, or individual symptoms, such as lesions. These variables are counted in whole numbers - 1, 2, 3, etc., making them discrete variables. Discrete variables contrast with continuous variables that can take any value within a defined range and can include fractions or decimals. In addition to being discrete, these variables are also non-negative, meaning they cannot take negative values. After all, you can’t have a negative number of diseased plants or lesions. Given these characteristics, a suitable distribution to model such data is the Poisson distribution. This distribution is particularly suitable for counting the number of times an event occurs in a given time or space.\nIn R, we can used the rpois() function to obtain 100 random observations following a Poisson distribution. For such, we need to inform the number of observation (n = 100) and lambda, the vector of means.\n\npoisson5 &lt;- rpois(100, lambda = 10)\npoisson35 &lt;- rpois(100, lambda = 35)\npoisson_data &lt;- data.frame(poisson5, poisson35)\n\n\npoisson_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\",\n                 bins = 15) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\nFigure 2.7: Poisson distribution to describe count data\n\n\n\n\n\n\n2.4.5 Negative binomial distribution\nWhile the Poisson distribution is indeed suitable for modeling count data, it assumes that the mean and variance of the data are equal. However, in real-world scenarios, especially in epidemiology, it is common to encounter overdispersed data - where the variance is greater than the mean. This could occur, for instance, if there’s greater variability in disease incidence across different plant populations than would be expected under the Poisson assumption.\nIn such cases, the Negative Binomial distribution is a better alternative. The Negative Binomial distribution is a discrete probability distribution that models the number of successes in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of failures occurs.\nOne of the key features of the Negative Binomial distribution is its ability to handle overdispersion. Unlike the Poisson distribution, which has one parameter (lambda, representing the mean and variance), the Negative Binomial distribution has two parameters. One parameter is the mean, but the other (often denoted as ‘size’ or ‘shape’) governs the variance independently, allowing it to be larger than the mean if necessary. Thus, it provides greater flexibility than the Poisson distribution for modeling count data and can lead to more accurate results when overdispersion is present.\nIn R, we can use the rnbinom() function to generate random variates from a Negative Binomial distribution. This function requires the number of observations (n), the target for the number of successful trials (size), and the probability of each success (prob).\nHere’s an example:\n\n# Generate 100 random variables from a Negative Binomial distribution\nnegbin14.6 &lt;- rnbinom(n = 100, size = 14, prob = 0.6)\nnegbin50.6 &lt;- rnbinom(n = 100, size = 50, prob = 0.6)\nnegbin_data &lt;- data.frame(negbin14.6, negbin50.6)\n\n\nnegbin_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\", bins = 15) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\nFigure 2.8: Negative binomial distribution to describe overdispersed count data\n\n\n\n\n\n\n2.4.6 Gamma distribution\nIn plant disease epidemiology and other fields of study, we may often encounter continuous variables - these are variables that can take on any value within a given range, including both whole numbers and fractions. An example of a continuous variable in this context is lesion size, which can theoretically be any non-negative value.\nOften, researchers use the normal (Gaussian) distribution to model such continuous variables. The normal distribution is symmetric, bell-shaped, and is fully described by its mean and standard deviation. However, a fundamental characteristic of the normal distribution is that it extends from negative infinity to positive infinity. While this is not a problem for many applications, it becomes an issue when the variable being modeled cannot take on negative values - like the size of a lesion.\nThis is where the Gamma distribution can be a good alternative. The Gamma distribution is a two-parameter family of continuous probability distributions, which does not include negative values, making it an appropriate choice for modeling variables like lesion sizes. While it might seem a bit more complicated due to its two parameters, this also allows it a greater flexibility in terms of the variety of shapes and behaviors it can describe. The Gamma distribution is often used in various scientific disciplines, including queuing models, climatology, financial services, and of course, epidemiology. Its main parameters are the shape and scale (or alternatively shape and rate), which control the shape, spread and location of the distribution.\nWe can use the rgamma() function that requires the number of samples (n = 100 in our case) and the shape, or the mean value.\n\ngamma10 &lt;- rgamma(n = 100, shape = 10, scale = 1)\ngamma35 &lt;- rgamma(n = 100, shape = 35, scale = 1)\ngamma_data &lt;- data.frame(gamma10, gamma35)\n\n\ngamma_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\",\n                 bins = 15) +\n  ylim(0, max(gamma_data$gamma35)) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\nFigure 2.9: Gamma distribution to describe continuous data\n\n\n\n\n\n\n2.4.7 Simulating ordinal data\nOrdinal data is a statistical data type consisting of numerical scores that fall into a set of categories which are ordered in a meaningful way. This can include survey responses (e.g., strongly disagree to strongly agree), levels of achievement (e.g., poor, average, good, excellent), or, in the case of plant disease, disease severity scales (e.g., 0 to 5, where 0 represents a healthy plant and 5 represents a plant with severe symptoms).\nWhen working with ordinal data, we often need to make assumptions about the distribution of the data. However, unlike continuous data which might be modeled by a normal or Gamma distribution, or count data which might be modeled by a Poisson distribution, ordinal data is discrete and has a clear order but the distances between the categories are not necessarily equal or known. This makes the modeling process slightly different.\nWe can use the sample() function and define the probability associated with each rank. Let’s generate 30 units with a distinct ordinal score. In the first situation, the higher probabilities (0.5) are for scores 4 and 5 and lower (0.1) for scores 0 and 1, and in the second situation is the converse.\n\nordinal1 &lt;- sample(0:5, 30, replace = TRUE, prob = c(0.1, 0.1, 0.2, 0.2, 0.5, 0.5))\nordinal2 &lt;- sample(0:5, 30, replace = TRUE, prob = c(0.5, 0.5, 0.2, 0.2, 0.1, 0.1))\nordinal_data &lt;- data.frame(ordinal1, ordinal2)\n\n\nordinal_data |&gt;\n  pivot_longer(1:2, names_to = \"P\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(value)) +\n  geom_histogram(fill = \"#339966\",\n                 color = \"white\",\n                 bins = 6) +\n  facet_wrap( ~ P) +\n  theme_r4pde()\n\n\n\n\nFigure 2.10: Sampling of ordinal data\n\n\n\n\n\n\n\n\nBock, C. H., Pethybridge, S. J., Barbedo, J. G. A., Esker, P. D., Mahlein, A.-K., and Del Ponte, E. M. 2021. A phytopathometry glossary for the twenty-first century: towards consistency and precision in intra- and inter-disciplinary dialogues. Tropical Plant Pathology. 47:14–24 Available at: http://dx.doi.org/10.1007/s40858-021-00454-0.\n\n\nNutter, F. W., Esker, P. D., and Netto, R. A. C. 2006. Disease Assessment Concepts and the Advancements Made in Improving the Accuracy and Precision of Plant Disease Data. European Journal of Plant Pathology. 115:95–103 Available at: http://dx.doi.org/10.1007/s10658-005-1230-z."
  },
  {
    "objectID": "data-ordinal.html#introduction",
    "href": "data-ordinal.html#introduction",
    "title": "3  Ordinal scales",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nOrdinal scales are organized as rank-ordered numeric classes, with a finite number of such classes. The utilization of ordinal scales is often due to their convenience and speed of rating (Madden et al. 2007). In fact, plant pathologists often encounter situations where direct estimates of the nearest percent severity are time-consuming or impractical, besides being unreliably and inacuratelly estimated. In plant pathological research, there are two commonly used types of ordinal scales: quantitative and qualitative (Chiang and Bock 2021).\n\n3.1.1 Quantitative ordinal\nIn the quantitative ordinal scale, each score signifies a defined interval of the percentage scale. The most renowned quantitative ordinal scale is the Horsfall-Barratt (HB) scale, which was developed in the early 1940s when the science of plant pathology was transitioning towards more quantitative methodologies (Hebert 1982). The HB scale partitions the percentage scale into twelve successive, logarithmic-based intervals of severity ranging from 0 to 100%. The intervals increase in size from 0 to 50% and decrease from 50 to 100%.\n\n\n\n\n\n\nControversy of the H-B scale\n\n\n\nThe divisions of the H-B scale were established on two assumptions. The first was the logarithmic relationship between the intensity of a stimulus and the subsequent sensation. The second was the propensity of a rater to focus on smaller objects when observing objects of two colors (Madden et al. 2007). This foundation is based on the so-called Weber-Fechner law. However, there is limited experimental evidence supporting these assumptions. Current evidence indicates a linear relationship, rather than a logarithmic one, between visually estimated and actual severity (Nutter and Esker 2006). Additionally, these authors demonstrated that raters more accurately discriminated disease severity between 25% and 50% than what the H-B scale allowed. New scale structures have been proposed to address the issues associated with the H-B scale (Liu et al. 2019; Chiang et al. 2014). The Chiang scale follows a linear relationship with the percentage area diseased at severities greater than 10% (class 6 on the scale).\n\n\nLet’s input the HB scale data and store as a data frame in R so we can prepare a table and a plot.\n\nHB &lt;- tibble::tribble(\n  ~ordinal, ~'range', ~midpoint,\n  0,          '0',    0,   \n  1,    '0+ to 3',  1.5,   \n  2,    '3+ to 6',  4.5,   \n  3,   '6+ to 12',  9.0,  \n  4,  '12+ to 25', 18.5, \n  5,  '25+ to 50', 37.5, \n  6,  '50+ to 75', 62.5, \n  7,  '75+ to 88', 81.5, \n  8,  '88+ to 94', 91.0, \n  9,  '94+ to 97', 95.5, \n  10,'97+ to 100', 98.5,  \n  11,      '100',   100 \n  )\nknitr::kable(HB, align = \"c\")\n\n\n\nTable 3.1: The Horsfal-Barrat quantitative ordinal scale used as a tool for assessing plant disease severity\n\n\nordinal\nrange\nmidpoint\n\n\n\n\n0\n0\n0.0\n\n\n1\n0+ to 3\n1.5\n\n\n2\n3+ to 6\n4.5\n\n\n3\n6+ to 12\n9.0\n\n\n4\n12+ to 25\n18.5\n\n\n5\n25+ to 50\n37.5\n\n\n6\n50+ to 75\n62.5\n\n\n7\n75+ to 88\n81.5\n\n\n8\n88+ to 94\n91.0\n\n\n9\n94+ to 97\n95.5\n\n\n10\n97+ to 100\n98.5\n\n\n11\n100\n100.0\n\n\n\n\n\n\nLet’s visualize the different sizes of the percent interval encompassing each score.\n\n\nCode\nlibrary(tidyverse)\nlibrary(r4pde)\nHB |&gt; \n  ggplot(aes(midpoint, ordinal))+\n  geom_point(size =2)+\n  geom_line()+\n  scale_x_continuous(breaks = c(0, 3, 6, 12, 25, 50, 75, 88, 94, 97))+\n  scale_y_continuous(breaks = c(1:12))+\n  geom_vline(aes(xintercept = 3), linetype = 2)+\n  geom_vline(aes(xintercept = 6), linetype = 2)+\n  geom_vline(aes(xintercept = 12), linetype = 2)+\n  geom_vline(aes(xintercept = 25), linetype = 2)+\n  geom_vline(aes(xintercept = 50), linetype = 2)+\n  geom_vline(aes(xintercept = 75), linetype = 2)+\n  geom_vline(aes(xintercept = 88), linetype = 2)+\n  geom_vline(aes(xintercept = 94), linetype = 2)+\n  geom_vline(aes(xintercept = 97), linetype = 2)+\n  labs(x = \"Percent severity\", y = \"HB score\")+\n  theme_r4pde()\n\n\n\n\n\nFigure 3.1: Ordinal scores of the Horsfal-Barrat scale\n\n\n\n\nWe can repeat those procedures to visualize the Chiang scale.\n\nchiang &lt;- tibble::tribble(\n  ~ordinal, ~'range', ~midpoint,\n  0,          '0',     0,   \n  1,  '0+ to 0.1',  0.05,   \n  2,'0.1+ to 0.5',   0.3,   \n  3,  '0.5+ to 1',  0.75,  \n  4,    '1+ to 2',   1.5, \n  5,    '2+ to 5',     3, \n  6,   '5+ to 10',   7.5, \n  7,  '10+ to 20',    15, \n  8,  '20+ to 30',    25, \n  9,  '30+ to 40',    35, \n  10, '40+ to 50',    45,  \n  11, '50+ to 60',    55,\n  12, '60+ to 70',    65,\n  13, '70+ to 80',    75,\n  14, '80+ to 90',    85,\n  15,'90+ to 100',   95\n  )\nknitr::kable(chiang, align = \"c\")\n\n\n\nTable 3.2: The Chiang quantitative ordinal scale used as a tool for assessing plant disease severity\n\n\nordinal\nrange\nmidpoint\n\n\n\n\n0\n0\n0.00\n\n\n1\n0+ to 0.1\n0.05\n\n\n2\n0.1+ to 0.5\n0.30\n\n\n3\n0.5+ to 1\n0.75\n\n\n4\n1+ to 2\n1.50\n\n\n5\n2+ to 5\n3.00\n\n\n6\n5+ to 10\n7.50\n\n\n7\n10+ to 20\n15.00\n\n\n8\n20+ to 30\n25.00\n\n\n9\n30+ to 40\n35.00\n\n\n10\n40+ to 50\n45.00\n\n\n11\n50+ to 60\n55.00\n\n\n12\n60+ to 70\n65.00\n\n\n13\n70+ to 80\n75.00\n\n\n14\n80+ to 90\n85.00\n\n\n15\n90+ to 100\n95.00\n\n\n\n\n\n\n\nchiang |&gt; \n  ggplot(aes(midpoint, ordinal))+\n  geom_point(size =2)+\n  geom_line()+\n  scale_y_continuous(breaks = c(0:15))+\n  scale_x_continuous(breaks = c(0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100))+\n  geom_vline(aes(xintercept = 0), linetype = 2)+\n  geom_vline(aes(xintercept = 0.1), linetype = 2)+\n  geom_vline(aes(xintercept = 0.5), linetype = 2)+\n  geom_vline(aes(xintercept = 1), linetype = 2)+\n  geom_vline(aes(xintercept = 2), linetype = 2)+\n  geom_vline(aes(xintercept = 5), linetype = 2)+\n  geom_vline(aes(xintercept = 10), linetype = 2)+\n  geom_vline(aes(xintercept = 20), linetype = 2)+\n  geom_vline(aes(xintercept = 30), linetype = 2)+\n   geom_vline(aes(xintercept = 40), linetype = 2)+\n   geom_vline(aes(xintercept = 50), linetype = 2)+\n   geom_vline(aes(xintercept = 60), linetype = 2)+\n   geom_vline(aes(xintercept = 70), linetype = 2)+\n   geom_vline(aes(xintercept = 80), linetype = 2)+\n   geom_vline(aes(xintercept = 90), linetype = 2)+\n   geom_vline(aes(xintercept = 100), linetype = 2)+\n  labs(x = \"Percent severity\", y = \"Chiang score\")+\n  theme_r4pde()\n\n\n\n\nFigure 3.2: Ordinal scores of the Chiang scale\n\n\n\n\n\n\n3.1.2 Qualitative ordinal\nIn the qualitative ordinal scale, each class provides a description of the symptoms. An example is the ordinal 0-3 scale for rating eyespot of wheat developed by (Scott and Hollins 1974).\n\nOrdinal scale for rating eyespot of wheat (Scott and Hollins 1974)\n\n\n\n\n\n\nClass\nDescription\n\n\n\n\n0\nuninfected\n\n\n1\nslight eyespot (or or more small lesion occupying in total less than half of the circumference of the stem)\n\n\n2\nmoderate eyespot (one or more lesions occupying at least half the circumference of the stem)\n\n\n3\nsevere eyespot (stem completely girdled by lesions; tissue softened so that lodging would really occur)"
  },
  {
    "objectID": "data-ordinal.html#disease-severity-index-dsi",
    "href": "data-ordinal.html#disease-severity-index-dsi",
    "title": "3  Ordinal scales",
    "section": "3.2 Disease severity index (DSI)",
    "text": "3.2 Disease severity index (DSI)\nSometimes, when quantitative or qualitative ordinal scales are used, the scores given to various individual specimens (the observational units) are transformed into an index on a percentage basis, such as the disease severity index (DSI) which is used as a value for the experimental unit further in data analysis. The DSI is a single number that summarizes a large amount of information on disease severity (Chester 1950). The formula for a DSI (%) can be written as follows:\n\\(DSI = \\frac{∑(class \\ freq. \\ ✕ \\ score \\  of \\ class)} {total \\ n \\ ✕ \\ maximal \\ class} ✕ 100\\)\nThe DSI() and DSI2() are part of the r4pde package. Let’s see how each function works.\nThe DSI() allows to automate the calculation of the disease severity index (DSI) in a series of units (e.g. leaves) that are further classified according to ordinal scores. The function requires three arguments:\n\nunit = the vector of the number of each unit\nclass = the vector of the scores for the units\nmax = the maximum value of the scale\n\nLet’s create a toy data set composed of 12 units where each received an ordinal score. The vectors were arranged as a data frame named scores.\n\nunit &lt;- c(1:12)\nclass &lt;- c(2,3,1,1,3,4,5,0,2,5,2,1)\nratings &lt;- data.frame(unit, class)\nknitr::kable(ratings)\n\n\n\n\nunit\nclass\n\n\n\n\n1\n2\n\n\n2\n3\n\n\n3\n1\n\n\n4\n1\n\n\n5\n3\n\n\n6\n4\n\n\n7\n5\n\n\n8\n0\n\n\n9\n2\n\n\n10\n5\n\n\n11\n2\n\n\n12\n1\n\n\n\n\n\nThe ordinal score used in this example has 6 as the maximum score. The function returns the DSI value.\n\nlibrary(r4pde)\nDSI(ratings$unit, ratings$class, 6)\n\n[1] 40.27778\n\n\nLet’s now deal with a situation of multiple plots (five replicates) where a fixed number of 12 samples were taken and assessed using an ordinal score. Let’s input the data using the tribble() function. Note that the data is in the wide format.\n\nexp &lt;- tibble::tribble(\n  ~rep, ~`1`, ~`2`, ~`3`, ~`4`, ~`5`, ~`6`, ~`7`, ~`8`, ~`9`, ~`10`, ~`11`,~`12`,\n  1, 2, 3, 1, 1, 3, 4, 5, 0, 2, 5, 2, 1,\n  2, 3, 4, 4, 6, 5, 4, 4, 0, 2, 1, 1, 5,\n  3, 5, 6, 6, 5, 4, 2, 0, 0, 0, 0, 2, 0,\n  4, 5, 6, 0, 0, 0, 3, 3, 2, 1, 0, 2, 3, \n  5, 0, 0, 0, 0, 2, 3, 2, 5, 6, 2, 1, 0,\n)\nknitr::kable(exp)\n\n\n\n\nrep\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n1\n2\n3\n1\n1\n3\n4\n5\n0\n2\n5\n2\n1\n\n\n2\n3\n4\n4\n6\n5\n4\n4\n0\n2\n1\n1\n5\n\n\n3\n5\n6\n6\n5\n4\n2\n0\n0\n0\n0\n2\n0\n\n\n4\n5\n6\n0\n0\n0\n3\n3\n2\n1\n0\n2\n3\n\n\n5\n0\n0\n0\n0\n2\n3\n2\n5\n6\n2\n1\n0\n\n\n\n\n\nAfter reshaping the data to the long format, we can calculate the DSI for each plot/replicate as follows:\n\nres &lt;- exp |&gt; \n  pivot_longer(2:13, names_to = \"unit\", values_to = \"class\") |&gt;\n  group_by(rep) |&gt; \n  summarise(DSI = DSI(unit, class, 6))\n\nAnd here we have the results of the DSI for each replicate.\n\nknitr::kable(res, align = \"c\")\n\n\n\n\nrep\nDSI\n\n\n\n\n1\n40.27778\n\n\n2\n54.16667\n\n\n3\n41.66667\n\n\n4\n34.72222\n\n\n5\n29.16667\n\n\n\n\n\nNow our data set is organized as the frequency of each class as follows:\n\nratings2 &lt;- ratings |&gt; \n  dplyr::count(class)\n\nratings2\n\n  class n\n1     0 1\n2     1 3\n3     2 3\n4     3 2\n5     4 1\n6     5 2\n\n\nNow we can apply the DSI2() function. The function requires three arguments:\n\nclass = the number of the respective class\nfreq = the frequency of the class\nmax = the maximum value of the scale\n\n\nlibrary(r4pde)\nDSI2(ratings2$class, ratings2$n, 6)\n\n[1] 40.27778"
  },
  {
    "objectID": "data-ordinal.html#analysis-of-ordinal-data",
    "href": "data-ordinal.html#analysis-of-ordinal-data",
    "title": "3  Ordinal scales",
    "section": "3.3 Analysis of ordinal data",
    "text": "3.3 Analysis of ordinal data\nOrdinal score data typically do not align well with the assumptions of traditional parametric statistical methods. Given this challenge, non-parametric methods have emerged as a compelling alternative for handling ordinal plant pathological data (Shah and Madden 2004). Unlike their parametric counterparts, these methods do not rest on the presumption of a specific distribution for the underlying population, offering greater flexibility in accommodating the intricacies inherent to ordinal data. On the other hand, when the conditions are right, parametric methods can also be harnessed effectively.\nA common strategy, particularly for ordinal scores, involves converting these values into the mid-points of their corresponding percent scales. This transformation renders the data more amenable to parametric analyses. However, the mid-point conversion has been criticized in the literature as it may amplify the imprecision, especially when the interval size is wide, and because it does not really reflect a true value, but an interval for each estimate (Chiang et al. 2023; Onofri et al. 2018).\nLet’s see some examples of analysis using the mid-point conversion in a parametric framework as well as the non-parametric tests.\n\n3.3.1 Example data\nWe will use a data set made available in an article on the application of the survival analysis technique to test hypotheses when using quantitative ordinal scales (Chiang et al. 2023). The data and codes used in the paper can be found in the specified GitHub repository. The data is structured into four treatments, each with 30 ratings ranging from a score of 1 to 6 on the H-B scale.\n\n# Create the vectors for the treatments\ntrAs &lt;- c(5,4,2,5,5,4,4,2,5,2,2,3,4,3,2,\n          2,6,2,2,4,2,4,2,4,5,3,4,2,2,3)\ntrBs &lt;- c(5,3,2,4,4,5,4,5,4,4,6,4,5,5,5,\n          2,6,2,3,5,2,6,4,3,2,5,3,5,4,5)\ntrCs &lt;- c(2,3,1,4,1,1,4,1,1,3,2,1,4,1,1,\n          2,5,2,1,3,1,4,2,2,2,4,2,3,2,2)\ntrDs &lt;- c(5,5,4,5,5,6,6,4,6,4,3,5,5,6,4,\n          6,5,6,5,4,5,5,5,3,5,6,5,5,5,6)\n\n# Create the tibble\ndat_ordinal &lt;- tibble::tibble(\n  treatment = rep(c(\"A\", \"B\", \"C\", \"D\"), \n                  each = 30),\n  score = c(trAs, trBs, trCs, trDs)\n)\n\ndat_ordinal\n\n# A tibble: 120 × 2\n   treatment score\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 A             5\n 2 A             4\n 3 A             2\n 4 A             5\n 5 A             5\n 6 A             4\n 7 A             4\n 8 A             2\n 9 A             5\n10 A             2\n# ℹ 110 more rows\n\n\nBecause the ordinal response was obtained using an interval scale, the mid-point of the score was also obtained.\n\n# Create the vectors for the treatments\ntrAm &lt;- c(18.5,9,1.5,18.5,18.5,9,9,1.5,\n          18.5,1.5,1.5,4.5,9,4.5,1.5,1.5,\n          37.5,1.5,1.5,9,1.5,9,1.5,9,18.5,\n          4.5,9,1.5,1.5,4.5)\ntrBm &lt;- c(18.5,4.5,1.5,9,9,18.5,9,18.5,9,\n          9,37.5,9,18.5,18.5,18.5,1.5,37.5,\n          1.5, 4.5,18.5,1.5,37.5,9,4.5,1.5,\n          18.5,4.5,18.5,9,18.5)\ntrCm &lt;- c(1.5,4.5,0,9,0,0,9,0,0,4.5,1.5,0,9\n          ,0,0,1.5,18.5,1.5,0,4.5,0,9,1.5,1.5,\n          1.5,9,1.5,4.5,1.5,1.5)\ntrDm &lt;- c(18.5,18.5,9,18.5,18.5,37.5,37.5,9,\n          37.5,9,4.5,18.5,18.5,37.5,9,37.5,\n       18.5,37.5,18.5,9,18.5,18.5,18.5,4.5,18.5,\n       37.5,18.5,18.5,18.5,37.5)\n\n# Create the tibble\ndat_ordinal_mp &lt;- tibble::tibble(\n  treatment = rep(c(\"A\", \"B\", \"C\", \"D\"), \n                  each = 30),\n  midpoint = c(trAm, trBm, trCm, trDm)\n)\ndat_ordinal_mp\n\n# A tibble: 120 × 2\n   treatment midpoint\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 A             18.5\n 2 A              9  \n 3 A              1.5\n 4 A             18.5\n 5 A             18.5\n 6 A              9  \n 7 A              9  \n 8 A              1.5\n 9 A             18.5\n10 A              1.5\n# ℹ 110 more rows\n\n\nThe visualization using box-plots suggests differences between the treatments.\n\np_score &lt;- dat_ordinal |&gt; \n  ggplot(aes(treatment, score))+\n  geom_boxplot(outlier.colour = NA)+\n  geom_point()+\n  theme_r4pde()\n\np_mp &lt;- dat_ordinal_mp |&gt; \n  ggplot(aes(treatment, midpoint))+\n  geom_boxplot(outlier.colour = NA)+\n  geom_point()+\n  theme_r4pde()\n\nlibrary(patchwork)\np_score | p_mp\n\n\n\n\n\n\n3.3.2 Parametric (mid-point conversion)\nParametric methods can be employed when analyzing data, provided that the underlying assumptions of these methods are satisfied. For data that is based on ordinal scores, one way to apply parametric methods is to convert these scores into the mid-points of their corresponding percent scales. Once converted, the data is better suited for parametric analyses. Among the available techniques, the most frequently utilized is the application of an ANOVA (Analysis of Variance) model. This model is particularly useful for testing the null hypothesis, which posits that there are no significant differences among the treatments being studied.\nWe can use the aov function to fit the model and check the parametric assumptions using the DHARMa package.\n\nm1_mp &lt;- aov(midpoint ~ treatment, data = dat_ordinal_mp)\n# normality and homocedasticity tests\nlibrary(DHARMa)\nplot(simulateResiduals(m1_mp))\n\n\n\n\nSince both normality and homoscedasticity assumptions are violated in our initial model, we will halt further analysis using this model. Instead, we will explore an alternative approach that includes data transformation (logit).\n\n# data transformation\nlibrary(car)\n# transform midpoint to proportion\ndat_ordinal_mp$midpoint2 &lt;- dat_ordinal_mp$midpoint/100\n\nm2_mp &lt;- aov(logit(midpoint2) ~ treatment, data = dat_ordinal_mp)\nplot(simulateResiduals(m2_mp)) # assumptions are met\n\n\n\n\nNow that both assumptions have been met, we can proceed to use a means comparison test to determine which treatments differ from one another, with the assistance of the {emmeans} package. It’s important to note that the results are displayed in the original scale after transformation, specifically when using type = \"response.\n\nlibrary(emmeans)\nmeans_m2_mp &lt;- emmeans(m2_mp, \"treatment\", type = \"response\")\nmeans_m2_mp\n\n treatment response      SE  df lower.CL upper.CL\n A           0.0808 0.00983 116   0.0634   0.1026\n B           0.1248 0.01446 116   0.0989   0.1564\n C           0.0461 0.00582 116   0.0358   0.0591\n D           0.2071 0.02173 116   0.1674   0.2535\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nThe best way to visualize the differences between treatments is using the pwpm function to display a matrix of estimates, pairwise differences, and P values. Most commonly, the compact letter display is used for means comparison.\n\n# matrix\npwpm(means_m2_mp)\n\n         A        B        C        D\nA [0.0808]   0.0530   0.0094   &lt;.0001\nB    0.617 [0.1248]   &lt;.0001   0.0085\nC    1.820    2.953 [0.0461]   &lt;.0001\nD    0.337    0.546    0.185 [0.2071]\n\nRow and column labels: treatment\nUpper triangle: P values   null = 1  adjust = \"tukey\"\nDiagonal: [Estimates] (response)   type = \"response\"\nLower triangle: Comparisons (odds.ratio)   earlier vs. later\n\n# compact letter display\nlibrary(multcomp)\ncld(means_m2_mp)\n\n treatment response      SE  df lower.CL upper.CL .group\n C           0.0461 0.00582 116   0.0358   0.0591  1    \n A           0.0808 0.00983 116   0.0634   0.1026   2   \n B           0.1248 0.01446 116   0.0989   0.1564   2   \n D           0.2071 0.02173 116   0.1674   0.2535    3  \n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \nP value adjustment: tukey method for comparing a family of 4 estimates \nTests are performed on the log odds ratio scale \nsignificance level used: alpha = 0.05 \nNOTE: Compact letter displays can be misleading\n      because they show NON-findings rather than findings.\n      Consider using 'pairs()', 'pwpp()', or 'pwpm()' instead. \n\n\n\n\n3.3.3 Non-parametric (ordinal score)\nBecause ordinal score data generally do not meet the assumptions of traditional parametric statistical methods, non-parametric methods can be considered as an alternative. Such methods have been proposed for analyzing ordinal data in plant pathology (Shah and Madden 2004). For our example, when more than two treatments are involved, a Kruskal-Wallis test can be utilized. The kruskal function of the {agricolae} package does the job we want. Note that in this case we use the ordinal score directly and not the mid-point value.\n\nlibrary(agricolae)\nkruskal(dat_ordinal$score, dat_ordinal$treatment, console = TRUE)\n\n\nStudy: dat_ordinal$score ~ dat_ordinal$treatment\nKruskal-Wallis test's\nTies or no Ties\n\nCritical Value: 52.34422\nDegrees of freedom: 3\nPvalue Chisq  : 2.529543e-11 \n\ndat_ordinal$treatment,  means of the ranks\n\n  dat_ordinal.score  r\nA          52.05000 30\nB          69.58333 30\nC          29.61667 30\nD          90.75000 30\n\nPost Hoc Analysis\n\nt-Student: 1.980626\nAlpha    : 0.05\nMinimum Significant Difference: 13.1991 \n\nTreatments with the same letter are not significantly different.\n\n  dat_ordinal$score groups\nD          90.75000      a\nB          69.58333      b\nA          52.05000      c\nC          29.61667      d\n\n\n\n\n3.3.4 Survival analysis\nA method based on survival analysis was more recently introduced for the analysis of ordinal data (Chiang et al. 2023). The authors developed an R script called “CompMuCens”, which facilitates the comparison of multiple treatment means for plant disease severity data. This is achieved through nonparametric survival analysis using class- or interval-based data derived from a quantitative ordinal scale. The code is accessible in this repository and has been incorporated into the {r4pde} package.\nWe continue with our working example data. Since the expected variable name for the score in the function is x, we must adjust our data frame accordingly.\n\nnames(dat_ordinal) &lt;- c(\"treatment\", \"x\")\n\nThe CompMuCens() function uses the ictest() from the {interval} package to conduct nonparametric survival analysis. Detailed explanation of the function’s input and output can be found here. We just need to set the dat and the scale arguments. The scale will be used to convert the scores in to the defined interval which is used as response variable in the analysis. For example, if the score is 2, the respective limits of the interval will be 3 and 6. For 7, the limits will be 75 and 88. The function takes care of this conversion based on the inputted scale values.\n\nlibrary(interval)\nlibrary(r4pde)\n\nscale &lt;- c(0,3,6,12,25,50,75,88,94,97,100, 100)\nCompMuCens(dat_ordinal, scale)\n\n$U.Score\n  treatment      score\n1         D -15.125000\n2         B  -4.541667\n3         A   4.225000\n4         C  15.441667\n\n$Hypothesis.test\n  treat1 treat2 p-value for H0: treat1 ≤ treat2 p-value for H0: treat1 = treat2\n1      D      B                    0.0018767018                              NA\n2      B      A                    0.0113215174                              NA\n3      A      C                    0.0007456484                              NA\n\n$adj.Signif\n[1] 0.01666667\n\n$Conclusion\n[1] \"D&gt;B&gt;A&gt;C\"\n\n\nThe outcomes are three: the ordered scores for each treatment, the pairwise comparison between treatments and the significance levels, followed by a conclusion section. In this example all treatments differ from each other.\n\n\n3.3.5 Interpretation\nUpon comparing the three methods, it becomes evident that there is a marked distinction in their outcomes. The Kruskal-Wallis and the survival analysis tests suggested a significant difference between treatment A and B, whereas the parametric counterpart did not. However, it’s crucial to note that the P-value is borderline significant at 0.0530, being just slightly above the conventional threshold of 0.05. Given this, it appears that the conversion to the mid-point might have resulted in a type II error, wherein a genuine difference between the treatments might have been missed or overlooked. A detailed comparison between the mid-point and survival analysis has been presented in the paper, which is worth a reading (Chiang et al. 2023)\n\n\n\n\nChester, K. S. 1950. Plant disease losses : Their appraisal and interpretation /. Available at: http://dx.doi.org/10.5962/bhl.title.86198.\n\n\nChiang, K.-S., and Bock, C. H. 2021. Understanding the ramifications of quantitative ordinal scales on accuracy of estimates of disease severity and data analysis in plant pathology. Tropical Plant Pathology. 47:58–73 Available at: http://dx.doi.org/10.1007/s40858-021-00446-0.\n\n\nChiang, K.-S., Chang, Y. M., Liu, H. I., Lee, J. Y., El Jarroudi, M., and Bock, C. 2023. Survival Analysis as a Basis to Test Hypotheses When Using Quantitative Ordinal Scale Disease Severity Data. Phytopathology®. Available at: http://dx.doi.org/10.1094/PHYTO-02-23-0055-R.\n\n\nChiang, K.-S., Liu, S.-C., Bock, C. H., and Gottwald, T. R. 2014. What Interval Characteristics Make a Good Categorical Disease Assessment Scale? Phytopathology®. 104:575–585 Available at: http://dx.doi.org/10.1094/phyto-10-13-0279-r.\n\n\nHebert, T. T. 1982. The rationale for the horsfall-barratt plant disease assessment scale. Phytopathology. 72:1269 Available at: http://dx.doi.org/10.1094/phyto-72-1269.\n\n\nLiu, H. I., Tsai, J. R., Chung, W. H., Bock, C. H., and Chiang, K. S. 2019. Effects of Quantitative Ordinal Scale Design on the Accuracy of Estimates of Mean Disease Severity. Agronomy. 9:565 Available at: http://dx.doi.org/10.3390/agronomy9090565.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. The study of plant disease epidemics. The American Phytopathological Society. Available at: http://dx.doi.org/10.1094/9780890545058.\n\n\nNutter, F. W., and Esker, P. D. 2006. The Role of Psychophysics in Phytopathology: The WeberFechner Law Revisited. European Journal of Plant Pathology. 114:199–213 Available at: http://dx.doi.org/10.1007/s10658-005-4732-9.\n\n\nOnofri, A., Piepho, H.-P., and Kozak, M. 2018. Analysing censored data in agricultural research: A review with examples and software tips. Annals of Applied Biology. 174:3–13 Available at: http://dx.doi.org/10.1111/aab.12477.\n\n\nScott, P. R., and Hollins, T. W. 1974. Effects of eyespot on the yield of winter wheat. Annals of Applied Biology. 78:269–279 Available at: http://dx.doi.org/10.1111/j.1744-7348.1974.tb01506.x.\n\n\nShah, D. A., and Madden, L. V. 2004. Nonparametric Analysis of Ordinal Data in Designed Factorial Experiments. Phytopathology®. 94:33–43 Available at: http://dx.doi.org/10.1094/PHYTO.2004.94.1.33."
  },
  {
    "objectID": "data-actual-severity.html#the-actual-severity-measure",
    "href": "data-actual-severity.html#the-actual-severity-measure",
    "title": "4  Image analysis",
    "section": "4.1 The actual severity measure",
    "text": "4.1 The actual severity measure\nAmong the various methods to express plant disease severity, the percent area affected (or symptomatic) by the disease is one of the most common, especially when dealing with diseases that affect leaves. In order to evaluate whether visual estimates of plant disease severity are sufficiently accurate (as discussed in the previous chapter), we require the actual severity values. These are also essential when creating Standard Area Diagrams (SADs), which are diagrammatic representations of severity values used as a reference either before or during visual assessment to standardize and produce more accurate results across different raters (Del Ponte et al. 2017).\nThe actual severity values are typically approximated using image analysis, wherein the image is segmented, and each pixel is categorized into one of three classes:\n\nDiseased (or symptomatic)\nNon-diseased (or healthy)\nBackground (the non-plant portion of the image)\n\nThe ratio of the diseased area to the total area of the unit (e.g., the entire plant organ or section of the image) yields the proportion of the diseased area, or the percent area affected (when multiplied by 100). Researchers have employed various proprietary or open-source software to determine the actual severity, as documented in a review on Standard Area Diagrams (Del Ponte et al. 2017).\nIn this section, we will utilize the measure_disease() function from the {pliman} (Plant IMage ANalysis) R package (Olivoto 2022), and its variations, to measure the percent area affected. The package was compared with other software for determining plant disease severity across five different plant diseases and was shown to produce accurate results in most cases (Olivoto et al. 2022).\nThere are essentially two methods to measure severity. The first is predicated on image palettes that define each class of the image. The second relies on RGB-based indices (Alves et al. 2021). Let’s explore the first method, as well as an interactive approach to setting color palettes."
  },
  {
    "objectID": "data-actual-severity.html#image-palettes",
    "href": "data-actual-severity.html#image-palettes",
    "title": "4  Image analysis",
    "section": "4.2 Image palettes",
    "text": "4.2 Image palettes\nThe most crucial step is the initial one, where the user needs to correctly define the color palettes for each class. In pliman, the palettes can be separate images representing each of the three classes: background (b), symptomatic (s), and healthy (h).\nThe reference image palettes can be constructed by manually sampling small areas of the image and creating a composite image. As expected, the results may vary depending on how these areas are selected. A study that validated pliman for determining disease severity demonstrated the effect of different palettes prepared independently by three researchers, in which the composite palette (combining the three users) was superior (Olivoto et al. 2022). During the calibration of the palettes, examining the processed masks is crucial to create reference palettes that are the most representative of the respective class.\nIn this example, I manually selected and pasted several sections of images representing each class from a few leaves into a Google slide. Once the image palette was ready, I exported each one as a separate PNG image file (JPG also works). These files were named: sbr_b.png, sbr_h.png, and sbr_s.png. They can be found here in this folder for downloading.\n\n\n\nFigure 4.1: Preparation of image palettes by manually sampling fraction of the images that represent background, heatlhy leaf and lesions\n\n\nNow that we have the image palettes, we need to import them into the environment, using image_import() function for further analysis. Let’s create an image object for each palette named h (healthy), s (symptoms) and b (background).\n\nlibrary(pliman)\nh &lt;- image_import(\"imgs/sbr_h.png\")\ns &lt;- image_import(\"imgs/sbr_s.png\")\nb &lt;- image_import(\"imgs/sbr_b.png\")\n\nWe can visualize the imported image palettes using the image_combine() function.\n\nimage_combine(h, s, b, ncol =3)\n\n\n\n\nFigure 4.2: Image palettes created to segment images into background, sypomtoms and healthy area of the image\n\n\n\n\nAn alternative way to set the palettes is to use the pick_palette() function. It allows to manually pick the colors for each class by clicking on top of the imported image. We can use one of the original images or a composite images with portions of several leaves. Let’s use here one of the original images and pick the background colors and assigned to b vector.\n\nimg &lt;- image_import(\"imgs/originals/img5.png\")\nb &lt;- pick_palette(img)\n\nThe original image is displayed and the user needs to click on the background colors to select the pixels. A message will be displayed as follows:\n\nUse the first mouse button to pick up points in the plot. Press Esc to exit.\n\n\n\n\nSoybean rust leaf with small red dots at the upper leaf portion that indicate which colors were selected for the background\n\n\nAfter pressing ESC, the image palette for the background is constructed and can be displayed.\n\nimage_combine(b)\n\n\n\n\nImage generating after picking the palette colors for the background of the leaf\n\n\nNow, we can proceed and pick the colors for the other categories following the same logic.\n\n# Symptoms\ns &lt;- pick_palette(img)\n\n# healthy\nh &lt;- pick_palette(img)"
  },
  {
    "objectID": "data-actual-severity.html#measuring-severity",
    "href": "data-actual-severity.html#measuring-severity",
    "title": "4  Image analysis",
    "section": "4.3 Measuring severity",
    "text": "4.3 Measuring severity\n\n4.3.1 Single image\n\n4.3.1.1 Using color palettes\nTo determine severity in a single image (e.g. img46.png), the image file needs to be loaded and assigned to an object using the same image_import() function used to load the palettes. We can then visualize the image, again using image_combine().\n\n\n\n\n\n\nTip\n\n\n\nThe collection of images used in this chapter can be found here.\n\n\n\nimg &lt;- image_import(\"imgs/originals/img46.png\")\nimage_combine(img)\n\n\n\n\nFigure 4.3: Imported image for further analysis\n\n\n\n\nNow the engaging part starts with the measure_disease() function. Four arguments are required when using the reference image palettes: the image representing the target image and the three images of the color palettes. As the author of the package states, “pliman will take care of all the details!” The severity is the value displayed under ‘symptomatic’ in the output.\n\nset.seed(123)\nmeasure_disease(\n  img = img,\n  img_healthy = h,\n  img_symptoms = s,\n  img_background = b\n)\n\n\n\n\n$severity\n   healthy symptomatic\n1 92.68077    7.319234\n\n$shape\nNULL\n\n$statistics\nNULL\n\nattr(,\"class\")\n[1] \"plm_disease\"\n\n\nIf we want to show the mask with two colors instead of the original, we can set to FALSE two “show_” arguments:\n\nset.seed(123)\nmeasure_disease(\n  img = img,\n  img_healthy = h,\n  img_symptoms = s,\n  img_background = b,\n  show_contour = FALSE,\n  show_original = FALSE\n)\n\n\n\n\n$severity\n   healthy symptomatic\n1 92.68077    7.319234\n\n$shape\nNULL\n\n$statistics\nNULL\n\nattr(,\"class\")\n[1] \"plm_disease\"\n\n\n\n\n\n4.3.2 Multiple images\nMeasuring severity in single images is indeed engaging, but we often deal with multiple images, not just one. Using the above procedure to process each image individually would be time-consuming and potentially tedious.\nTo automate the process, {pliman} offers a batch processing approach. Instead of using the img argument, one can use the pattern argument and define the prefix of the image names. Moreover, we also need to specify the directory where the original files are located.\nIf the user wants to save the processed masks, they should set the save_image argument to TRUE and also specify the directory where the images will be saved. Here’s an example of how to process 10 images of soybean rust symptoms. The output is a list object with the measures of the percent healthy and percent symptomatic area for each leaf in the severity object.\n\npliman &lt;- measure_disease(\n  pattern = \"img\",\n  dir_original = \"imgs/originals\" ,\n  dir_processed = \"imgs/processed\",\n  save_image = TRUE,\n  img_healthy = h,\n  img_symptoms = s,\n  img_background = b,\n  verbose = FALSE,\n  plot = FALSE\n)\nseverity &lt;- pliman$severity\nseverity\n\n     img  healthy symptomatic\n1  img11 70.79655  29.2034481\n2  img35 46.94177  53.0582346\n3  img37 60.47440  39.5256013\n4  img38 79.14060  20.8594011\n5  img46 93.14958   6.8504220\n6   img5 20.53175  79.4682534\n7  img63 97.15669   2.8433141\n8  img67 99.83720   0.1627959\n9  img70 35.56684  64.4331583\n10 img75 93.04453   6.9554686\n\n\nWhen the argument save_image is set to TRUE, the images are all saved in the folder with the standard prefix “proc.”\n\n\n\nFigure 4.4: Images created by pliman and exported to a specific folder\n\n\nLet’s have a look at one of the processed images.\n\n\n\nFigure 4.5: Figure created by pliman after batch processing to segment the images and calculate percent area covered by symptoms. The symptomatic area is delinated in the image.\n\n\n\n4.3.2.1 More than a target per image\n{pliman} offers a custom function to estimate the severity in multiple targets (e.g. leaf) per image. This is convenient to decrease the time when scanning the specimens, for example. Let’s combine three soybean rust leaves into a single image and import it for processing. We will further set the index_lb (leaf background), save_image to TRUE and inform the directory for the processed images using dir_processed.\n\nimg2 &lt;- image_import(\"imgs/soybean_three.png\")\nimage_combine(img2)\n\n\n\n\nFigure 4.6: Imported image with multiple targets in a single image for further analysis using measure_disease_byl() function of the {pliman} package\n\n\n\n\n\n pliman2 &lt;- measure_disease_byl(img = img2,\n                        index_lb = b,\n                        img_healthy = h,\n                        img_symptoms = s, \n                        save_image = TRUE,\n                        dir_processed = \"imgs/proc\")\n\n\n\n\n\n\n\n\n\n pliman2$severity\n\n  img leaf  healthy symptomatic\n1 img    1 59.11737   40.882632\n2 img    2 61.36616   38.633841\n3 img    3 93.21756    6.782436\n\n\nThe original image is splited and the individual new images are saved in the proc folder.\n\n\n\nIndividual images of the soybean leaves after processed using the measure_disease_byl function of the {pliman} package."
  },
  {
    "objectID": "data-actual-severity.html#how-good-are-these-measurements",
    "href": "data-actual-severity.html#how-good-are-these-measurements",
    "title": "4  Image analysis",
    "section": "4.4 How good are these measurements?",
    "text": "4.4 How good are these measurements?\nThese 10 images were previously processed in QUANT software for measuring severity which is also based on image threshold. Let’s create a tibble for the image code and respective “actual” severity - assuming QUANT measures as reference.\n\nlibrary(tidyverse)\nlibrary(r4pde)\nquant &lt;- tribble(\n  ~img, ~actual,\n   \"img5\",     75,\n  \"img11\",     24,\n  \"img35\",     52,\n  \"img37\",     38,\n  \"img38\",     17,\n  \"img46\",      7,\n  \"img63\",    2.5,\n  \"img67\",   0.25,\n  \"img70\",     67,\n  \"img75\",     10\n  )\n\nWe can now combine the two dataframes and produce a scatter plot relating the two measures.\n\ndat &lt;- left_join(severity, quant)\n\nJoining with `by = join_by(img)`\n\ndat %&gt;%\n  ggplot(aes(actual, symptomatic)) +\n  geom_point(size = 3, shape = 16) +\n  ylim(0, 100) +\n  xlim(0, 100) +\n  geom_abline(slope = 1, intercept = 0) +\n  labs(x = \"Quant\",\n       y = \"pliman\")+\n  theme_r4pde()\n\n\n\n\nFigure 4.7: Scatter plot for the relationship between severity values measured by pliman and Quant software\n\n\n\n\nThe concordance correlation coefficient is a test for agreement between two observers or two methods (see previous chapter). It is an indication of how accurate the pliman measures are compared with a standard. The coefficient is greater than 0.99 (1.0 is perfect concordance), suggesting an excellent agreement!\n\nlibrary(epiR)\nccc &lt;- epi.ccc(dat$actual, dat$symptomatic)\nccc$rho.c\n\n        est     lower     upper\n1 0.9940835 0.9774587 0.9984566\n\n\nIn conclusion, as mentioned earlier, the most critical step is defining the reference image palettes. A few preliminary runs may be necessary for some images to ensure that the segmentation is being carried out correctly, based on visual judgment. This is not different from any other color-threshold based methods, where the choices made by the user impact the final result and contribute to variation among assessors. The drawbacks are the same as those encountered with direct competitors, namely, the need for images to be taken under uniform and controlled conditions, especially with a contrasting background."
  },
  {
    "objectID": "data-actual-severity.html#creating-palettes-interactively",
    "href": "data-actual-severity.html#creating-palettes-interactively",
    "title": "4  Image analysis",
    "section": "4.5 Creating palettes interactively",
    "text": "4.5 Creating palettes interactively\nPliman offers another function measure_disease_iter() which allows the user to pick up samples in the image to create the color palettes for each required class (background, healthy and symptoms). Check the video below.\n\n\n\n\n\nAlves, K. S., Guimarães, M., Ascari, J. P., Queiroz, M. F., Alfenas, R. F., Mizubuti, E. S. G., et al. 2021. RGB-based phenotyping of foliar disease severity under controlled conditions. Tropical Plant Pathology. 47:105–117 Available at: http://dx.doi.org/10.1007/S40858-021-00448-Y.\n\n\nDel Ponte, E. M., Pethybridge, S. J., Bock, C. H., Michereff, S. J., Machado, F. J., and Spolti, P. 2017. Standard Area Diagrams for Aiding Severity Estimation: Scientometrics, Pathosystems, and Methodological Trends in the Last 25 Years. Phytopathology®. 107:1161–1174 Available at: http://dx.doi.org/10.1094/PHYTO-02-17-0069-FI.\n\n\nOlivoto, T. 2022. Lights, camera, pliman! An R package for plant image analysis. Methods in Ecology and Evolution. 13:789–798 Available at: http://dx.doi.org/10.1111/2041-210X.13803.\n\n\nOlivoto, T., Andrade, S. M. P., and M. Del Ponte, E. 2022. Measuring plant disease severity in R: introducing and evaluating the pliman package. Tropical Plant Pathology. 47:95–104 Available at: http://dx.doi.org/10.1007/s40858-021-00487-5."
  },
  {
    "objectID": "data-accuracy.html",
    "href": "data-accuracy.html",
    "title": "5  Reliability and accuracy",
    "section": "",
    "text": "6 Severity data"
  },
  {
    "objectID": "data-accuracy.html#terminology",
    "href": "data-accuracy.html#terminology",
    "title": "5  Reliability and accuracy",
    "section": "6.1 Terminology",
    "text": "6.1 Terminology\nDisease severity, mainly when expressed in percent area diseased assessed visually, is acknowledged as a more difficult and less time- and cost-effective plant disease variable to obtain. However, errors may occur even when assessing a more objective measure such as incidence. This is the case when an incorrect assignment or confusion of symptoms occur. In either case, the quality of the assessment of any disease variable is very important and should be gauged in the studies. Several terms can be used when evaluating the quality of disease assessments, including reliability, precision, accuracy or agreement.\nReliability: The extent to which the same estimates or measurements of diseased specimens obtained under different conditions yield similar results. There are two types. The inter-rater reliability (or reproducibility) is a measure of consistency of disease assessment across the same specimens between raters or devices. The intra-rater reliability (or repeatability) measures consistency by the same rater or instrument on the same specimens (e.g. two assessments in time by the same rater).\n\n\n\nFigure 6.1: Two types of reliability of estimates or measures of plant disease intensity\n\n\nPrecision: A statistical term to express the measure of variability of the estimates or measurements of disease on the same specimens obtained by different raters (or instruments). However, reliable or precise estimates (or measurements) are not necessarily close to an actual value, but precision is a component of accuracy or agreement.\nAccuracy or agreement: These two terms can be treated as synonymous in plant pathological research. They refer to the closeness (or concordance) of an estimate or measurement to the actual severity value for a specimen on the same scale. Actual values may be obtained using various methods, against which estimates or measurements using an experimental assessment method are compared.\nAn analogy commonly used to explain accuracy and precision is the archer shooting arrows at a target and trying to hit the bull’s eye (center of the target) with each of five arrows. The figure below is used to demonstrate four situations from the combination of two levels (high and low) for precision and accuracy. The figure was produced using the ggplot function of ggplot2 package.\n\n\nCode\nlibrary(ggplot2)\ntarget &lt;- \n  ggplot(data.frame(c(1:10),c(1:10)))+\n  geom_point(aes(x = 5, y = 5), size = 71.5, color = \"black\")+\n  geom_point(aes(x = 5, y = 5), size = 70, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 60, color = \"white\")+\n  geom_point(aes(x = 5, y = 5), size = 50, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 40, color = \"white\")+\n  geom_point(aes(x = 5, y = 5), size = 30, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 20, color = \"white\")+\n  geom_point(aes(x = 5, y = 5), size = 10, color = \"#99cc66\")+\n  geom_point(aes(x = 5, y = 5), size = 4, color = \"white\")+\n  ylim(0,10)+\n  xlim(0,10)+\n  theme_void()\n\nhahp &lt;- target +\n  labs(subtitle = \"High Accuracy High Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 5, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5, y = 5.2), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5, y = 4.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.8, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.2, y = 5), shape = 4, size =2, color = \"blue\")\n\n\nlahp &lt;- target +\n  labs(subtitle = \"Low Accuracy High Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 6, y = 6), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 6, y = 6.2), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 6, y = 5.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.8, y = 6), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 6.2, y = 6), shape = 4, size =2, color = \"blue\")\n\n\nhalp &lt;- target +\n  labs(subtitle = \"High Accuracy Low Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 5, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5, y = 5.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.8, y = 4.4), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.4, y = 5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.6, y = 5.6), shape = 4, size =2, color = \"blue\")\n\nlalp &lt;- target +\n  labs(subtitle = \"Low Accuracy Low Precision\")+\n  theme(plot.subtitle = element_text(hjust = 0.5))+\n  geom_point(aes(x = 5.5, y = 5.5), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.5, y = 5.4), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.2, y = 6.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 4.8, y = 3.8), shape = 4, size =2, color = \"blue\")+\n  geom_point(aes(x = 5.2, y = 3), shape = 4, size =2, color = \"blue\")\n\n\nlibrary(patchwork)\n(hahp | lahp) /\n(halp | lalp)\n\n\n\n\n\nFigure 6.2: The accuracy and precision of the archer is determined by the location of the group of arrows\n\n\n\n\nAnother way to visualize accuracy and precision is via scatter plots for the relationship between the actual values and the estimates.\n\n\nCode\nlibrary(tidyverse)\nlibrary(r4pde)\ntheme_set(theme_r4pde())\ndat &lt;- \ntibble::tribble(\n  ~actual,   ~ap,   ~ip,   ~ai,   ~ii,\n        0,     0,    10,     0,    25,\n       10,    10,    20,     5,    10,\n       20,    20,    30,    30,    10,\n       30,    30,    40,    30,    45,\n       40,    40,    50,    30,    35,\n       50,    50,    60,    60,    65,\n       60,    60,    70,    50,    30\n  )\n\nap &lt;- dat |&gt; \n  ggplot(aes(actual, ap))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n    geom_smooth(method = \"lm\")+\n   geom_point(color = \"#99cc66\", size = 3)+\n   ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"High Acccuracy High Precision\")\n\nip &lt;- dat |&gt; \n  ggplot(aes(actual, ip))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n  geom_smooth(method = \"lm\", se = F)+\n  geom_point(color = \"#99cc66\", size = 3)+\n  ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"Low Acccuracy High Precision\")\n\nai &lt;- dat |&gt; \n  ggplot(aes(actual, ai))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n  geom_smooth(method = \"lm\", se = F)+\n  geom_point(color = \"#99cc66\", size = 3)+\n  ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"High Acccuracy Low precision\")\n\nii &lt;- dat |&gt; \n  ggplot(aes(actual, ii))+\n  geom_abline(intercept = 0, slope = 1, \n              linetype = 2, size = 1)+\n  geom_smooth(method = \"lm\", se = F)+\n  geom_point(color = \"#99cc66\", size = 3)+\n  ylim(0,70)+\n  xlim(0,70)+\n  labs(x = \"Actual\", y = \"Estimate\",\n       subtitle = \"Low Acccuracy Low Precision\")\n\nlibrary(patchwork)\n(ap | ip) / (ai | ii)\n\n\n\n\n\nFigure 6.3: Scatter plots for the relationship between actual and estimated values representing situations of low or high precision and accuracy. The dashed line indicates the perfect concordance and the solid blue line represents the fit of the linear regression model"
  },
  {
    "objectID": "data-accuracy.html#statistical-summaries",
    "href": "data-accuracy.html#statistical-summaries",
    "title": "5  Reliability and accuracy",
    "section": "6.2 Statistical summaries",
    "text": "6.2 Statistical summaries\nA formal assessment of the quality of estimates or measures is made using statistical summaries of the data expressed as indices that represent reliability, precision and accuracy. These indices can further be used to test hypothesis such as if one or another method is superior than the other. The indices or the tests vary according to the nature of the variable, whether continuous, binary or categorical.\n\n6.2.1 Inter-rater reliability\nTo calculate measures of inter-rater reliability (or reproducibility) we will work with a fraction of a larger dataset used in a published study. There, the authors tested the effect of standard area diagrams (SADs) on the reliability and accuracy of visual estimates of severity of soybean rust.\nThe selected dataset consists of five columns with 20 rows. The first is the leaf number and the others correspond to assessments of percent soybean rust severity by four raters (R1 to R4). Each row correspond to one symptomatic leaf. Let’s assign the tibble to a dataframe called sbr (an acronym for soybean rust). Note that the variable is continuous.\n\nlibrary(tidyverse)\nsbr &lt;- tribble(\n~leaf, ~R1, ~R2,  ~R3, ~R4,\n1, 0.6, 0.6,  0.7, 0.6,\n2,   2, 0.7,    5,   1,\n3,   5,   5,    8,   5,\n4,   2,   4,    6,   2,\n5,   6,  14,   10,   7,\n6,   5,   6,   10,   5,\n7,  10,  18, 12.5,  12,\n8,  15,  30,   22,  10,\n9,   7,   2,   12,   8,\n10,  6,   9, 11.5,   8,\n11,  7,   7,   20,   9,\n12,  6,  23,   22,  14,\n13, 10,  35, 18.5,  20,\n14, 19,  10,    9,  10,\n15, 15,  20,   19,  20,\n16, 17,  30,   18,  13,\n17, 19,  53,   33,  38,\n18, 17, 6.8,   15,   9,\n19, 15,  20,   18,  16,\n20, 18,  22,   24,  15\n         )\n\nLet’s explore the data using various approaches. First, we can visualize how the individual estimates by the raters differ for a same leaf.\n\n# transform from wide to long format\nsbr2 &lt;- sbr |&gt; \n  pivot_longer(2:5, names_to = \"rater\",\n               values_to = \"estimate\") \n\n# create the plot\nsbr2 |&gt; \n  ggplot(aes(leaf, estimate, color = rater,\n             group = leaf))+\n  geom_line(color = \"black\")+\n  geom_point(size = 2)+\n  labs(y = \"Severity estimate (%)\",\n       x = \"Leaf number\",\n       color = \"Rater\")\n\n\n\n\nFigure 6.4: Visual estimates of soybean rust severity for each leaf by each of four raters\n\n\n\n\nAnother interesting visualization is the correlation matrix of the estimates between all possible pair of raters. The ggpairs function of the GGally package is handy for this task.\n\nlibrary(GGally)\n\n\n# create a new dataframe with only raters\nraters &lt;- sbr |&gt; \n  select(2:5)\n\nggpairs(raters)+\n  theme_r4pde()\n\n\n\n\nFigure 6.5: Correlation plots relating severity estimates for all pairs of raters\n\n\n\n\n\n6.2.1.1 Coefficient of determination\nWe noticed earlier that the correlation coefficients varied across all pairs of rater. Sometimes, the means of squared Pearson’s R values (R2), or the coefficient of determination is used as a measure of inter-rater reliability. We can further examine the pair-wise correlations in more details using the cor function,\n\nknitr::kable(cor(raters))\n\n\n\nTable 6.1: Pearson correlation coefficients for all pairs of raters\n\n\n\nR1\nR2\nR3\nR4\n\n\n\n\nR1\n1.0000000\n0.6325037\n0.6825936\n0.6756986\n\n\nR2\n0.6325037\n1.0000000\n0.8413333\n0.8922181\n\n\nR3\n0.6825936\n0.8413333\n1.0000000\n0.8615470\n\n\nR4\n0.6756986\n0.8922181\n0.8615470\n1.0000000\n\n\n\n\n\n\nThe means of coefficient of determination can be easily obtained as follows.\n\n# All pairwise R2\n\nraters_cor &lt;- reshape2::melt(cor(raters))\n\nraters2 &lt;- raters_cor |&gt; \n  filter(value != 1) \n\n# means of R2\nraters2$value\n\n [1] 0.6325037 0.6825936 0.6756986 0.6325037 0.8413333 0.8922181 0.6825936\n [8] 0.8413333 0.8615470 0.6756986 0.8922181 0.8615470\n\nround(mean(raters2$value^2), 3)\n\n[1] 0.595\n\n\n\n\n6.2.1.2 Intraclass Correlation Coefficient\nA common statistic to report in reliability studies is the Intraclass Correlation Coefficient (ICC). There are several formulations for the ICC whose choice depend on the particular experimental design. Following the convention of the seminal work by Shrout and Fleiss (1979), there are three main ICCs:\n\nOne-way random effects model, ICC(1,1): in our context, each leaf is rated by different raters who are considered as sampled from a larger pool of raters (random effects)\nTwo-way random effects model, ICC(2,1): both raters and leaves are viewed as random effects\nTwo-way mixed model, ICC(3,1): raters are considered as fixed effects and leaves are considered as random.\n\nAdditionally, the ICC may depend on whether the ratings are an average or not of several ratings. When an average is considered, these are called ICC(1,k), ICC(2,k) and ICC(3,k).\nThe ICC can be computed using the ICC() or the icc() functions of the psych or irr packages, respectively. They both provide the coefficient, F value, and the upper and lower bounds of the 95% confidence interval.\n\nlibrary(psych)\nic &lt;- ICC(raters)\nknitr::kable(ic$results[1:2]) # only selected columns\n\n\n\n\n\ntype\nICC\n\n\n\n\nSingle_raters_absolute\nICC1\n0.6405024\n\n\nSingle_random_raters\nICC2\n0.6464122\n\n\nSingle_fixed_raters\nICC3\n0.6919099\n\n\nAverage_raters_absolute\nICC1k\n0.8769479\n\n\nAverage_random_raters\nICC2k\n0.8797008\n\n\nAverage_fixed_raters\nICC3k\n0.8998319\n\n\n\n\n# call ic list for full results\n\nThe output of interest is a dataframe with the results of all distinct ICCs. We note that the ICC1 and ICC2 gave very close results. Now, let’s obtain the various ICCs using the irr package. Differently from the the ICC() function, this one requires further specification of the model to use.\n\nlibrary(irr)\nicc(raters, \"oneway\")\n\n Single Score Intraclass Correlation\n\n   Model: oneway \n   Type : consistency \n\n   Subjects = 20 \n     Raters = 4 \n     ICC(1) = 0.641\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n   F(19,60) = 8.13 , p = 1.8e-10 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.44 &lt; ICC &lt; 0.813\n\n# The one used in the SBR paper\nicc(raters, \"twoway\")\n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : consistency \n\n   Subjects = 20 \n     Raters = 4 \n   ICC(C,1) = 0.692\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n   F(19,57) = 9.98 , p = 6.08e-12 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.503 &lt; ICC &lt; 0.845\n\n\n\n\n6.2.1.3 Overall Concordance Correlation Coefficient\nAnother useful index is the Overall Concordance Correlation Coefficient (OCCC) for evaluating agreement among multiple observers. It was proposed by Barnhart et al. (2002) based on the original index proposed by Lin (1989), earlier defined in the context of two fixed observers. In the paper, the authors introduced the OCCC in terms of the interobserver variability for assessing agreement among multiple fixed observers. As outcome, and similar to the original CCC, the approach addresses the precision and accuracy indices as components of the OCCC. The epi.occc function of the epiR packge does the job but it does compute a confidence interval.\n\nlibrary(epiR)\nepi.occc(raters, na.rm = FALSE, pairs = TRUE)\n\n\nOverall CCC           0.6372\nOverall precision     0.7843\nOverall accuracy      0.8125\n\n\n\n\n\n6.2.2 Intrarater reliability\nAs defined, the intrarater reliability is also known as repeatability, because it measures consistency by the same rater at repeated assessments (e.g. different times) on the same sample. In some studies, we may be interested in testing whether a new method increases repeatability of assessments by a single rater compared with another one. The same indices used for assessing reproducibility (interrater) can be used to assess repeatability, and these are reported at the rater level.\n\n\n6.2.3 Precision\nWhen assessing precision, one measures the variability of the estimates (or measurements) of disease on the same sampling units obtained by different raters (or instruments). A very high precision does not mean that the estimates are closer to the actual value (which is given by measures of bias). However, precision is a component of overall accuracy, or agreement. It is given by the Pearson’s correlation coefficient.\nDifferent from reliability, that requires only the estimates or measures by the raters, now we need a reference (gold standard) value to compare the estimates to. These can be an accurate rater or measures by an instrument. Let’s get back to the soybean rust severity estimation dataset and add a column for the (assumed) actual values of severity on each leaf. In that work, the actual severity values were obtained using image analysis.\n\nsbr &lt;- tibble::tribble(\n~leaf, ~actual, ~R1, ~R2,  ~R3, ~R4,\n1,    0.25, 0.6, 0.6,  0.7, 0.6,\n2,     2.5,   2, 0.7,    5,   1,\n3,    7.24,   5,   5,    8,   5,\n4,    7.31,   2,   4,    6,   2,\n5,    9.07,   6,  14,   10,   7,\n6,    11.6,   5,   6,   10,   5,\n7,   12.46,  10,  18, 12.5,  12,\n8,    13.1,  15,  30,   22,  10,\n9,   14.61,   7,   2,   12,   8,\n10,  16.06,   6,   9, 11.5,   8,\n11,   16.7,   7,   7,   20,   9,\n12,   19.5,   6,  23,   22,  14,\n13,  20.75,  10,  35, 18.5,  20,\n14,  23.56,  19,  10,    9,  10,\n15,  23.77,  15,  20,   19,  20,\n16,  24.45,  17,  30,   18,  13,\n17,  25.78,  19,  53,   33,  38,\n18,  26.03,  17, 6.8,   15,   9,\n19,  26.42,  15,  20,   18,  16,\n20,  28.89,  18,  22,   24,  15\n         )\n\nWe can explore visually via scatter plots the relationships between the actual value and the estimates by each rater (Figure 6.6). To facilitate, we need the data in the long format.\n\nsbr2 &lt;- sbr |&gt; \n  pivot_longer(3:6, names_to = \"rater\",\n               values_to = \"estimate\") \n\nsbr2 |&gt; \n  ggplot(aes(actual, estimate))+\n  geom_point(size = 2, alpha = 0.7)+\n  facet_wrap(~rater)+\n  ylim(0,45)+\n  xlim(0,45)+\n  geom_abline(intercept = 0, slope =1)+\n  theme_r4pde()+\n  labs(x = \"Actual severity (%)\",\n       y = \"Estimate severity (%)\")\n\n\n\n\nFigure 6.6: Scatterplots for the relationship between estimated and actual severity for each rater\n\n\n\n\nThe Pearson’s r for the relationship, or the precision of the estimates by each rater, can be obtained using the correlation function of the correlation package.\n\nprecision &lt;- sbr2 |&gt; \n  select(-leaf) |&gt; \n  group_by(rater) |&gt; \n  correlation::correlation() \n\nknitr::kable(precision[1:4])\n\n\n\n\nGroup\nParameter1\nParameter2\nr\n\n\n\n\nR1\nactual\nestimate\n0.8725643\n\n\nR2\nactual\nestimate\n0.5845291\n\n\nR3\nactual\nestimate\n0.7531983\n\n\nR4\nactual\nestimate\n0.7108260\n\n\n\n\n\nThe mean precision can then be obtained.\n\nmean(precision$r)\n\n[1] 0.7302795\n\n\n\n\n6.2.4 Accuracy\n\n6.2.4.1 Absolute errors\nIt is useful to visualize the errors of the estimates which are obtained by subtracting the estimates from the actual severity values. This plot allows to visualize patterns in over or underestimations across a range of actual severity values.\n\nsbr2 |&gt; \n  ggplot(aes(actual, estimate-actual))+\n  geom_point(size = 3, alpha = 0.7)+\n  facet_wrap(~rater)+\n  geom_hline(yintercept = 0)+\n  theme_r4pde()+\n  labs(x = \"Actual severity (%)\",\n       y = \"Error (Estimate - Actual)\")\n\n\n\n\nFigure 6.7: Error (estimated - actual) of visual severity estimates\n\n\n\n\n\n\n6.2.4.2 Concordance correlation coefficient\nLin’s (1989, 2000) proposed the concordance correlation coefficient (CCC) for agreement on a continuous measure obtained by two methods. The CCC combines measures of both precision and accuracy to determine how far the observed data deviate from the line of perfect concordance. Lin’s CCC increases in value as a function of the nearness of the data reduced major axis to the line of perfect concordance (the accuracy of the data) and of the tightness of the data about its reduced major axis (the precision of the data).\nThe epi.ccc function of the epiR package allows to obtain the Lin’s CCC statistics. Let’s filter only rater 2 and calculate the CCC statistics for this rater.\n\nlibrary(epiR)\n# Only rater 2\nsbr3 &lt;- sbr2 |&gt; filter(rater == \"R2\")\nccc &lt;- epi.ccc(sbr3$actual, sbr3$estimate)\n# Concordance coefficient\nrho &lt;- ccc$rho.c[,1]\n# Bias coefficient\nCb &lt;- ccc$C.b\n# Precision\nr &lt;- ccc$C.b*ccc$rho.c[,1]\n# Scale-shift\nss &lt;- ccc$s.shift\n# Location-shift\nls &lt;- ccc$l.shift\nMetrics &lt;- c(\"Agreement\", \"Bias coefficient\", \"Precision\", \"scale-shift\", \"location-shift\")\nValue &lt;- c(rho, Cb, r, ss, ls)\nres &lt;- data.frame(Metrics, Value)\nknitr::kable(res)\n\n\n\nTable 6.2: Statitics of the concordance correlation coefficient summarizing accuracy and precision of visual severity estimates of soybean rust for a single rater\n\n\nMetrics\nValue\n\n\n\n\nAgreement\n0.5230656\n\n\nBias coefficient\n0.8948494\n\n\nPrecision\n0.4680649\n\n\nscale-shift\n1.6091178\n\n\nlocation-shift\n-0.0666069\n\n\n\n\n\n\nNow let’s create a function that will allow us to estimate the CCC for all raters in the data frame in the wide format. The function assumes that the first two columns are the actual and estimates and the rest of the columns are the raters, which is the case for our sbr dataframe . Let’s name this function ccc_byrater.\n\nccc_byrater &lt;- function(data) {\n  long_data &lt;- pivot_longer(data, cols = -c(leaf, actual),\n                            names_to = \"rater\", values_to = \"measurement\")\n  ccc_results &lt;- long_data %&gt;%\n    group_by(rater) %&gt;%\n    summarise(Agreement = as.numeric(epi.ccc(measurement, actual)$rho.c[1]),\n              `Bias coefficient` = epi.ccc(measurement, actual)$C.b,\n              Precision = Agreement * `Bias coefficient`,\n              scale_shift = epi.ccc(measurement, actual)$s.shift,\n              location_shift = epi.ccc(measurement, actual)$l.shift)\n  \n  return(ccc_results)\n}\n\nThen, we use the ccc_byrater function with the original sbr dataset - or any other dataset in the wide format of similar structure. The output is a dataframe with all CCC statistics.\n\nresults &lt;- ccc_byrater(sbr)\nknitr::kable(results)\n\n\n\n\n\n\n\n\n\n\n\n\nrater\nAgreement\nBias coefficient\nPrecision\nscale_shift\nlocation_shift\n\n\n\n\nR1\n0.5968136\n0.6839766\n0.4082065\n1.3652694\n0.9090386\n\n\nR2\n0.5230656\n0.8948494\n0.4680649\n0.6214585\n0.0666069\n\n\nR3\n0.7306948\n0.9701226\n0.7088635\n1.1028813\n0.2280303\n\n\nR4\n0.5861371\n0.8245860\n0.4833205\n1.0044929\n0.6522573"
  },
  {
    "objectID": "data-accuracy.html#accuracy-1",
    "href": "data-accuracy.html#accuracy-1",
    "title": "5  Reliability and accuracy",
    "section": "7.1 Accuracy",
    "text": "7.1 Accuracy\nIncidence data are binary at the individual level; an individual is diseased or not. Here, different from severity that is estimated, the specimen is classified. Let’s create two series of binary data, each being a hypothetical scenario of assignment of 12 plant specimens into two classes: healthy (0) or diseased (1).\n\norder &lt;- c(1:12)\nactual &lt;- c(1,1,1,1,1,1,1,1,0,0,0,0)\nclass &lt;- c(0,0,1,1,1,1,1,1,1,0,0,0)\n\ndat_inc &lt;- data.frame(order, actual, class)\ndat_inc \n\n   order actual class\n1      1      1     0\n2      2      1     0\n3      3      1     1\n4      4      1     1\n5      5      1     1\n6      6      1     1\n7      7      1     1\n8      8      1     1\n9      9      0     1\n10    10      0     0\n11    11      0     0\n12    12      0     0\n\n\nIn the example above, the rater makes 9 accurate classification and misses 3: 2 diseased plants classified as being disease-free (sample 1 and 2), and 1 healthy plant that is wrongly classified as diseased (sample 9).\nNotice that there are four outcomes:\nTP = true positive, a positive sample correctly classified\nTN = true negative, a negative sample correctly classified\nFP = false positive, a negative sample classified as positive\nFN = false negative, a positive sample classified as positive.\nThere are several metrics that can be calculated with the help of a confusion matrix, also known as error matrix. Considering the above outcomes, here is a how a confusion matrix looks like.\nSuppose a 2x2 table with notation\n\n\n\n\nActual value\n\n\n\n\n\nClassification value\nDiseased\nHealthy\n\n\nDiseased\nTP\nFP\n\n\nHealthy\nFN\nTN\n\n\n\nLet’s create this matrix using a function of the caret package.\n\nlibrary(caret)\nattach(dat_inc)\ncm &lt;- confusionMatrix(factor(class), reference = factor(actual))\ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 0 1\n         0 3 2\n         1 1 6\n                                          \n               Accuracy : 0.75            \n                 95% CI : (0.4281, 0.9451)\n    No Information Rate : 0.6667          \n    P-Value [Acc &gt; NIR] : 0.3931          \n                                          \n                  Kappa : 0.4706          \n                                          \n Mcnemar's Test P-Value : 1.0000          \n                                          \n            Sensitivity : 0.7500          \n            Specificity : 0.7500          \n         Pos Pred Value : 0.6000          \n         Neg Pred Value : 0.8571          \n             Prevalence : 0.3333          \n         Detection Rate : 0.2500          \n   Detection Prevalence : 0.4167          \n      Balanced Accuracy : 0.7500          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nThe function returns the confusion matrix and several statistics such as accuracy = (TP + TN) / (TP + TN + FP + FN). Let’s manually calculate the accuracy and compare the results:\n\nTP = 3\nFP = 2\nFN = 1\nTN = 6\naccuracy = (TP+TN)/(TP+TN+FP+FN)\naccuracy\n\n[1] 0.75\n\n\nTwo other important metrics are sensitivity and specificity.\n\nsensitivity = TP/(TP+FN)\nsensitivity\n\n[1] 0.75\n\nspecificity = TN/(FP+TN)\nspecificity\n\n[1] 0.75\n\n\nWe can calculate some metrics using the MixtureMissing package.\n\nlibrary(MixtureMissing)\nevaluation_metrics(actual, class)\n\n$matr\n       pred_0 pred_1\ntrue_0      3      1\ntrue_1      2      6\n\n$TN\n[1] 3\n\n$FP\n[1] 1\n\n$FN\n[1] 2\n\n$TP\n[1] 6\n\n$TPR\n[1] 0.75\n\n$FPR\n[1] 0.25\n\n$TNR\n[1] 0.75\n\n$FNR\n[1] 0.25\n\n$precision\n[1] 0.8571429\n\n$accuracy\n[1] 0.75\n\n$error_rate\n[1] 0.25\n\n$FDR\n[1] 0.1428571"
  },
  {
    "objectID": "data-accuracy.html#reliability",
    "href": "data-accuracy.html#reliability",
    "title": "5  Reliability and accuracy",
    "section": "7.2 Reliability",
    "text": "7.2 Reliability\n\nlibrary(psych)\ntab &lt;- table(class, actual)\nphi(tab)\n\n[1] 0.48\n\n\n\n\n\n\nBarnhart, H. X., Haber, M., and Song, J. 2002. Overall Concordance Correlation Coefficient for Evaluating Agreement Among Multiple Observers. Biometrics. 58:1020–1027 Available at: http://dx.doi.org/10.1111/j.0006-341x.2002.01020.x.\n\n\nLin, L. I.-K. 1989. A concordance correlation coefficient to evaluate reproducibility. Biometrics. 45:255 Available at: http://dx.doi.org/10.2307/2532051.\n\n\nShrout, P. E., and Fleiss, J. L. 1979. Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin. 86:420–428 Available at: http://dx.doi.org/10.1037/0033-2909.86.2.420."
  },
  {
    "objectID": "data-sads.html#definitions",
    "href": "data-sads.html#definitions",
    "title": "6  Standard area diagrams",
    "section": "6.1 Definitions",
    "text": "6.1 Definitions\nAccording to a glossary on phytopathometry (Bock et al. 2021), standard area diagram (SAD) can be defined as “a generic term for a pictorial or graphic representation (drawing or true-color photo) of selected disease severities on plants or plant parts (leaves, fruit, flowers, etc.) generally used as an aid for more accurate visual estimation (on the percentage scale) or classification (using an ordinal scale) of severity on a specimen”.\nThe Standard Area Diagrams (SADs), also known as diagrammatic scales, have a long history of use in plant pathology. The concept dates back to the late 1800s when the Cobb scale was developed, featuring five diagrams depicting a range of severity levels of rust pustules on wheat leaves.\nIn the past 20 years, plant pathologists have leveraged advancements in image processing and analysis tools, along with insights from psychophysical and measurement sciences, to develop SADs that are realistic (e.g., true-color photographs), validated, and depict severities that maximize estimation accuracy. SADs have been created in various color formats (black or white, two-color, or true-color) and with varying incremental scales (approximated linear or logarithmic) (Del Ponte et al. 2017).\nSADs have proven beneficial in increasing the accuracy of visual estimates, as estimating percentage areas is generally more challenging than classifying severity into ordinal classes - there are numerous possibilities on the percentage scale, compared to the finite and small number of classes in ordinal scales. A recent quantitative review confirmed that using SADs often results in improved accuracy and precision of visual estimates. However, it also identified factors related to SAD design and structure, disease symptoms, and actual severity that affected the outcomes. In particular, SADs have shown greater utility for raters who are inherently less accurate and for diseases characterized by small and numerous lesions (Del Ponte et al. 2022). Here are examples of SADs in black and white, two-color, and true-color formats:\n\n\n\n\nFigure 6.1: Actual photos of symptoms of loquat scab on fruit (left) and a SADs with eight diagrams (right). Each number represents severity as the percent area affected (González-Domínguez et al. 2014)\n\n\n\n\n\n\nFigure 6.2: SADs for Glomerella leaf spot on apple leaf. Each number represents severity as the percent area affected (Moreira et al. 2018)\n\n\n\n\n\n\nFigure 6.3: SADs for soybean rust. Each number represents severity as the percent area affected (Franceschi et al. 2020)\n\n\nMore SADs can be found in the SADBank, a curated collection of articles on SAD development and validation. Click on the image below to get access to the database.\n\n\n\nFigure 6.4: SADBank, a curated collection of articles"
  },
  {
    "objectID": "data-sads.html#sad-development-and-validation",
    "href": "data-sads.html#sad-development-and-validation",
    "title": "6  Standard area diagrams",
    "section": "6.2 SAD development and validation",
    "text": "6.2 SAD development and validation\nA systematic review of the literature on SADs highlighted the most important aspects related with the development and validation of the tool (Del Ponte et al. 2017). A list of best practices was proposed in the review to guide future research in the area. Follows the most important aspects to be noted:\n\n\n\n\n\n\nBest practices on SADs development\n\n\n\n\nSample a minimum number (e.g., n = 100) of specimens from natural epidemics representing the range of disease severity and typical symptoms observed.\nUse reliable image analysis software to discriminate disease symptoms from healthy areas to calculate percent area affected.\nWhen designing the illustrations for the SAD set, ensure that the individual diagrams are prepared realistically, whether line drawn, actual photos, or computer generated.\nThe number of diagrams should be no less than 6 and no more than 10, distributed approximately linearly, and spaced no more than 15% apart. Additional diagrams (±2) should be included between 0 and 10% severity.\nFor the validation trial, select at least 50 specimens representing the full range of actual severity and symptom patterns.\nWhen selecting raters (a minimum of 15) for validation, make sure they do not have previous experience in using the SAD under evaluation.\nProvide standard instructions on how to recognize the symptoms of the disease and how to assess severity, first without and then with the SAD.\nIdeally repeat the assessment in time, with a 1- or 2-week interval, both without and with the aid, using the same set of raters in order to evaluate the effect of training and experience on gains in accuracy.\nBoth pre- and posttest experiment conditions should be the same to avoid any impact of distraction on accuracy of estimates during the tests."
  },
  {
    "objectID": "data-sads.html#designing-sads-in-r",
    "href": "data-sads.html#designing-sads-in-r",
    "title": "6  Standard area diagrams",
    "section": "6.3 Designing SADs in R",
    "text": "6.3 Designing SADs in R\nThe diagrams used in a set have been developed using various methods and technologies, ranging from hand-drawn diagrams to actual photographs (Del Ponte et al. 2017). There is an increasing trend towards using actual photos that are digitally analyzed using standard image analysis software to determine the percent area affected. With this approach, a large set of images is analyzed, and some images are chosen to represent the severities in the SAD according to the scale structure.\nIn R, the pliman package has a function called sad() which allows the automatic generation of a SADs with a pre-defined number of diagrams. Firstly, as shown in the previous chapter, the set of images to be selected needs to be analysed using the measure_disease() function. Then, a SADs is automatically generated. In the function, the specimens with the smallest and highest severity will be selected for the SAD. The intermediate diagrams are sampled sequentially to achieve the pre-defined number of images after the severity has been ordered from low to high. More details of the function here.\nLet’s use the same set of 10 soybean leaves, as seen in the previous chapter, depicting the rust symptoms and create the sbr object.\n\nlibrary(pliman)\nh &lt;- image_import(\"imgs/sbr_h.png\")\ns &lt;- image_import(\"imgs/sbr_s.png\")\nb &lt;- image_import(\"imgs/sbr_b.png\")\n\nsbr &lt;- measure_disease(\n  pattern = \"img\",\n  dir_original = \"imgs/originals\" ,\n  dir_processed = \"imgs/processed\",\n  save_image = TRUE,\n  img_healthy = h,\n  img_symptoms = s,\n  img_background = b,\n  show_original = FALSE, # set to TRUE for showing the original.\n  col_background = \"white\", \n  verbose = FALSE,\n  plot = FALSE\n)\n\nWe are ready to run the sad() function to create a SADs with five diagrams side by side. The resulting SADs is in two-color as standard. Set the argument show_original to TRUE for showing the orignal image in the SADs.\n\nsad(sbr, 5, ncol = 5)"
  },
  {
    "objectID": "data-sads.html#analysis-of-sads-validation-data",
    "href": "data-sads.html#analysis-of-sads-validation-data",
    "title": "6  Standard area diagrams",
    "section": "6.4 Analysis of SADs validation data",
    "text": "6.4 Analysis of SADs validation data\nTo evaluate the effect of SAD on accuracy components, analyze the data, preferably using concordance analysis methods (see chapter), to fully explore which component is affected and to gain insight into the ramification of errors. Linear regression should not be used as the sole method but it could be complementary for comparison with previous literature.\nInferential methods should be used for testing hypotheses related to gain in accuracy. If parametric tests are used (paired t-test for example), make sure to check that the assumptions are not violated. Alternatively, nonparametric tests (Wilcoxon signed rank) or nonparametric bootstrapping should be used when the conditions for parametric tests are not met. More recently, a (parametric) mixed modelling framework has been used to analyse SADs validation data where raters are taken as a random effects in the model (González-Domínguez et al. 2014; Franceschi et al. 2020; Pereira et al. 2020).\n\n6.4.1 Non parametric boostrapping of differences\nBootstrap is a resampling method where large numbers of samples of the same size are repeatedly drawn, with replacement, from a single original sample. It is commonly used when the distribution of a statistic is unknown or complicated and the sample size is too small to draw a valid inference.\nA bootstrap-based equivalence test procedure was first proposed as complementary to parametric (paired t-test) or non-parametric (Wilcoxon) to analyze severity estimation data in a study on the development and validation of a SADs for pecan scab (Yadav et al. 2012). The equivalence test was used to calculate 95% confidence intervals for each statistic by bootstrapping using the percentile method (with an equivalence test, the null hypothesis is the converse of H0, i.e. the null hypothesis is non-equivalence). In that study, the test was used to compare means of the CCC statistics across raters under two conditions: 1) without versus with the SAD; and 2) experienced versus inexperienced raters.\nTo apply the bootstrap-based equivalence test, let’s work with the CCC data for a sample of 20 raters who estimated severity of soybean rust SAD first without and then with the aid. The CCC was calculated as shown here.\n\nlibrary(tidyverse)\nlibrary(r4pde)\n\nsbr &lt;- tibble::tribble(\n  ~rater, ~aided, ~unaided,\n      1,   0.97,     0.85,\n      2,   0.97,     0.85,\n      3,   0.95,     0.82,\n      4,   0.93,     0.69,\n      5,   0.97,     0.84,\n      6,   0.96,     0.86,\n      7,   0.98,     0.78,\n      8,   0.93,     0.72,\n      9,   0.94,     0.67,\n     10,   0.95,     0.53,\n     11,   0.94,     0.78,\n     12,   0.98,     0.89,\n     13,   0.96,      0.8,\n     14,   0.98,     0.87,\n     15,   0.98,      0.9,\n     16,   0.98,     0.87,\n     17,   0.98,     0.84,\n     18,   0.97,     0.86,\n     19,   0.98,     0.89,\n     20,   0.98,     0.78\n  )\n\nLet’s visualize the data using boxplots. Each point in the plot represents a rater.\n\ntheme_set(theme_r4pde())\nsbr |&gt; \n  pivot_longer(2:3, names_to = \"condition\", values_to =\"estimate\") |&gt; \n  ggplot(aes(condition, estimate))+\n  geom_boxplot(outlier.colour = NA)+\n  geom_jitter(width = 0.05, size = 2, alpha = 0.5)+\n  theme_r4pde()+\n  ylim(0.4,1)\n\n\n\n\nTo proceed with bootstrapping, we first create a new variable to hold the differences between the means of the estimates (aided minus unaided). If the 95% CI does not include zero, this means that there was a significant improvement in the statistics.\n\n# diff of means\nsbr$diff &lt;- sbr$aided - sbr$unaided\n\nsbr |&gt; \n  ggplot(aes(x= diff))+\n  theme_r4pde()+\n  geom_histogram(bins = 10, color = \"white\")\n\n\n\n\nUsing the simpleboot and boot packages of R:\n\nlibrary(simpleboot)\nb.mean &lt;- one.boot(sbr$diff, mean, 999)\nboot::boot.ci(b.mean)\n\nWarning in boot::boot.ci(b.mean): bootstrap variances needed for studentized\nintervals\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = b.mean)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.1234,  0.1952 )   ( 0.1155,  0.1915 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.1275,  0.2035 )   ( 0.1310,  0.2065 )  \nCalculations and Intervals on Original Scale\n\nmean(b.mean$data)\n\n[1] 0.1595\n\nhist(b.mean)\n\n\n\n\nUsing the bootstrap package:\n\nlibrary(bootstrap)\nb &lt;- bootstrap(sbr$diff, 999, mean)\nquantile(b$thetastar, c(.025,.975))\n\n    2.5%    97.5% \n0.129000 0.195025 \n\nmean(b$thetastar)\n\n[1] 0.159953\n\nsd(b$thetastar)\n\n[1] 0.017419\n\nse &lt;- function(x) sqrt(var(x)/length(x))\nse(b$thetastar)\n\n[1] 0.0005511128\n\n\nBoth procedures shown above have led to similar results. The 95% CIs of the differences did not include zero, so a significant improvement in accuracy can be inferred.\n\n\n6.4.2 Parametric and non-parametric paired sample tests\nWhen two estimates are gathered from the same rater at different times, these data points are not independent. In such situations, a paired sample t-test can be utilized to test if the mean difference between two sets of observations is zero. This test requires each subject (or leaf, in our context) to be measured or estimated twice, resulting in pairs of observations. However, if the assumptions of the test (such as normality) are violated, a non-parametric equivalent, such as the Wilcoxon signed-rank test, also known as the Wilcoxon test, can be employed. This alternative is particularly useful when the data are not normally distributed.\nTo proceed with these tests, we first need to ascertain whether our data are normally distributed. We should also verify whether the variances are equal. Let’s now apply these two tests to our data and compare the results.\n\n# normality test\nshapiro.test(sbr$aided)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sbr$aided\nW = 0.82529, p-value = 0.002111\n\nshapiro.test(sbr$unaided)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sbr$unaided\nW = 0.83769, p-value = 0.003338\n\n# equal variance test\nvar.test(sbr$aided, sbr$unaided)\n\n\n    F test to compare two variances\n\ndata:  sbr$aided and sbr$unaided\nF = 0.037789, num df = 19, denom df = 19, p-value = 1.53e-09\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.01495720 0.09547109\nsample estimates:\nratio of variances \n        0.03778862 \n\n# paired t-test\nt.test(sbr$aided, sbr$unaided, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  sbr$aided and sbr$unaided\nt = 8.812, df = 19, p-value = 3.873e-08\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.1216158 0.1973842\nsample estimates:\nmean difference \n         0.1595 \n\n# Wilcoxon test\nwilcox.test(sbr$aided, sbr$unaided, paired = TRUE)\n\nWarning in wilcox.test.default(sbr$aided, sbr$unaided, paired = TRUE): não é\npossível computar o valor de p exato com o de desempate\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  sbr$aided and sbr$unaided\nV = 210, p-value = 9.449e-05\nalternative hypothesis: true location shift is not equal to 0\n\n\nAs shown above, the two assumptions were violated, so we could rely more confidently on the non-parametric test.\n\n\n6.4.3 Mixed effects modeling\nMixed models, also known as mixed effects models or multilevel models, are an extension of traditional linear models that are used for analyzing hierarchical or clustered data. These models are particularly useful when dealing with data where observations may not be fully independent, or when the assumption of independence is violated. This happens, for instance, when data are collected over time from the same individuals or units, or when individuals are grouped or nested within higher-level units, such as in our case where measurements are taken by different raters (Brown 2021).\nMixed models enable us to model both fixed and random effects. Fixed effects represent the usual regression parameters that we are primarily interested in estimating, while random effects model the random variation that occurs at different levels of hierarchy or clustering. They allow us to account for variability among different levels of data, like inter-rater variability or intra-subject variability in repeated measures designs.\nIn our context, we consider raters as random effects because we view them as a sample drawn from a larger population of potential raters, and our goal is to generalize our findings to this larger population. If we were to sample additional raters, we would expect these new raters to differ from our current ones. However, by considering raters as a random effect in our model, we can account for this inter-rater variability and make more accurate inferences about the overall population.\nThe random effects component in the mixed model allows us to capture and model the additional variance that is not explicitly accounted for by the fixed effects in our model. In other words, random effects help us to capture and quantify the ‘unexplained’ or ‘residual’ variation that exists within and between the clusters or groups in our data. This could include, for instance, variation in disease measurements that are taken repeatedly from the same subjects. In conclusion, mixed models provide a robust and flexible framework for modeling hierarchical or clustered data, allowing us to effectively account for both fixed and random effects and to make more accurate inferences about our data.\nLet’s start reshaping our data to the long format and assign them to a new data frame.\n\nsbr2 &lt;- sbr |&gt; \n  pivot_longer(2:3, names_to = \"condition\", values_to = \"estimate\")\n\nNow we fit the mixed model using the lmer function of the lme4 package. We will fit the model to the logit of the estimate because they should be bounded between zero and one. Preliminary analysis using non-transformed or log-transformed data resulted in lack of normality of residuals and heterocedasticity (not shown).\n\nlibrary(lme4) \nlibrary(car) # for logit function\nmix &lt;- lmer(logit(estimate) ~ condition + (1 | rater), data = sbr2)\n\n# Check model performance\nlibrary(performance)\ncheck_normality(mix)\n\nOK: residuals appear as normally distributed (p = 0.381).\n\ncheck_heteroscedasticity(mix)\n\nOK: Error variance appears to be homoscedastic (p = 0.961).\n\n# Check effect of condition\ncar::Anova(mix)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: logit(estimate)\n           Chisq Df Pr(&gt;Chisq)    \ncondition 458.44  1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Estimate the means for each group\nlibrary(emmeans)\nem &lt;- emmeans(mix, ~ condition, type =\"response\")\nem\n\n condition response      SE   df lower.CL upper.CL\n aided        0.968 0.00359 25.5    0.959    0.974\n unaided      0.817 0.01719 25.5    0.779    0.849\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n# Contrast the means\npairs(em)\n\n contrast        odds.ratio    SE df null t.ratio p.value\n aided / unaided       6.72 0.597 19    1  21.411  &lt;.0001\n\nDegrees-of-freedom method: kenward-roger \nTests are performed on the log odds ratio scale \n\n# plot the means with 95% CIs\nplot(em) +\n  coord_flip()+\n  xlim(0.7,1)+\n  theme_r4pde()\n\n\n\n\nAs shown above, we can reject the null hypothesis that the means are the same between the two groups.\nAlternatively, we could fit GLMMs - generalized linear mixed models, which extend the traditional linear mixed models to accommodate response variables that follow different distributions. They are particularly useful when the response variable does not follow a normal distribution and cannot be adequately transformed to meet the parametric assumptions of traditional linear models. The glmmTMB package in R provides a convenient and flexible platform to fit GLMMs using a variety of distributions (Brooks et al. 2017).\nIn our case, considering our response variable bounded between 0 and 1, a Beta distribution might be a suitable choice. Beta distribution is a continuous probability distribution defined on the interval [0, 1], and is commonly used for modelling variables that represent proportions or percentages.\nThe function glmmTMB() from the glmmTMB package can be used to fit a GLMM with a Beta distribution. In this function, we specify the distribution family as beta_family().\n\nlibrary(glmmTMB)\nmix2 &lt;-  glmmTMB(estimate ~ condition + (1| rater), \n                 data = sbr2, \n                 family = beta_family())\n\nBecause the package performance does not handle the glmmTMB output, we will use the DHARMa package in R which can be particularly useful for checking the assumptions of your GLMM fitted with glmmTMB(). The package provides a convenient way to carry out residual diagnostics for models fitted via maximum likelihood estimation, including GLMMs. This package creates standardized residuals from the observed responses and the predicted responses of a fitted model, and then compares these residuals to a simulated set of residuals under a correct model.\n\nlibrary(DHARMa)\n\nplot(simulateResiduals(mix2))\n\n\n\n\nIn this example, simulateResiduals() generates simulated residuals from your fitted model, and the plot creates a plot of these residuals. This showed that the residuals from our model are uniformly distributed, which is an assumption of GLMMs. We can now proceed with the posthoc analysis and noticed that the results are similar to when the response variable was transformed to logit.\n\ncar::Anova(mix2)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: estimate\n           Chisq Df Pr(&gt;Chisq)    \ncondition 400.93  1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlibrary(emmeans)\nem &lt;- emmeans(mix2, ~ condition, type = \"response\")\nem\n\n condition response     SE  df asymp.LCL asymp.UCL\n aided        0.967 0.0043 Inf     0.958     0.975\n unaided      0.814 0.0167 Inf     0.779     0.845\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n# Contrast the means\npairs(em)\n\n contrast        odds.ratio    SE  df null z.ratio p.value\n aided / unaided       6.71 0.638 Inf    1  20.023  &lt;.0001\n\nTests are performed on the log odds ratio scale \n\n\n\n\n\n\nBock, C. H., Pethybridge, S. J., Barbedo, J. G. A., Esker, P. D., Mahlein, A.-K., and Del Ponte, E. M. 2021. A phytopathometry glossary for the twenty-first century: towards consistency and precision in intra- and inter-disciplinary dialogues. Tropical Plant Pathology. 47:14–24 Available at: http://dx.doi.org/10.1007/s40858-021-00454-0.\n\n\nBrooks, M. E., Kristensen, K., van Benthem, K. J., Magnusson, A., Berg, C. W., Nielsen, A., et al. 2017. glmmTMB balances speed and flexibility among packages for zero-inflated generalized linear mixed modeling. The R Journal. 9:378–400.\n\n\nBrown, V. A. 2021. An Introduction to Linear Mixed-Effects Modeling in R. Advances in Methods and Practices in Psychological Science. 4:251524592096035 Available at: http://dx.doi.org/10.1177/2515245920960351.\n\n\nDel Ponte, E. M., Cazón, L. I., Alves, K. S., Pethybridge, S. J., and Bock, C. H. 2022. How much do standard area diagrams improve accuracy of visual estimates of the percentage area diseased? A systematic review and meta-analysis. Tropical Plant Pathology. 47:43–57 Available at: http://dx.doi.org/10.1007/s40858-021-00479-5.\n\n\nDel Ponte, E. M., Pethybridge, S. J., Bock, C. H., Michereff, S. J., Machado, F. J., and Spolti, P. 2017. Standard Area Diagrams for Aiding Severity Estimation: Scientometrics, Pathosystems, and Methodological Trends in the Last 25 Years. Phytopathology®. 107:1161–1174 Available at: http://dx.doi.org/10.1094/PHYTO-02-17-0069-FI.\n\n\nFranceschi, V. T., Alves, K. S., Mazaro, S. M., Godoy, C. V., Duarte, H. S. S., and Del Ponte, E. M. 2020. A new standard area diagram set for assessment of severity of soybean rust improves accuracy of estimates and optimizes resource use. Plant Pathology. 69:495–505 Available at: http://dx.doi.org/10.1111/ppa.13148.\n\n\nGonzález-Domínguez, E., Martins, R. B., Del Ponte, E. M., Michereff, S. J., García-Jiménez, J., and Armengol, J. 2014. Development and validation of a standard area diagram set to aid assessment of severity of loquat scab on fruit. European Journal of Plant Pathology. Available at: http://dx.doi.org/10.1007/s10658-014-0400-2.\n\n\nMoreira, R. R., Silva Silveira Duarte, H. da, and De Mio, L. L. M. 2018. Improving accuracy, precision and reliability of severity estimates of Glomerella leaf spot on apple leaves using a new standard area diagram set. European Journal of Plant Pathology. 153:975–982 Available at: http://dx.doi.org/10.1007/s10658-018-01610-0.\n\n\nPereira, W. E. L., Andrade, S. M. P. de, Del Ponte, E. M., Esteves, M. B., Canale, M. C., Takita, M. A., et al. 2020. Severity assessment in the Nicotiana tabacum-Xylella fastidiosa subsp. pauca pathosystem: design and interlaboratory validation of a standard area diagram set. Tropical Plant Pathology. 45:710–722 Available at: http://dx.doi.org/10.1007/s40858-020-00401-5.\n\n\nYadav, N. V. S., Vos, S. M. de, Bock, C. H., and Wood, B. W. 2012. Development and validation of standard area diagrams to aid assessment of pecan scab symptoms on fruit. Plant Pathology. 62:325–335 Available at: http://dx.doi.org/10.1111/j.1365-3059.2012.02641.x."
  },
  {
    "objectID": "data-training.html#training-sessions",
    "href": "data-training.html#training-sessions",
    "title": "7  Training sessions",
    "section": "7.1 Training sessions",
    "text": "7.1 Training sessions\nIn the same way that Standard Area Diagrams (SADs) can improve the accuracy of visual estimates of disease severity, exposure to a diverse set of diagrams or actual images with known severity values can significantly enhance a rater’s assessment proficiency. The key to this improvement is the frequent exposure to various levels of severity, which enables the rater to better calibrate their judgments over time.\nAs the rater engages with these reference diagrams or images, they develop a mental model of the severity scale. This mental model is continually refined through repeated exposure to a variety of severity values. This iterative learning process allows the rater to adjust their estimations based on the feedback from known values, thus improving their overall accuracy and precision in disease severity estimation.\nSuch a process, often termed ‘training’, is particularly beneficial in scenarios where visual estimation is the primary tool for assessing disease severity. Training raters using sets of reference images is an effective strategy to enhance inter-rater reliability and consistency over time, especially when coupled with other tools like SADs."
  },
  {
    "objectID": "data-training.html#software",
    "href": "data-training.html#software",
    "title": "7  Training sessions",
    "section": "7.2 Software",
    "text": "7.2 Software\nIndeed, the use of computerized training sessions in assessing disease severity has a rich history, dating back to the mid-1980s when personal computers were first introduced. These early applications were developed using operating systems like DOS or Windows and involved software like AREAGRAM, DISTRAIN, DISEASE.PRO, ESTIMATE, SEVERITY.PRO, and COMBRO. These programs utilized computerized images with specific and measured disease severities to train raters, as outlined in the review by Bock et al. (2021).\nThe main advantage of these computerized training sessions is that they allow raters to familiarize themselves with various disease severity levels, thereby enhancing their performance in severity estimation. Such training has been proven to significantly improve the accuracy and consistency of disease severity evaluations.\nHowever, a potential limitation of this approach is the short-lived nature of the benefits derived from such training. The skills and proficiency gained from these computerized training sessions may degrade over time, necessitating regular retraining for raters to maintain their performance level. This could be due to the fact that estimation skills, like many other skills, require regular practice for maintenance. Without ongoing exposure to severity scales and continued practice, the accuracy and precision of a rater’s estimates may decline.\nTo address this challenge, it would be beneficial to implement a structured training regimen that includes regular retraining sessions. This could help ensure the continued proficiency of raters in estimating disease severity, thus maintaining the accuracy and reliability of assessments over time. Furthermore, it would be advantageous to investigate the optimal frequency and structure of these training sessions to maximize their effectiveness and sustainability in the long term.\n\n\n\nFigure 7.1: Selected screenshots from Severity.Pro, the disease assessment training program by Forrest W. Nutter (Madden et al. 2021).\n\n\n\n7.2.1 Online training tools\nIn Brazil, the “Sistema de treinamento de acuidade visual” was initially developed as a web-based system to train raters in assessing citrus canker. The system has evolved over time and now has a current version that is accessible on both iOS and Android platforms. You can find the current version of the system at this link. This platform provides an interactive training experience to enhance the ability of raters in accurately assessing the severity of citrus canker.\nIn Mexico, a specific application called Validar-PER has been developed to train raters in visually assessing the severity of coffee leaf rust. This application utilizes diagrammatic log-based scales as a standardized approach for severity assessment. You can access the Validar-PER application online here. The application aims to improve the proficiency of raters in evaluating the severity of coffee leaf rust using a systematic and standardized methodology.\n\n\n\nFigure 7.2: Screen of Validar-PER, an online training module for assessing coffee leaf rust severity\n\n\n\n\n7.2.2 Training software made with R\n\n7.2.2.1 TraineR\nTraineR, developed by the author of this book, is created using R and Shiny. Its purpose is to train users in assessing disease severity, specifically expressed as the percentage area of an organ (leaf or fruit) affected by lesions.\nTo use the app, users can adjust parameters for organ shape, organ color, as well as lesion shape, lesion color, lesion number, and lesion size. These adjustments will generate a standard area diagram with an ellipsoidal shape.\nTo initiate the training, users should first set the desired number of attempts for the session and click on the “generate new” button. A diagram will then be displayed, and users should input their estimate of the diseased area as a numeric value in percentage. The estimate will be recorded and shown in a table along with the actual value, enabling a comparison between the actual and estimated values.\nUsers can continue generating new diagrams and providing estimates until they reach the defined number of attempts. Once the final attempt is completed, the app will present the accuracy in the form of Lin’s concordance correlation coefficient to the user. Plots depicting the relationship between estimates and actual values, as well as the error of the estimates, will be displayed. Furthermore, comprehensive accuracy statistics are also made available.\nCurrently, the app has certain limitations, including the inability to overlap lesions and a maximum severity representation of approximately 60%. Nonetheless, it remains a valuable educational and demonstration tool.\n\n\n\nFigure 7.3: Screen of TraineR, an online app for training in the assessment of plant disease severity\n\n\n\n\n7.2.2.2 Trainer2\nTrainer2 the second generation of TraineR, takes advantage of actual photographs showcasing disease symptoms. This updated version allows for testing the ability of raters to assess disease severity, particularly by evaluating the percentage area affected based on real symptoms captured in the photographs.\nBy utilizing actual images, Trainer2 offers a more realistic and practical approach to training raters. Raters can now evaluate disease severity by visually inspecting the symptoms depicted in the photographs, enhancing their ability to accurately assess the extent of damage in terms of the affected area.\nThe incorporation of real symptoms in Trainer2 serves as a valuable tool for evaluating and refining the skills of raters in disease severity assessment. It provides a more authentic training experience and helps raters become proficient in identifying and quantifying the extent of disease based on visual cues observed in real-life scenarios.\n\n\n\nFigure 7.4: Screen of traineR2, an online for training in the assessment of plant disease severity based on real symptoms captured in photographs\n\n\n\n\n\n\nBock, C. H., Chiang, K.-S., and Del Ponte, E. M. 2021. Plant disease severity estimated visually: a century of research, best practices, and opportunities for improving methods and practices to maximize accuracy. Tropical Plant Pathology. 47:25–42 Available at: http://dx.doi.org/10.1007/s40858-021-00439-z.\n\n\nMadden, L. V., Esker, P. D., and Pethybridge, S. J. 2021. Forrest W. Nutter, Jr.: a career in phytopathometry. Tropical Plant Pathology. 47:5–13 Available at: http://dx.doi.org/10.1007/s40858-021-00469-7."
  },
  {
    "objectID": "temporal-dpc.html#how-epidemics-occur",
    "href": "temporal-dpc.html#how-epidemics-occur",
    "title": "8  Disease progress curves",
    "section": "8.1 How epidemics occur",
    "text": "8.1 How epidemics occur\nBefore knowing how epidemics develop in time, it is important to understand how an epidemic occur. An epidemic begins when the primary inoculum (a variable number of propagules able to infect the plant) that is surviving somewhere establishes an intimate contact with individuals of the host population - this process is called infection. These inocula are usually surviving externally to the plant host and need to disperse (move), passively or by means of a vector, to reach the plant. It can also be that a growing host encounter a localized (static) source of inoculum.\nOnce the infection is established, the pathogen colonizes the plant tissues and disease symptoms are noticed. When this happens, the incubation period can be measured in time units. A successful colonization will lead to reproduction of the pathogen inside and/or external to the crop, and so the latent period is completed, and can also be measure in time units. Finally, the infectious period takes place and continues until the pathogen is not capable of producing the secondary inoculum on the infected site.\n\n\n\n\n\nflowchart\n  A[Infection] --&gt; B[Colonization]\n  B --&gt; C[Reproduction]\n  C -. New inoculum .-&gt; D[Dispersal]\n  E[Survival] -- Primary inoculum --&gt; D\n  D  --&gt; A\n  D  -.-&gt; A\n  C --&gt; E \n\n\nFigure 8.1: Five main processes of the disease cycle\n\n\n\n\nEpidemiologists are generally interested in determining the length of the incubation, latent, and infectious periods as influenced by factors related to the host, pathogen, or environment. This is relevant because the longer it takes for the completion of the incubation and latent periods, the lower the potential number of repeated cycles. In summary, a single “infection cycle” represents all events that occur from infection to dispersal, and this occurs only once for many diseases, while for others there may be multiple cycles, which are defined as an “infection chain.”\n\n\nCode\nlibrary(tidyverse)\nlibrary(r4pde)\nperiods &lt;- tibble::tribble(\n  ~period, ~length, ~color, ~order,\n  \"Incubation\", 10, 0, 1,\n  \"Latent\" , 15, 0, 2,\n  \"Infectious\", 25, 15, 3\n)\n\np &lt;- periods |&gt; \n  ggplot(aes(reorder(period, order), length, fill = period))+\n  geom_col()+\n  geom_col(aes(period, color), color = \"white\", fill = \"white\")+\n  coord_flip()+\n  theme_void()+\n  theme(legend.position = \"none\")+\n  annotate(geom = \"text\", x = 0.5, y = 15, label = \"----- Time ---&gt;\")+\n  annotate(geom = \"text\", x = 1, y = 5, label = \"Incubation\", color = \"white\")+\n  annotate(geom = \"text\", x = 2, y = 8, label = \"Latent\", color = \"white\")+\n  annotate(geom = \"text\", x = 3, y = 20, label = \"Infectious\", color = \"white\")+\n  annotate(geom = \"text\", x = 1, y = 10.5, label = \"Visible symptoms\", angle = 90, size = 1.7)+\n  annotate(geom = \"text\", x = 2, y = 15.5, label = \"Reproduction starts\", angle = 90, size =1.7)+\n  annotate(geom = \"text\", x = 3, y = 25.5, label = \"Reproduction ends\", angle = 90, size =1.7)+\n  scale_fill_manual(values = c(\"darkgreen\",  \"brown\", \"darkorange\"))+\n  geom_segment(mapping=aes(x=0.6, y=0, xend=0.6, yend=10), arrow=arrow(ends='both'), size=1, color = \"black\")+ \n  geom_segment(mapping=aes(x=1.6, y=0, xend=1.6, yend=15), arrow=arrow(ends='both'), size=1, color = \"black\")  +\n   geom_segment(mapping=aes(x=2.6, y=15, xend=2.6, yend=25), arrow=arrow(ends='both'), size=1, color = \"black\") \n  library(png)\n  library(cowplot)\n  incubation &lt;- readPNG(\"imgs/incubation3.png\", native = TRUE)\n  latent &lt;- readPNG(\"imgs/latent3.png\", native = TRUE)\n  p2 &lt;- p + draw_image(incubation , x = 0.5, y = 13, scale = 5)+\n    draw_image(latent , x = 1.5, y = 20, scale = 5)\n  ggsave(\"imgs/periods.png\", width =6, height =2, bg = \"white\")  \n\n\n\n\n\nFigure 8.2: Three time-related epidemiological periods and their relations with stages of the disease cycle including colonization (symptoms) and reproduction (sporulation in the case of fungi). Drawings of apple scab symptoms and signs adapted from Agrios (2005)"
  },
  {
    "objectID": "temporal-dpc.html#disease-curves",
    "href": "temporal-dpc.html#disease-curves",
    "title": "8  Disease progress curves",
    "section": "8.2 Disease curves",
    "text": "8.2 Disease curves\nA key understanding of the epidemics relates to the knowledge of rates and patterns. Epidemics can be viewed as dynamic systems that change their state as time goes. The first and simplest way to characterize such changes in time is to produce a graphical plot called disease progress curve (DPC). This curve can be obtained as long as the intensity of the disease (y) in the host population is assessed sequentially in time (t).\nA DPC summarizes the interaction of the three main components of the disease triangle occurring during the epidemic. The curves can vary greatly in shape according to variations in each of the components, in particular due to management practices that alter the course of the epidemics and for which the goal is to stop disease increase. We can create a data frame in R for a single DPC and make a plot using ggplot. By convention we use t for time and y for disease intensity, expressed in percentage (0 to 100%).\nFirstly, let’s load the essential R packages and set up the environment.\n\nlibrary(tidyverse) # essential packages \ntheme_set(theme_r4pde()) # set global theme\n\nThere are several ways to create a data frame in R. I like to use the tribble function as below. The entered data will be assigned to a dataframe called dpc.\n\ndpc &lt;- \n  tribble(\n   ~t,  ~y, \n   0,  0.1, \n   7,  1, \n  14,  9, \n  21,  25, \n  28,  80, \n  35, 98, \n  42, 99, \n  49, 99.9\n  )\n\nNow the plot\n\ndpc1 &lt;- dpc |&gt;\n  ggplot(aes(t, y)) +\n  theme_r4pde()+\n  geom_line(size = 1)+\n  geom_point(size = 3, shape = 16)+\n  labs(x = \"Assessment time (days)\",\n       y = \"Disease intensity (%)\")\n\nggsave(\"imgs/dpc1.png\", dpc1)\n\n\n\n\nFigure 8.3: A typical disease progress curve for an epidemic that reaches the maximum value"
  },
  {
    "objectID": "temporal-dpc.html#epidemic-classification",
    "href": "temporal-dpc.html#epidemic-classification",
    "title": "8  Disease progress curves",
    "section": "8.3 Epidemic classification",
    "text": "8.3 Epidemic classification\nVanderplank analysed the shapes of great number of epidemic curves and classified the epidemics into two basic types: monocyclic or polycyclic (Vanderplank 1963). In monocyclic epidemics, inoculum capable of infecting the crop is not produced during the epidemics. These epidemics are initiated and maintained only by the primary inoculum. There is no secondary infection and hence no further spread of newly produced inoculum among the host individuals. Tipically, the progress curves for monocyclic epidemics have a saturation type shape.\nConversely, when the secondary inoculum produced during the epidemics is capable of infecting the host during the same crop cycle, a polycyclic epidemic is established. The number of repeated cycles just depends on how long it takes to complete a single infection cycle. These epidemics most commonly present a sigmoid shape Figure 8.4.\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_bw(base_size = 16))\n\nlibrary(epifitter)\npolyc &lt;- sim_logistic(N = 50, dt = 5, \n                      y0 = 0.01, r = 0.2, \n                      K = 0.8, n = 1, \n                      alpha =0)\n\np &lt;- polyc |&gt; \n  ggplot(aes(time, y))+\n  geom_point(aes(time, y), size =19, shape =1)+\n  geom_line()+\n  ylim(0,1)+\n  theme_r4pde()+\n  labs(x = \"Time\", y = \"Disease intensity\")\n\n\nmonoc &lt;- sim_monomolecular(N = 50, dt = 5, \n                           y0 = 0.01, r = 0.1,\n                           K = 0.8, n = 1, \n                           alpha =0)\nlibrary(ggforce)\nm &lt;- monoc |&gt; \n  ggplot(aes(time, y))+\n  geom_point(aes(x = 25, y = 0.5), size =90, shape = 1)+\n   geom_line()+\n theme_r4pde()+\n  ylim(0,1)+\n  labs(x = \"Time\", y = \"Disease intensity\")\n\nlibrary(patchwork)\ncycles &lt;- m | p\nggsave(\"imgs/cycles.png\", bg = \"white\", width = 8, height =4)\n\n\n\n\n\nFigure 8.4: Hypothetical curves for monocyclic (left) and polycyclic (right) epidemics. Each circle represents a single infection cycle."
  },
  {
    "objectID": "temporal-dpc.html#curve-descriptors-and-audpc",
    "href": "temporal-dpc.html#curve-descriptors-and-audpc",
    "title": "8  Disease progress curves",
    "section": "8.4 Curve descriptors and AUDPC",
    "text": "8.4 Curve descriptors and AUDPC\nThe depiction and analysis of disease progress curves can provide useful information for gaining understanding of the underlying epidemic process. The curves are extensively used to evaluate how disease control measures affect epidemics. When characterizing DPCs, a researcher may be interested in describing and comparing epidemics that result from different treatments, or simply in their variations as affected by changes in environment, host or pathogen.\nThe precision and complexity of the analysis of progress curve data depends on the objective of the study. In general, the goal is to synthesize similarities and differences among epidemics based on common descriptors of the disease progress curves. For example, the simple appraisal of the disease intensity at any time during the course of the epidemic should be sufficient for certain situations. Furthermore, a few quantitative and qualitative descriptors can be extracted including:\n\nEpidemic duration\nMaximum disease\nCurve shape\nArea under the area under the disease progress curve (AUDPC).\n\nLet’s visualize the AUDPC in the same plot that we produced above.\n\ndpc2 &lt;- dpc |&gt;\n  ggplot(aes(t, y)) +\n  labs(x = \"Assessment time (days)\",\n       y = \"Disease intensity (%)\")+\n    geom_area(fill = \"#339966\")+\n    geom_line(linewidth = 1)+\n  theme_r4pde()+\n  geom_point(size = 3, shape = 16)+\n  scale_x_continuous(breaks = c(0, 7, 14, 21, 28, 35, 42))\nggsave(\"imgs/dpc2.png\")\n\n\n\n\nFigure 8.5: Representation of the area under the disease progress curve\n\n\nThe AUDPC summarizes the “total measure of disease stress” and is largely used to compare epidemics (Jeger and Viljanen-Rollinson 2001). The most common approach to calculate AUDPC is the trapezoidal method, which splits the disease progress curves into a series of rectangles, calculating the area of each of them and then summing the areas. Let’s extend the plot code to show those rectangles using the annotate function.\n\n\nCode\ndpc3 &lt;- dpc |&gt;\n  ggplot(aes(t, y)) +\n  theme_r4pde()+\n  labs(x = \"Assessment time (days)\",\n       y = \"Disease intensity (%)\")+\n  annotate(\"rect\", xmin = dpc$t[1], xmax = dpc$t[2], \n           ymin = 0, ymax = (dpc$y[1]+ dpc$y[2])/2, \n           color = \"white\", fill = \"#339966\")+\n   annotate(\"rect\", xmin = dpc$t[2], xmax = dpc$t[3], \n            ymin = 0, ymax = (dpc$y[2]+ dpc$y[3])/2, \n            color = \"white\", fill = \"#339966\")+\n   annotate(\"rect\", xmin = dpc$t[3], xmax = dpc$t[4], \n            ymin = 0, ymax = (dpc$y[3]+ dpc$y[4])/2,\n            color = \"white\", fill = \"#339966\")+\n   annotate(\"rect\", xmin = dpc$t[4], xmax = dpc$t[5], \n            ymin = 0, ymax = (dpc$y[4]+ dpc$y[5])/2, \n            color = \"white\", fill = \"#339966\")+\n   annotate(\"rect\", xmin = dpc$t[5], xmax = dpc$t[6], \n            ymin = 0, ymax = (dpc$y[5]+ dpc$y[6])/2, \n            color = \"white\",fill = \"#339966\")+\n   annotate(\"rect\", xmin = dpc$t[6], xmax = dpc$t[7], \n            ymin = 0, ymax = (dpc$y[6]+ dpc$y[7])/2, \n            color = \"white\", fill = \"#339966\")+\n  annotate(\"rect\", xmin = dpc$t[7], xmax = dpc$t[8], \n            ymin = 0, ymax = (dpc$y[7]+ dpc$y[8])/2, \n            color = \"white\", fill = \"#339966\")+\n  geom_line(linewidth = 1)+\n  geom_point(size = 3, shape = 16)+\n  annotate(geom = \"text\", x = 36.5, y = 50,\n           label = \"AUDPC = 2534\" , size = 4)+\n  scale_x_continuous(breaks = c(0, 7, 14, 21, 28, 35, 42, 49))\nggsave(\"imgs/dpc3.png\")\n\n\n\n\n\nFigure 8.6: Representation of the area under the disease progress curve calculated using the trapezoidal method\n\n\nIn R, we can obtain the AUDPC for the DPC we created earlier using the AUDPC function offered by the epifitter package. Because we are using the percent data, we need to set the argument y_proportion = FALSE. The function returns the absolute AUDPC. If one is interested in relative AUDPC, the argument type should be set to \"relative\". There is also the alternative to AUDPC, the area under the disease progress stairs (AUDPS) (Simko and Piepho 2012).\n\nlibrary(epifitter)\nAUDPC(dpc$t, dpc$y, \n      y_proportion = FALSE)\n\n[1] 2534\n\n# The relative AUDPC \nAUDPC(dpc$t, dpc$y, \n      y_proportion = FALSE, \n      type = \"relative\")\n\n[1] 0.5171429\n\n# To calculate AUDPS, the alternative to AUDPC\nAUDPS(dpc$t, dpc$y, \n      y_proportion = FALSE)\n\n[1] 2884\n\n\n\n\n\n\nJeger, M. J., and Viljanen-Rollinson, S. L. H. 2001. The use of the area under the disease-progress curve (AUDPC) to assess quantitative disease resistance in crop cultivars. Theoretical and Applied Genetics. 102:32–40 Available at: http://dx.doi.org/10.1007/s001220051615.\n\n\nSimko, I., and Piepho, H.-P. 2012. The Area Under the Disease Progress Stairs: Calculation, Advantage, and Application. Phytopathology®. 102:381–389 Available at: http://dx.doi.org/10.1094/phyto-07-11-0216.\n\n\nVanderplank, J. 1963. Plant disease epidemics and control. Elsevier. Available at: http://dx.doi.org/10.1016/C2013-0-11642-X."
  },
  {
    "objectID": "temporal-models.html#non-flexible-models",
    "href": "temporal-models.html#non-flexible-models",
    "title": "9  Population models",
    "section": "9.1 Non-flexible models",
    "text": "9.1 Non-flexible models\nThese population dynamics models require at least two parameters, hence they are known as non-flexible, as opposed to the flexible ones for which there are at least one additional (third) parameter.\nFollowing the convention proposed by (Madden et al. 2007) in their book “The study of plant disease epidemics”:\n\ntime is represented by \\(t\\)\ndisease intensity by \\(y\\)\nthe rate of change in \\(y\\) between two time units is represented by \\(\\frac{dy}{dt}\\)\n\nNow we can proceed and learn which non-flexible models exist and for which situation they are more appropriate.\n\n9.1.1 Exponential\nThe differential equation for the exponential model is given by\n\\(\\frac{dy}{dt} = r_E.y\\),\nwhere \\(r_E\\) is the apparent infection rate (subscript E for this model) (sensu Vanderplank) and \\(y\\) is the disease intensity. Biologically, this formulation suggests that diseased plants, or \\(y\\), and \\(r_E\\) at each time contribute to disease increase. The value of \\(\\frac{dy}{dt}\\) is minimal when \\(y = 0\\) and increases exponentially with the increase in \\(y\\).\nThe integral for the exponential model is given by\n\\(y = y_0 e^{r_Et}\\),\nwhere \\(y0\\) is and \\(r\\) are obtained via estimation. Let’s simulate two curves by varying \\(r\\) while fixing \\(y0\\) and varying the latter while fixing \\(r_E\\). We produce the two plots in ggplot and add the predicted curve using the `stat_function`. But first, we need to define values for the two model parameters. Further modifications to these values will be handled directly in the simulation (e.g. doubling infection rate, reducing initial inoculum by half, etc.).\n\nlibrary(tidyverse) # essential packages \nlibrary(cowplot)\nlibrary(r4pde)\ntheme_set(theme_r4pde()) # set global theme\n\n\ny0 &lt;- 0.001 \nr &lt;- 0.06 \ntmax &lt;- 60 # maximum duration t of the epidemics\ndat &lt;- data.frame(t = seq(1:tmax), y = seq(0:1)) # define the axes\n\nIn the plot below, note that the infection rate in one curve was doubled (\\(r\\) = 0.12)\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) y0 * exp(r * t), linetype = 1) +\n  stat_function(fun = function(t) y0 * exp(r * 2 * t), linetype = 2) +\n  ylim(0, 1) +\n  theme_r4pde()+\n  labs(x = \"Time\")\n\n\n\n\nFigure 9.1: Exponential curves with two rates of infection (0.06 and 0.12) and the same initial inoculum (0.001)\n\n\n\n\nNow the inoculum was increased five times while using the same doubled rate.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) y0 * exp(r * 2 * t), linetype = 1) +\n  stat_function(fun = function(t) y0 * 5 * exp(r * 2 * t), linetype = 2) +\n  ylim(0, 1) +\n  theme_r4pde()+\n  labs(x = \"Time\")\n\n\n\n\nFigure 9.2: Exponential curves with the same rate of infection (0.12) and single and five times the initial inoculum (0.001)\n\n\n\n\n\n\n9.1.2 Monomolecular\nThe differential of the monomolecular model is given by\n\\(\\frac{dy}{dt} = r_M (1-y)\\)\nwhere now the \\(r_M\\) is the rate parameter of the monomolecular model and \\((1-y)\\) is the proportion of non-infected (healthy) individuals or host tissue. Note that \\(\\frac{dy}{dt}\\) is maximum when \\(y = 0\\) and decreases when \\(y\\) approaches 1. Its decline is due to decrease in the proportion of individuals or healthy sites with the increase in \\(y\\). Any inoculum capable of infecting the host will more likely land on infected individuals or sites.\nThe integral of the monomolecular model is given by\n\\(\\frac{dy}{dt} = 1 - (1-y)e^{-r_Mt}\\)\nThis model commonly describes the temporal patterns of the monocyclic epidemics. In those, the inoculum produced during the course of the epidemics do not contribute new infections. Therefore, different from the exponential model, disease intensity \\(y\\) does not affect the epidemics and so the absolute rate is proportional to \\((1-y)\\).\nLet’s simulate two monomolecular curve with different rate parameters where one is one third of the other.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-r * t))) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-(r / 3) * t))) +\n  labs(x = \"Time\") +\n theme_r4pde()+\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"r = 0.06\") +\n  annotate(geom = \"text\", x = 50, y = 0.55, label = \"r = 0.02\")\n\n\n\n\nFigure 9.3: Monomolecular curves with two rates of infection (0.06 and 0.02) and the same initial inoculum (0.001)\n\n\n\n\nNow inoculum was increased 100 times with the reduced rate.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-r / 2 * t))) +\n  stat_function(fun = function(t) 1 - ((1 - (y0 * 100)) * exp(-r / 2 * t))) +\n  theme_r4pde()+\n  labs(x = \"Time\") +\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"y0 = 0.1\") +\n  annotate(geom = \"text\", x = 45, y = 0.65, label = \"y0 = 0.001\")\n\n\n\n\nFigure 9.4: Monomolecular curves with one rate (0.06) and the initial inoculum increased 100 times\n\n\n\n\n\n\n9.1.3 Logistic\nThe logistic model is a more elaborated version of the two previous models as it incorporates the features of them both. Its differential is given by\n\\(\\frac{dy}{dt} = r_L. y . (1 - y)\\),\nwhere \\(r_L\\) is the infection rate of the logistic model, \\(y\\) is the proportion of diseased individuals or host tissue and \\((1-y)\\) is the proportion of non-affected individuals or host area.\nBiologically, \\(y\\) in its differential equation implies that \\(\\frac{dy}{dt}\\) increases with the increase in \\(y\\) (as in the exponential) because more disease means more inoculum. However, \\((1-y)\\) leads to a decrease in \\(\\frac{dy}{dt}\\) when \\(y\\) approaches the maximum \\(y=1\\), because the proportion of healthy individuals or host area decreases (as in the monomolecular). Therefore, \\(\\frac{dy}{dt}\\) is minimal at the onset of the epidemics, reaches a maximum when \\(y/2\\) and declines until \\(y=1\\).\nThe integral is given by\n\\(y = \\frac{1}{1 + (1-y_0).e^{-r.t}}\\),\nwhere \\(r_L\\) is the apparent infection rate of the logistic model and \\(y0\\) is the disease intensity at \\(t=0\\). This model provides a good fit to polycyclic epidemics.\nLet’s check two curves where in one the infection rate is double while keeping the same initial inoculum.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 2 * t))\n  ) +\n  stat_function(fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 4 * t))) +\n  labs(x = \"Time\") +\ntheme_r4pde()+\n  annotate(geom = \"text\", x = 41, y = 0.77, label = \"r = 0.18\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"r = 0.024\")\n\n\n\n\nFigure 9.5: Logistic curves with two rates of infection (0.18 and 0.024) and the same initial inoculum (0.001)\n\n\n\n\nNow the inoculum is reduced 10 times for a same infection rate.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) 1 / (1 + ((1 - (y0 / 10)) / (y0 / 10)) * exp(-r * 3 * t))\n  ) +\n  stat_function(fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 3 * t))) +\n  labs(x = \"Time\") +\n theme_r4pde()+\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"y0 = 0.001\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"y0 = 0.0001\")\n\n\n\n\nFigure 9.6: Logistic curves with a single rate of infection (0.24) and two initial inoculum (0.001 and 0.0001)\n\n\n\n\n\n\n9.1.4 Gompertz\nThe Gompertz model is similar to the logistic and also provides a very good fit to several polycyclic diseases. The differential equation is given by\n\\(\\frac{dy}{dt} = r_G.[ln(1) - ln(y)]\\)\nDifferently from the logistic, the variable representing the non-infected individuals or host area is \\(-ln(y)\\). The integral equation is given by\n\\(y = e^{(ln(y0)).{e^{-r_G.t)}}}\\),\nwhere \\(r_G\\) is the apparent infection rate for the Gompertz models and \\(y_0\\) is the disease intensity at \\(t = 0\\).\nLet’s check curves for two rates.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) exp(log(y0) * exp(-r/2 * t))\n  ) +\n  stat_function(fun = function(t) exp(log(y0) * exp(-r*2 * t))) +\n  labs(x = \"Time\") +\n theme_r4pde()+\n  annotate(geom = \"text\", x = 41, y = 0.77, label = \"r = 0.12\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"r = 0.03\")\n\n\n\n\nFigure 9.7: Gompertz curves with two rates of infection (0.12 and 0.03) and the same initial inoculum (0.001)\n\n\n\n\nAnd those when inoculum was reduced one thousand times.\n\ndat |&gt;\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) exp(log(y0) * exp(-r*2 * t))\n  ) +\n  stat_function(fun = function(t) exp(log(y0/1000) * exp(-r*2 * t))) +\n  labs(x = \"Time\") +\n theme_r4pde()+\n  annotate(geom = \"text\", x = 15, y = 0.77, label = \"y0 = 0.001\") +\n  annotate(geom = \"text\", x = 25, y = 0.10, label = \"y0 = 0.00001\")\n\n\n\n\nFigure 9.8: Gompertz curves with a single rate of infection (0.12) and two levels of initial inoculum (0.001 and 0.00001)"
  },
  {
    "objectID": "temporal-models.html#interactive-application",
    "href": "temporal-models.html#interactive-application",
    "title": "9  Population models",
    "section": "9.2 Interactive application",
    "text": "9.2 Interactive application\nA shiny app was developed to demonstrate these four models interactively. Click on the image below to get access to the app.\n\n\n\nFigure 9.9: Screenshot of the application to visualize the population dynamics models by varying the model's parameters\n\n\n\n\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. Temporal analysis i: Quantifying and comparing epidemics. In The American Phytopathological Society, p. 63–116. Available at: http://dx.doi.org/10.1094/9780890545058.004."
  },
  {
    "objectID": "temporal-fitting.html#linear-regression-single-epidemics",
    "href": "temporal-fitting.html#linear-regression-single-epidemics",
    "title": "10  Model fitting",
    "section": "10.1 Linear regression: single epidemics",
    "text": "10.1 Linear regression: single epidemics\n\nlibrary(tidyverse)\nlibrary(r4pde)\ntheme_set(theme_r4pde())\ndpc &lt;- \n  tribble(\n   ~t,  ~y, \n   0,  0.1, \n   7,  1, \n  14,  9, \n  21,  25, \n  28,  80, \n  35, 98, \n  42, 99, \n  49, 99.9\n  )\n\n\ndpc |&gt; \n  ggplot(aes(t, y))+\n  geom_point(size =3)+\n  geom_line()+\n  theme_r4pde()+\n  labs(x = \"Time\", y = \"Disease intensity (%)\")\n\n\n\n\nFigure 10.1: Disease progress curves for one tobacco etch epidemics in pepper. Reproduced from Madden et al. (2007) page 94\n\n\n\n\nTo start, we’ll need to transform the disease intensity (in proportion scale) data according to each of the models we aim to fit. In this instance, we’ll look at the four models discussed in the previous chapter: exponential, monomolecular, logistic, and Gompertz. We can use the mutate() function of dplyr package. The transformed y will be referred to as y* (or y2 in the code) followed by the letter E, M, L or G, for each model (exponential, monomolecular, etc) respectively.\n\ndpc1 &lt;- dpc |&gt; \n  mutate(y = y/100) |&gt; # transform to proportion\n  mutate(exponential = log(y),\n         monomolecular = log(1 / (1 - y)),\n         logistic = log(y / (1 - y)),\n         gompertz = -log(-log(y)))\nknitr::kable(round(dpc1, 4)) \n\n\n\n\nt\ny\nexponential\nmonomolecular\nlogistic\ngompertz\n\n\n\n\n0\n0.001\n-6.9078\n0.0010\n-6.9068\n-1.9326\n\n\n7\n0.010\n-4.6052\n0.0101\n-4.5951\n-1.5272\n\n\n14\n0.090\n-2.4079\n0.0943\n-2.3136\n-0.8788\n\n\n21\n0.250\n-1.3863\n0.2877\n-1.0986\n-0.3266\n\n\n28\n0.800\n-0.2231\n1.6094\n1.3863\n1.4999\n\n\n35\n0.980\n-0.0202\n3.9120\n3.8918\n3.9019\n\n\n42\n0.990\n-0.0101\n4.6052\n4.5951\n4.6001\n\n\n49\n0.999\n-0.0010\n6.9078\n6.9068\n6.9073\n\n\n\n\n\nNow we can plot the curves using the transformed values regressed against time. The curve that appears most linear, closely coinciding with the regression fit line, is a strong candidate for the best-fitting model. To accomplish this, we’ll first reshape the dataframe into long format, and then generate plots for each of the four models.\n\ndpc2 &lt;- dpc1 |&gt; \n  pivot_longer(3:6, names_to = \"model\", values_to = \"y2\") \n\n\ndpc2 |&gt; \n  ggplot(aes(t, y2))+\n  geom_point()+\n  geom_smooth(method = \"lm\", color = \"black\", se = F)+\n  facet_wrap(~ model)+\n  theme_r4pde()+\n  labs(x = \"Time\", y = \"Transformed value (y*)\",\n       color = \"Model\")+\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 10.2: Curves of the transformed data for each epidemiological against time. The goal is to check which of the models provides the best fit based on the straight line\n\n\n\n\nFor this particular curve, it’s readily apparent that the logistic model offers the best fit to the data, as evidenced by the data points being closely aligned with the regression line, compared to the other models. However, to make a more nuanced decision between the logistic and Gompertz models—which are both typically used for sigmoid curves—we can rely on additional statistical measures.\nSpecifically, we can fit a regression model for each and examine key metrics such as the R-squared value and the residual standard error. To further validate the model’s accuracy, we can use Lin’s Concordance Correlation Coefficient to assess how closely the model’s predictions match the actual (transformed) data points.\nFor this exercise, let’s focus on the logistic and Gompertz models. We’ll start by fitting the logistic model and then move on to analyzing the summary of the regression model.\n\nlogistic &lt;- dpc2 |&gt; \n  filter(model == \"logistic\")\n\nm_logistic &lt;- lm(y2 ~ t, data = logistic)\n\n\n# R-squared\nsummary(m_logistic)$r.squared\n\n[1] 0.9923659\n\n# RSE \nsummary(m_logistic)$sigma\n\n[1] 0.4523616\n\n# calculate the Lin's CCC\nlibrary(epiR)\nccc_logistic &lt;- epi.ccc(logistic$y2, predict(m_logistic))\nccc_logistic$rho.c[1]\n\n        est\n1 0.9961683\n\n\nWe repeat the procedure for the Gompertz model.\n\ngompertz &lt;- dpc2 |&gt; \n  filter(model == \"gompertz\")\n\nm_gompertz &lt;- lm(y2 ~ t, data = gompertz)\n\n# R-squared\nsummary(m_gompertz)$r.squared\n\n[1] 0.9431066\n\n# RSE \nsummary(m_gompertz)$sigma\n\n[1] 0.8407922\n\n# calculate the Lin's CCC\nlibrary(epiR)\nccc_gompertz &lt;- epi.ccc(gompertz$y2, predict(m_gompertz))\nccc_gompertz$rho.c[1]\n\n        est\n1 0.9707204\n\n\nNext, let’s extract the two parameters of interest from each fitted model and incorporate them into the integral form of the respective models. To do this, we’ll need to back-transform the intercept, which represents the initial inoculum. This can be accomplished using specific equations, which we’ll outline next.\n\n\n\nModel\nTransformation\nBack-transformation\n\n\n\n\nExponential\nlog(y)\nexp(y*E)\n\n\nMonomolecular\nlog(1 / (1 - y))\n1 - exp(-y*M)\n\n\nLogistic\nlog(y / (1 - y))\n1 / (1 + exp(-y*L))\n\n\nGompertz\n-log(-log(y))\nexp(-exp(-y*G))\n\n\n\n\nrL &lt;- m_logistic$coefficients[2]\nrL\n\n        t \n0.2784814 \n\ny02 &lt;- m_logistic$coefficients[1]\ny0L = 1 / (1 + exp(-y02))\ny0L\n\n(Intercept) \n0.001372758 \n\nrG &lt;-m_gompertz$coefficients[2]\nrG\n\n        t \n0.1848378 \n\ny03 &lt;- m_gompertz$coefficients[1]\ny0G &lt;- exp(-exp(-y03))\ny0G\n\n (Intercept) \n1.968829e-09 \n\n\nNow the plot:\n\nlogistic |&gt;\n  ggplot(aes(t, y)) +\n  geom_point(size = 2)+\n  stat_function(\n    linetype = 2,\n    fun = function(t) 1 / (1 + ((1 - y0L) / y0L) * exp(-rL * t)))+\nstat_function(\n    linetype = 1,\n    fun = function(t) exp(log(y0G) * exp(-rG * t))\n  )+\n  theme_r4pde()+\n  labs(x = \"Time\", y = \"Disease intensity\")\n\n\n\n\nFigure 10.3: Disease progress curve and the fit of the logistic (dashed line) and the Gompertz (solid line) based on parameters estimated using linear regression\n\n\n\n\nIn this case, it’s clear that the logistic model (the solid line above) emerges as the best fit based on our statistical evaluation. The approach for model selection outlined here is straightforward and manageable when dealing with a single epidemic and comparing only two models. However, real-world scenarios often require analyzing multiple curves and fitting various models to each, making manual comparison impractical for selecting a single best-fitting model. To streamline this task, it’s advisable to automate the process using custom functions designed to simplify the coding work involved.\nThat’s where the epifitter package comes into play! This package offers a range of custom functions designed to automate the model fitting and selection process, making it much more efficient to analyze multiple curves across different epidemics. By using epifitter, one can expedite the statistical evaluation needed to identify the best-fitting models."
  },
  {
    "objectID": "temporal-fitting.html#non-linear-regression",
    "href": "temporal-fitting.html#non-linear-regression",
    "title": "10  Model fitting",
    "section": "10.2 Non linear regression",
    "text": "10.2 Non linear regression\nAlternatively, one can fit a nonlinear model to the data for each combination of curve and model using the nlsLM function in R of the minpack.lm package.\n\nlibrary(minpack.lm)\nfit_logistic &lt;- nlsLM(y/100 ~ 1 / (1+(1/y0-1)*exp(-r*t)), \n           start = list(y0 = 0.01, r = 0.3), \n           data = dpc)\n\nsummary(fit_logistic)\n\n\nFormula: y/100 ~ 1/(1 + (1/y0 - 1) * exp(-r * t))\n\nParameters:\n    Estimate Std. Error t value Pr(&gt;|t|)    \ny0 0.0003435  0.0002065   1.663    0.147    \nr  0.3321352  0.0249015  13.338  1.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02463 on 6 degrees of freedom\n\nNumber of iterations to convergence: 15 \nAchieved convergence tolerance: 1.49e-08\n\nfit_gompertz &lt;- nlsLM(y/100 ~ exp(log(y0/1)*exp(-r*t)), \n                    start = list(y0 = 0.01, r = 0.1), \n                    data = dpc)\nsummary(fit_gompertz)\n\n\nFormula: y/100 ~ exp(log(y0/1) * exp(-r * t))\n\nParameters:\n    Estimate Std. Error t value Pr(&gt;|t|)   \ny0 1.647e-14  3.712e-13   0.044  0.96605   \nr  1.626e-01  3.017e-02   5.391  0.00168 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06782 on 6 degrees of freedom\n\nNumber of iterations till stop: 50 \nAchieved convergence tolerance: 1.49e-08\nReason stopped: Number of iterations has reached `maxiter' == 50.\n\n\nWe can see that the model coefficients are not the same as those estimated using linear regression. Among other reasons, nls() often uses iterative techniques to estimate parameters, such as the Levenberg-Marquardt algorithm, which may provide different estimates than algebraic methods used in linear regression. While both methods aim to fit a model to data, they do so in ways that have distinct assumptions, strengths, and weaknesses, and this can result in different estimated parameters.\nBoth approaches—nonlinear least squares and linear regression on transformed data—have their own merits and limitations. The choice between the two often depends on various factors like the nature of the data, the underlying assumptions, and the specific requirements of the analysis. For an epidemiologist, the choice might come down to preference, familiarity with the techniques, or specific aims of the analysis.\nIn summary, both methods are valid tools in the toolkit of an epidemiologist or any researcher working on curve fitting and model selection. Understanding the nuances of each can help in making an informed choice tailored to the needs of a particular study."
  },
  {
    "objectID": "temporal-fitting.html#epifitter---multiple-epidemics",
    "href": "temporal-fitting.html#epifitter---multiple-epidemics",
    "title": "10  Model fitting",
    "section": "10.3 epifitter - multiple epidemics",
    "text": "10.3 epifitter - multiple epidemics\nWe will now examine three disease progress curves (DPCs) representing the incidence of the tobacco etch virus, a disease affecting peppers. Incidence evaluations were conducted at 7-day intervals up to 49 days. The relevant data can be found in Chapter 4, page 93, of the book “Study of Plant Disease Epidemics” (Madden et al. 2007). To get started, let’s input the data manually and create a data frame. The first column will represent the assessment time, while the remaining columns will correspond to the treatments, referred to as ‘groups’ in the book, ranging from 1 to 3."
  },
  {
    "objectID": "temporal-fitting.html#entering-data",
    "href": "temporal-fitting.html#entering-data",
    "title": "10  Model fitting",
    "section": "10.4 Entering data",
    "text": "10.4 Entering data\n\nlibrary(tidyverse) # essential packages \ntheme_set(theme_bw(base_size = 16)) # set global theme\n\n\npepper &lt;- \n  tribble(\n   ~t,  ~`1`,  ~`2`,  ~`3`,\n   0,  0.08, 0.001, 0.001,\n   7,  0.13,  0.01, 0.001,\n  14,  0.78,  0.09,  0.01,\n  21,  0.92,  0.25,  0.05,\n  28,  0.99,   0.8,  0.18,\n  35, 0.995,  0.98,  0.34,\n  42, 0.999,  0.99,  0.48,\n  49, 0.999, 0.999,  0.74\n  )"
  },
  {
    "objectID": "temporal-fitting.html#visualize-the-dpcs",
    "href": "temporal-fitting.html#visualize-the-dpcs",
    "title": "10  Model fitting",
    "section": "10.5 Visualize the DPCs",
    "text": "10.5 Visualize the DPCs\nBefore proceeding with model selection and fitting, let’s visualize the three epidemics. The code below reproduces quite exactly the top plot of Fig. 4.15 (Madden et al. (2007) page 94). The appraisal of the curves might give us a hint on which models are the best candidates.\nBecause the data was entered in the wide format (each DPC is in a different column) we need to reshape it to the long format. The pivot_longer() function will do the job of reshaping from wide to long format so we can finally use the ggplot() function to produce the plot.\n\npepper |&gt; \n  pivot_longer(2:4, names_to =\"treat\", values_to = \"inc\") |&gt; \n  ggplot (aes(t, inc, \n              linetype = treat, \n              shape = treat, \n              group = treat))+\n  scale_color_grey()+\n  theme_grey()+\n  geom_line(linewidth = 1)+\n  geom_point(size =3, shape = 16)+\n  annotate(geom = \"text\", x = 15, y = 0.84, label = \"1\")+\n  annotate(geom = \"text\", x = 23, y = 0.6, label = \"2\")+\n  annotate(geom = \"text\", x = 32, y = 0.33, label = \"3\")+\n  labs(y = \"Disease incidence (y)\",\n       x = \"Time (days)\")+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 10.4: Disease progress curves for three tobacco etch epidemics in pepper. Reproduced from Madden et al. (2007) page 94\n\n\n\n\nMost of the three curves show a sigmoid shape with the exception of group 3 that resembles an exponential growth, not reaching the maximum value, and thus suggesting an incomplete epidemic. We can easily eliminate the monomolecular and exponential models and decide on the other two non-flexible models: logistic or Gompertz. To do that, let’s proceed to model fitting and evaluate the statistics for supporting a final decision. There are two modeling approaches for model fitting in epifitter: the linear or nonlinear parameter-estimation methods.\n\n10.5.1 epifitter: linear regression\nAmong the several options offered by epifitter we start with the simplest one, which is to fit a model to a single epidemics using the linear regression approach. For such, the fit_lin() requires two arguments: time (time) and disease intensity (y) each one as a vector stored or not in a dataframe.\nSince we have three epidemics, fit_lin() will be use three times. The function produces a list object with six elements. Let’s first look at the Stats dataframe of each of the three lists named epi1 to epi3.\n\nlibrary(epifitter)\nepi1 &lt;- fit_lin(time = pepper$t,  \n                y = pepper$`1` )\nknitr::kable(epi1$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nGompertz\n0.9848\n0.9700\n0.5911\n\n\nMonomolecular\n0.9838\n0.9681\n0.5432\n\n\nLogistic\n0.9782\n0.9572\n0.8236\n\n\nExponential\n0.7839\n0.6447\n0.6705\n\n\n\n\n\n\nepi2 &lt;- fit_lin(time = pepper$t,  \n  y = pepper$`2` )\nknitr::kable(epi2$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nLogistic\n0.9962\n0.9924\n0.4524\n\n\nGompertz\n0.9707\n0.9431\n0.8408\n\n\nMonomolecular\n0.9248\n0.8601\n1.0684\n\n\nExponential\n0.8971\n0.8134\n1.2016\n\n\n\n\n\n\nepi3 &lt;- fit_lin(time = pepper$t,  \n  y = pepper$`3` )\nknitr::kable(epi3$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nLogistic\n0.9829\n0.9665\n0.6045\n\n\nGompertz\n0.9825\n0.9656\n0.2263\n\n\nExponential\n0.9636\n0.9297\n0.7706\n\n\nMonomolecular\n0.8592\n0.7531\n0.2534\n\n\n\n\n\nThe statistics of the model fit confirms our initial guess that the predictions by the logistic or the Gompertz are closer to the observations than predictions by the other models. There is a slight difference between them based on these statistics. However, to pick one of the models, it is important to inspect the curves with the observed and predicted values to check which model is best for all curves. For such, we can use the plot_fit() function from epifitter to explore visually the fit of the four models to each curve.\n\nplot_fit(epi1)+\n  ylim(0,1)+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\n\n\nknitr::kable(epi1$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbest_model\nmodel\nr\nr_se\nr_ci_lwr\nr_ci_upr\nv0\nv0_se\nr_squared\nRSE\nCCC\ny0\ny0_ci_lwr\ny0_ci_upr\n\n\n\n\n1\nGompertz\n0.1815713\n0.0130299\n0.1496882\n0.2134544\n-1.2050364\n0.3815570\n0.9700273\n0.5911056\n0.9847857\n0.0355477\n0.0002059\n0.2693349\n\n\n2\nMonomolecular\n0.1616413\n0.0119739\n0.1323423\n0.1909404\n-0.4625249\n0.3506326\n0.9681251\n0.5431977\n0.9838044\n-0.5880787\n-2.7452636\n0.3266178\n\n\n3\nLogistic\n0.2104047\n0.0181544\n0.1659824\n0.2548270\n-2.2715851\n0.5316185\n0.9572410\n0.8235798\n0.9781534\n0.0935038\n0.0273207\n0.2747287\n\n\n4\nExponential\n0.0487634\n0.0147802\n0.0125974\n0.0849293\n-1.8090602\n0.4328113\n0.6446531\n0.6705085\n0.7839381\n0.1638080\n0.0568061\n0.4723623\n\n\n\n\nplot_fit(epi2)+\n  ylim(0,1)+\n  scale_color_grey()+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\n\n\nknitr::kable(epi2$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbest_model\nmodel\nr\nr_se\nr_ci_lwr\nr_ci_upr\nv0\nv0_se\nr_squared\nRSE\nCCC\ny0\ny0_ci_lwr\ny0_ci_upr\n\n\n\n\n1\nLogistic\n0.2784814\n0.0099716\n0.2540818\n0.3028809\n-6.589560\n0.2919981\n0.9923659\n0.4523616\n0.9961683\n0.0013728\n0.0006724\n0.0028007\n\n\n2\nGompertz\n0.1848378\n0.0185339\n0.1394871\n0.2301886\n-2.998021\n0.5427290\n0.9431066\n0.8407922\n0.9707204\n0.0000000\n0.0000000\n0.0049309\n\n\n3\nMonomolecular\n0.1430234\n0.0235503\n0.0853979\n0.2006489\n-1.325645\n0.6896255\n0.8600832\n1.0683633\n0.9247793\n-2.7646136\n-19.3503499\n0.3035837\n\n\n4\nExponential\n0.1354579\n0.0264869\n0.0706469\n0.2002689\n-5.263915\n0.7756171\n0.8134015\n1.2015809\n0.8971003\n0.0051750\n0.0007757\n0.0345258\n\n\n\n\nplot_fit(epi3)+\n  ylim(0,1)+\n  scale_color_grey()+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\n\n\nknitr::kable(epi3$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbest_model\nmodel\nr\nr_se\nr_ci_lwr\nr_ci_upr\nv0\nv0_se\nr_squared\nRSE\nCCC\ny0\ny0_ci_lwr\ny0_ci_upr\n\n\n\n\n1\nLogistic\n0.1752146\n0.0133257\n0.1426077\n0.2078215\n-7.1136060\n0.3902187\n0.9664590\n0.6045243\n0.9829434\n0.0008133\n0.0003132\n0.0021104\n\n\n2\nGompertz\n0.0647145\n0.0049874\n0.0525107\n0.0769182\n-2.2849079\n0.1460470\n0.9655894\n0.2262550\n0.9824935\n0.0000541\n0.0000008\n0.0010358\n\n\n3\nExponential\n0.1513189\n0.0169860\n0.1097556\n0.1928822\n-6.8629493\n0.4974031\n0.9297097\n0.7705736\n0.9635747\n0.0010458\n0.0003097\n0.0035322\n\n\n4\nMonomolecular\n0.0238957\n0.0055853\n0.0102291\n0.0375624\n-0.2506567\n0.1635537\n0.7531307\n0.2533763\n0.8591837\n-0.2848689\n-0.9171854\n0.1389001\n\n\n\n\n\n\n\n10.5.2 epifitter: non linear regression\n\nepi11 &lt;- fit_nlin(time = pepper$t,  \n                y = pepper$`1` )\nknitr::kable(epi11$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nGompertz\n0.9963\n0.9956\n0.0381\n\n\nLogistic\n0.9958\n0.9939\n0.0403\n\n\nMonomolecular\n0.9337\n0.8883\n0.1478\n\n\nExponential\n0.7161\n0.5903\n0.2770\n\n\n\n\nepi22 &lt;- fit_nlin(time = pepper$t,  \n                y = pepper$`2` )\nknitr::kable(epi22$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nLogistic\n0.9988\n0.9981\n0.0246\n\n\nGompertz\n0.9904\n0.9857\n0.0683\n\n\nMonomolecular\n0.8697\n0.8020\n0.2329\n\n\nExponential\n0.8587\n0.7862\n0.2413\n\n\n\n\nepi33 &lt;- fit_nlin(time = pepper$t,  \n                y = pepper$`3` )\nknitr::kable(epi33$Stats)\n\n\n\n\n\nCCC\nr_squared\nRSE\n\n\n\n\nLogistic\n0.9957\n0.9922\n0.0270\n\n\nGompertz\n0.9946\n0.9894\n0.0306\n\n\nExponential\n0.9880\n0.9813\n0.0445\n\n\nMonomolecular\n0.8607\n0.7699\n0.1426\n\n\n\n\n\nAnd now we can produce the plot of the fitted curves together with the original incidence dat. The stats_all dataframe shows everything we need regarding the statistics and the values of the parameteres.\n\nplot_fit(epi11)+\n  scale_color_grey()+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\nknitr::kable(epi11$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ny0\ny0_se\nr\nr_se\ndf\nCCC\nr_squared\nRSE\ny0_ci_lwr\ny0_ci_upr\nr_ci_lwr\nr_ci_upr\nbest_model\n\n\n\n\nGompertz\n0.0000004\n0.0000017\n0.2865036\n0.0313160\n6\n0.9962708\n0.9956357\n0.0380871\n-0.0000039\n0.0000046\n0.2098761\n0.3631311\n1\n\n\nLogistic\n0.0092495\n0.0060330\n0.4181351\n0.0539931\n6\n0.9957910\n0.9938810\n0.0403179\n-0.0055126\n0.0240117\n0.2860188\n0.5502513\n2\n\n\nMonomolecular\n-0.0264718\n0.1405690\n0.0836206\n0.0212473\n6\n0.9337210\n0.8883222\n0.1477596\n-0.3704319\n0.3174883\n0.0316303\n0.1356109\n3\n\n\nExponential\n0.4159606\n0.1376766\n0.0215063\n0.0088658\n6\n0.7160769\n0.5903409\n0.2770420\n0.0790782\n0.7528430\n-0.0001874\n0.0432000\n4\n\n\n\n\nplot_fit(epi22)+\n  scale_color_grey()+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\nknitr::kable(epi22$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ny0\ny0_se\nr\nr_se\ndf\nCCC\nr_squared\nRSE\ny0_ci_lwr\ny0_ci_upr\nr_ci_lwr\nr_ci_upr\nbest_model\n\n\n\n\nLogistic\n0.0003435\n0.0002065\n0.3321341\n0.0249014\n6\n0.9988253\n0.9980693\n0.0246349\n-0.0001618\n0.0008488\n0.2712025\n0.3930657\n1\n\n\nGompertz\n0.0000000\n0.0000000\n0.1618741\n0.0301525\n6\n0.9904450\n0.9856896\n0.0682506\n0.0000000\n0.0000000\n0.0880937\n0.2356545\n2\n\n\nMonomolecular\n-0.1971531\n0.2005248\n0.0442060\n0.0131684\n6\n0.8696734\n0.8020353\n0.2328814\n-0.6878195\n0.2935134\n0.0119840\n0.0764280\n3\n\n\nExponential\n0.1612234\n0.0848398\n0.0410936\n0.0125521\n6\n0.8587176\n0.7862042\n0.2412526\n-0.0463722\n0.3688189\n0.0103797\n0.0718076\n4\n\n\n\n\nplot_fit(epi33)+\n  scale_color_grey()+\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\nknitr::kable(epi33$stats_all)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ny0\ny0_se\nr\nr_se\ndf\nCCC\nr_squared\nRSE\ny0_ci_lwr\ny0_ci_upr\nr_ci_lwr\nr_ci_upr\nbest_model\n\n\n\n\nLogistic\n0.0056634\n0.0019662\n0.1249931\n0.0086711\n6\n0.9957129\n0.9922359\n0.0270311\n0.0008522\n0.0104746\n0.1037758\n0.1462105\n1\n\n\nGompertz\n0.0000002\n0.0000008\n0.0759837\n0.0061025\n6\n0.9945740\n0.9894187\n0.0305883\n-0.0000017\n0.0000022\n0.0610515\n0.0909159\n2\n\n\nExponential\n0.0225267\n0.0072184\n0.0718824\n0.0070479\n6\n0.9880169\n0.9812854\n0.0445057\n0.0048638\n0.0401896\n0.0546367\n0.0891281\n3\n\n\nMonomolecular\n-0.1371485\n0.1060292\n0.0169603\n0.0042530\n6\n0.8606954\n0.7698769\n0.1426044\n-0.3965926\n0.1222956\n0.0065536\n0.0273669\n4\n\n\n\n\n\nFor multiple epidemics, we can use another handy function that allows us to simultaneously fit the models to multiple DPC data. Different from fit_lin(), fit_multi() requires the data to be structured in the long format where there is a column specifying each of the epidemics.\nLet’s then create a new data set called pepper2 using the data transposing functions of the tidyr package.\n\npepper2 &lt;- pepper |&gt; \n  pivot_longer(2:4, names_to =\"treat\", values_to = \"inc\")\n\nNow we fit the models to all DPCs. Note that the name of the variable indicating the DPC code needs to be informed in strata_cols argument. To use the nonlinear regression approach we set nlin argument to TRUE.\n\nepi_all &lt;- fit_multi(\n  time_col = \"t\",\n  intensity_col = \"inc\",\n  data = pepper2,\n  strata_cols = \"treat\",\n  nlin = FALSE\n)\n\nNow let’s select the statistics of model fitting. Again, Epifitter ranks the models based on the CCC (the higher the better) but it is important to check the RSE as well - the lower the better. In fact, the RSE is more important when the goal is prediction.\n\nepi_all$Parameters |&gt; \n  select(treat, model, best_model, RSE, CCC)\n\n   treat         model best_model       RSE       CCC\n1      1      Gompertz          1 0.5911056 0.9847857\n2      1 Monomolecular          2 0.5431977 0.9838044\n3      1      Logistic          3 0.8235798 0.9781534\n4      1   Exponential          4 0.6705085 0.7839381\n5      2      Logistic          1 0.4523616 0.9961683\n6      2      Gompertz          2 0.8407922 0.9707204\n7      2 Monomolecular          3 1.0683633 0.9247793\n8      2   Exponential          4 1.2015809 0.8971003\n9      3      Logistic          1 0.6045243 0.9829434\n10     3      Gompertz          2 0.2262550 0.9824935\n11     3   Exponential          3 0.7705736 0.9635747\n12     3 Monomolecular          4 0.2533763 0.8591837\n\n\nThe code below calculates the frequency that each model was the best. This would facilitate in the case of many epidemics to analyse.\n\nfreq_best &lt;- epi_all$Parameters %&gt;% \n    filter(best_model == 1) %&gt;% \n    group_by(treat, model) %&gt;% \n    summarise(first = n()) %&gt;%\n  ungroup() |&gt; \n  count(model) \nfreq_best \n\n# A tibble: 2 × 2\n  model        n\n  &lt;chr&gt;    &lt;int&gt;\n1 Gompertz     1\n2 Logistic     2\n\n\nWe can see that the Logistic model was the best model in two out of three epidemics.\nTo be more certain about our decision, let’s advance to the final step which is to produce the plots with the observed and predicted values for each assessment time by calling the Data dataframe of the `epi_all list.\n\nepi_all$Data |&gt;\n filter(model %in% c(\"Gompertz\", \"Logistic\")) |&gt; \n  ggplot(aes(time, predicted, shape = treat)) +\n  geom_point(aes(time, y)) +\n  geom_line() +\n  facet_wrap(~ model) +\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"bottom\")+\n coord_cartesian(ylim = c(0, 1)) + # set the max to 0.6\n  labs(\n    shape = \"Epidemic\",\n    y = \"Disease incidence\",\n    x = \"Time (days after emergence)\"\n  )\n\n\n\n\nFigure 10.5: Observed (dots) and fitted (line) values for three tobacco etch epidemics in pepper\n\n\n\n\nOverall, the logistic model seems a better fit for all the curves. Let’s produce a plot with the prediction error versus time.\n\nepi_all$Data |&gt;\n filter(model %in% c(\"Gompertz\", \"Logistic\")) |&gt; \n  ggplot(aes(time, predicted -y, shape = treat)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype =2)+\n  facet_wrap(~ model) +\n coord_cartesian(ylim = c(-0.4, 0.4)) + # set the max to 0.6\n  labs(\n    y = \"Prediction error\",\n    x = \"Time (days after emergence)\",\n    shape = \"Epidemic\"\n  )\n\n\n\n\nFigure 10.6: Prediction error (dotted lines) by two models fitted to the progress curves of three tobacco etch epidemics in pepper\n\n\n\n\nThe plots above confirms the logistic model as good fit overall because the errors for all epidemics combined are more scattered around the non-error line.\nWe can then now extract the parameters of interest of the chosen model. These data are stored in the Parameters data frame of the epi_all list. Let’s filter the Logistic model and apply a selection of the parameters of interest.\n\n  epi_all$Parameters |&gt;\n    filter(model == \"Logistic\") |&gt;\n    select(treat, y0, y0_ci_lwr, y0_ci_upr, r, r_ci_lwr, r_ci_upr \n)\n\n  treat           y0    y0_ci_lwr   y0_ci_upr         r  r_ci_lwr  r_ci_upr\n1     1 0.0935037690 0.0273207272 0.274728744 0.2104047 0.1659824 0.2548270\n2     2 0.0013727579 0.0006723537 0.002800742 0.2784814 0.2540818 0.3028809\n3     3 0.0008132926 0.0003131745 0.002110379 0.1752146 0.1426077 0.2078215\n\n\nWe can produce a plot for visual inference on the differences in the parameters.\n\np1 &lt;- epi_all$Parameters |&gt;\n  filter(model == \"Logistic\") |&gt;\n  ggplot(aes(treat, r)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Epidemic\",\n    y = \"r\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np2 &lt;- epi_all$Parameters |&gt;\n  filter(model == \"Logistic\") |&gt;\n  ggplot(aes(treat, 1 - exp(-y0))) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Epidemic\",\n    y = \"y0\"\n  )\n\nlibrary(patchwork)\np1 | p2\n\n\n\n\nFigure 10.7: Estimated infection rates (left) and initial inoculum (right) by a logistic model fitted to the progress curves of three epidemics of tobacco etch on pepper\n\n\n\n\nWe can compare the rate parameter (slopes) from two separate linear regression models using a t-test. This is sometimes referred to as a “test of parallelism” in the context of comparing slopes. The t-statistic for comparing two slopes with their respective standard errors can be calculated as:\n\\(t = \\frac{\\beta_1 - \\beta_2}{\\sqrt{SE_{\\beta_1}^2 + SE_{\\beta_2}^2}}\\)\nThis t-statistic follows a t-distribution with ( df = n_1 + n_2 - 4 ) degrees of freedom, where ( n_1 ) and ( n_2 ) are the sample sizes of the two groups. In our case, ( n_1 = n_2 = 8 ), so ( df = 8 + 8 - 4 = 12 ).\nHere’s how to perform the t-test for comparing curve 1 and 2.\n\n# Given slopes and standard errors from curve 1 and 2\nbeta1 &lt;- 0.2104 \nbeta2 &lt;- 0.2784 \nSE_beta1 &lt;- 0.01815 \nSE_beta2 &lt;- 0.00997\n\n# Sample sizes for both treatments (n1 and n2)\nn1 &lt;- 8\nn2 &lt;- 8\n\n# Calculate the t-statistic\nt_statistic &lt;- abs(beta1 - beta2) / sqrt(SE_beta1^2 + SE_beta2^2)\n\n# Degrees of freedom\ndf &lt;- n1 + n2 - 4\n\n# Calculate the p-value\np_value &lt;- 2 * (1 - pt(abs(t_statistic), df))\n\n# Print the results\nprint(paste(\"t-statistic:\", round(t_statistic, 4)))\n\n[1] \"t-statistic: 3.2837\"\n\nprint(paste(\"Degrees of freedom:\", df))\n\n[1] \"Degrees of freedom: 12\"\n\nprint(paste(\"p-value:\", round(p_value, 4)))\n\n[1] \"p-value: 0.0065\"\n\n\nThe pt() function in R gives the cumulative distribution function of the t-distribution. The 2 * (1 - pt(abs(t_statistic), df)) line calculates the two-tailed p-value. This will tell us if the slopes are significantly different at your chosen alpha level (commonly 0.05)."
  },
  {
    "objectID": "temporal-fitting.html#designed-experiments",
    "href": "temporal-fitting.html#designed-experiments",
    "title": "10  Model fitting",
    "section": "10.6 Designed experiments",
    "text": "10.6 Designed experiments\nIn the following section, we’ll focus on disease data collected over time from the same plot unit, also known as repeated measures. This data comes from a designed experiment aimed at evaluating and comparing the effects of different treatments.\nSpecifically, we’ll use a dataset of progress curves found on page 98 of “Study of Plant Disease Epidemics” (Madden et al. 2007). These curves depict the incidence of soybean plants showing symptoms of bud blight, which is caused by the tobacco streak virus. Four different treatments, corresponding to different planting dates, were evaluated using a randomized complete block design with four replicates. Each curve has four time-based assessments.\nThe data for this study is stored in a CSV file, which we’ll load into our environment using the read_csv() function. Once loaded, we’ll store the data in a dataframe named budblight.\n\n10.6.1 Loading data\n\nbudblight &lt;- read_csv(\"https://raw.githubusercontent.com/emdelponte/epidemiology-R/main/data/bud-blight-soybean.csv\")\n\nLet’s have a look at the first six rows of the dataset and check the data type for each column. There is an additional column representing the replicates, called block.\n\nbudblight\n\n# A tibble: 64 × 4\n   treat  time block     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 PD1      30     1  0.1 \n 2 PD1      30     2  0.3 \n 3 PD1      30     3  0.1 \n 4 PD1      30     4  0.1 \n 5 PD1      40     1  0.3 \n 6 PD1      40     2  0.38\n 7 PD1      40     3  0.36\n 8 PD1      40     4  0.37\n 9 PD1      50     1  0.57\n10 PD1      50     2  0.52\n# ℹ 54 more rows\n\n\n\n\n10.6.2 Visualizing the DPCs\nLet’s have a look at the curves and produce a combo plot figure similar to Fig. 4.17 of the book, but without the line of the predicted values.\n\np3 &lt;- budblight |&gt;\n  ggplot(aes(\n    time, y,\n    group = block,\n    shape = factor(block)\n  )) +\n  geom_point(size = 1.5) +\n  ylim(0, 0.6) +\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")+\n  facet_wrap(~treat, ncol =1)+\n  labs(y = \"Disease incidence\",\n       x = \"Time (days after emergence)\")\n\np4 &lt;- budblight |&gt;\n  ggplot(aes(\n    time, log(1 / (1 - y)),\n    group = block,\n    shape = factor(block)\n  )) +\n  geom_point(size = 2) +\n  facet_wrap(~treat, ncol = 1) +\n  scale_color_grey()+\n  theme_r4pde()+\n  theme(legend.position = \"none\")+\n  labs(y = \"Transformed incidence\", x = \"Time (days after emergence)\")\nlibrary(patchwork)\np3 | p4\n\n\n\n\nFigure 10.8: Disease progress curves for the incidence of budblight of soybean in Brazil for four planting dates\n\n\n\n\n\n\n10.6.3 Model fitting\nRemember that the first step in model selection is the visual appraisal of the curve data linearized with the model transformation. In the case the curves represent complete epidemics (close to 100%) appraisal of the absolute rate (difference in y between two times) over time is also helpful.\nFor the treatments above, it looks like the curves are typical of a monocyclic disease (the case of soybean bud blight), for which the monomolecular is usually a good fit, but other models are also possible as well. For this exercise, we will use both the linear and the nonlinear estimation method.\n\n10.6.3.1 Linear regression\nFor convenience, we use the fit_multi() to handle multiple epidemics. The function returns a list object where a series of statistics are provided to aid in model selection and parameter estimation. We need to provide the names of columns (arguments): assessment time (time_col), disease incidence (intensity_col), and treatment (strata_cols).\n\nlin1 &lt;- fit_multi(\n  time_col = \"time\",\n  intensity_col = \"y\",\n  data = budblight,\n  strata_cols = \"treat\",\n  nlin = FALSE\n)\n\nLet’s look at how well the four models fitted the data. Epifitter suggests the best fitted model (1 to 4, where 1 is best) for each treatment. Let’s have a look at the statistics of model fitting.\n\nlin1$Parameters |&gt; \nselect(treat, best_model, model, CCC, RSE)\n\n   treat best_model         model       CCC        RSE\n1    PD1          1 Monomolecular 0.9348429 0.09805661\n2    PD1          2      Gompertz 0.9040182 0.22226189\n3    PD1          3      Logistic 0.8711178 0.44751963\n4    PD1          4   Exponential 0.8278055 0.36124036\n5    PD2          1 Monomolecular 0.9547434 0.07003116\n6    PD2          2      Gompertz 0.9307192 0.17938711\n7    PD2          3      Logistic 0.9062012 0.38773023\n8    PD2          4   Exponential 0.8796705 0.32676216\n9    PD3          1 Monomolecular 0.9393356 0.06832499\n10   PD3          2      Gompertz 0.9288436 0.17156394\n11   PD3          3      Logistic 0.9085414 0.39051075\n12   PD3          4   Exponential 0.8896173 0.33884790\n13   PD4          1      Gompertz 0.9234736 0.17474422\n14   PD4          2 Monomolecular 0.8945962 0.06486949\n15   PD4          3      Logistic 0.8911344 0.52412586\n16   PD4          4   Exponential 0.8739618 0.49769642\n\n\nAnd now we extract values for each parameter estimated from the fit of the monomolecular model.\n\nlin1$Parameters |&gt;\nfilter(model == \"Monomolecular\") |&gt;\nselect(treat, y0, r)\n\n  treat         y0          r\n1   PD1 -0.5727700 0.02197351\n2   PD2 -0.5220593 0.01902952\n3   PD3 -0.4491365 0.01590586\n4   PD4 -0.3619898 0.01118047\n\n\nNow we visualize the fit of the monomolecular model (using filter function - see below) to the data together with the observed data and then reproduce the right plots in Fig. 4.17 from the book.\n\nlin1$Data |&gt;\n  filter(model == \"Monomolecular\") |&gt;\n  ggplot(aes(time, predicted)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(aes(time, y)) +\n  geom_line(linewidth = 0.5) +\n  facet_wrap(~treat) +\n  coord_cartesian(ylim = c(0, 0.6)) + # set the max to 0.6\n  labs(\n    y = \"Disease incidence\",\n    x = \"Time (days after emergence)\"\n  )\n\n\n\n\nFigure 10.9: Observed (dot) and fitted values by a monomolecular model (line) to the data on the incidence of budblight of soybean in Brazil for four planting dates\n\n\n\n\nNow we can plot the means and respective 95% confidence interval of the apparent infection rate (\\(r\\)) and initial inoculum (\\(y_0\\)) for visual inference.\n\np5 &lt;- lin1$Parameters |&gt;\n  filter(model == \"Monomolecular\") |&gt;\n  ggplot(aes(treat, r)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Epidemic\",\n    y = \"Infection rate (r)\"\n  )\n\np6 &lt;- lin1$Parameters |&gt;\n  filter(model == \"Monomolecular\") |&gt;\n  ggplot(aes(treat, y0)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"Initial inoculum (y0)\"\n  )\np5 | p6\n\n\n\n\nFigure 10.10: Estimates of the infection rate (left) and initial inoculum (right) from the fit of a monomolecular model to the data on the incidence of budblight of soybean in Brazil for four planting dates\n\n\n\n\n\n\n10.6.3.2 Non-linear regression\nTo estimate the parameters using the non-linear approach, we repeat the same arguments in the fit_multi function, but include an additional argument nlin set to TRUE.\n\nnlin1 &lt;- fit_multi(\n  time_col = \"time\",\n  intensity_col = \"y\",\n  data = budblight,\n  strata_cols = \"treat\",\n  nlin = TRUE\n)\n\nLet’s check statistics of model fit.\n\nnlin1$Parameters |&gt;\nselect(treat, model, CCC, RSE, best_model)\n\n   treat         model       CCC        RSE best_model\n1    PD1 Monomolecular 0.9382991 0.06133704          1\n2    PD1      Gompertz 0.9172407 0.06986307          2\n3    PD1      Logistic 0.8957351 0.07700720          3\n4    PD1   Exponential 0.8544194 0.08799512          4\n5    PD2 Monomolecular 0.9667886 0.04209339          1\n6    PD2      Gompertz 0.9348370 0.05726761          2\n7    PD2      Logistic 0.9077857 0.06657793          3\n8    PD2   Exponential 0.8702365 0.07667322          4\n9    PD3 Monomolecular 0.9570853 0.04269129          1\n10   PD3      Gompertz 0.9261609 0.05443852          2\n11   PD3      Logistic 0.8997106 0.06203037          3\n12   PD3   Exponential 0.8703443 0.06891021          4\n13   PD4 Monomolecular 0.9178226 0.04595409          1\n14   PD4      Gompertz 0.9085579 0.04791331          2\n15   PD4      Logistic 0.8940731 0.05083336          3\n16   PD4   Exponential 0.8842437 0.05267415          4\n\n\nAnd now we obtain the two parameters of interest. Note that the values are not the sames as those estimated using linear regression, but they are similar and highly correlated.\n\nnlin1$Parameters |&gt;\nfilter(model == \"Monomolecular\") |&gt;\nselect(treat, y0, r)\n\n  treat         y0          r\n1   PD1 -0.7072562 0.02381573\n2   PD2 -0.6335713 0.02064629\n3   PD3 -0.5048763 0.01674209\n4   PD4 -0.3501234 0.01094368\n\n\n\np7 &lt;- nlin1$Parameters |&gt;\n  filter(model == \"Monomolecular\") |&gt;\n  ggplot(aes(treat, r)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Epidemic\",\n    y = \"Infection rate (r)\"\n  )\n\np8 &lt;- nlin1$Parameters |&gt;\n  filter(model == \"Monomolecular\") |&gt;\n  ggplot(aes(treat, y0)) +\n  scale_color_grey()+\n  theme_r4pde()+\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Epidemic\",\n    y = \"Initial inoculum (y0)\"\n  )\n\np7 | p8\n\n\n\n\nFigure 10.11: Estimates of the infection rate (left) and initial inoculum (right) from the fit of a monomolecular model to the data on the incidence of budblight of soybean in Brazil for four planting dates\n\n\n\n\n\n\n\n\nAlves, K. S., and Del Ponte, E. M. 2021. Analysis and simulation of plant disease progress curves in R: introducing the epifitter package. Phytopathology Research. 3 Available at: http://dx.doi.org/10.1186/s42483-021-00098-7.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. Temporal analysis i: Quantifying and comparing epidemics. In The American Phytopathological Society, p. 63–116. Available at: http://dx.doi.org/10.1094/9780890545058.004."
  },
  {
    "objectID": "spatial-gradients.html#introduction",
    "href": "spatial-gradients.html#introduction",
    "title": "11  Spatial gradients",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nThe assessment of disease in terms of its spatial distribution, particularly considering changes in its intensity as it spreads over distance, is defined as the “disease gradient.” It’s the dispersal, or migration, of the pathogen through various means—such as wind, vectors, rain, movement of infected material, or even human mediation—that encourages the spread of plant diseases within a field or across continents, thereby creating these disease gradients.\nThere exist two distinct types of gradients: the inoculum gradient, in which the availability of a host is not necessarily a prerequisite, and the disease gradient, where all three elements of the disease triangle are essential.\nIn the ensuing chapters, we shall explore examples of actual disease gradients measured in the field, each exhibiting its own unique pattern.\nOur first example, from Mundt’s 1999 study (Mundt et al. 1999), sought to measure the dispersal potential of the pathogenic bacteria, Xanthomonas oryzae pv. oryzae, which is responsible for leaf blight in rice. This study was conducted using experimental plots in the Philippines during the wet seasons of 1994 and 1995.\nThe data were made available in this tutorial. We enter the data manually and then produce two plots, one for each year.\n\nlibrary(tidyverse)\nlibrary(r4pde)\ntheme_set(theme_r4pde())\n\nxo &lt;- \ntibble::tribble(\n    ~d,   ~y4,   ~y5,\n     0, 3.083, 7.185,\n  0.22, 0.521,  0.38,\n  0.44, 0.083, 0.157,\n  0.66, 0.021, 0.028\n  )\n\ng1 &lt;- xo |&gt; \n  ggplot(aes(d, y4))+\n theme_r4pde()+\n  geom_point(size = 2)+\n  geom_line()+\n  ylim(0,8)+\n  labs(y = \"Number of new lesions\",\n       x = \"Distance (m)\",\n       title = \"1994 wet season\")\n\ng2 &lt;- xo |&gt; \n  ggplot(aes(d, y5))+\n  theme_r4pde()+\n  geom_point(size = 2)+\n  geom_line()+\n  ylim(0,8)+\n  labs(y = \"Number of new lesions\",\n       x = \"Distance (m)\",\n       title = \"1995 wet season\")\n\nlibrary(patchwork)\n(g1 | g2) +  plot_annotation(\n    caption = \"Source: Mundt et al. (1999)\")\n\n\n\n\nFigure 11.1: Primary gradients of bacerial blight of rice in two wet seasons in the Philippines\n\n\n\n\nThe second example of a disease gradient pertains to stripe rust, caused by Puccinia striiformis f. sp. tritici, on wheat. This data was collected during a field experiment conducted at Hermiston in 2002, as reported in Sackett’s 2005 study (Sackett and Mundt 2005). Later, the data was made publicly available in 2015, courtesy of Mikaberidze (Mikaberidze et al. 2015). For our discussion, we’ll manually input the data in a tibble format.\nThis tibble contains five columns. The first and second columns represent distances from the source of infection, denoted in feet and meters, respectively. The remaining three columns consist of measures of stripe rust severity, each from a separate replicated plot. These measurements offer us a quantifiable view of the disease gradient of stripe rust in the field, thereby shedding light on the infection’s spatial distribution and intensity.\n\nhermiston &lt;- \n  tibble::tribble(\n  ~dist_f, ~dist_m,  ~`1`,  ~`2`,  ~`3`,\n  0,        0,    65,    65,    39,\n  5,      1.5,    35,    44,   7.5,\n  10,       3,  21.5,  14.5,  1.75,\n  20,     6.1,     8,  0.75,   0.2,\n  40,    12.2,     1,  0.08, 0.025,\n  60,    18.3,  0.25, 0.026, 0.015,\n  80,    24.4, 0.035, 0.015, 0.009,\n  100,   30.5,  0.01, 0.003, 0.008,\n  120,   36.6, 0.008, 0.016,  0.01,\n  140,   42.7, 0.003, 0.003,  0.01,\n  160,   48.8, 0.001, 0.006, 0.006,\n  180,   54.9, 0.001, 0.002, 0.002,\n  200,     61, 0.001, 0.003, 0.004,\n  220,   67.1, 0.001, 0.003, 0.002,\n  240,   73.2, 0.001, 0.001,     0,\n  260,   79.2, 0.001, 0.002,     0,\n  280,   85.3, 0.001, 0.001,     0,\n  300,   91.4, 0.001, 0.001, 0.001\n  )\n\n\nlibrary(tidyverse)\nlibrary(ggthemes)\n\nhermiston |&gt; \n  pivot_longer(3:5, names_to = \"replicate\", values_to = \"severity\") |&gt; \n  ggplot(aes(dist_m, severity, color = replicate))+\n  theme_r4pde()+\n  theme(legend.position = \"bottom\")+\n  geom_point(size = 2)+\n  geom_line(size = 1)+\n  scale_color_grey()+\n  labs(x = \"Distance from the source (m)\",\n       y = \"Stripe rust severity (%)\",\n       color = \"Replicate\",\n       caption = \"source: Sackett et al. (2005)\")\n\n\n\n\nFigure 11.2: Primary gradients of stripe rust of wheat on a replicated experiment\n\n\n\n\nAs evidenced by the examples presented above, disease gradients, assuming a single source of inoculum, typically display a pattern wherein the disease’s intensity diminishes more steeply within shorter proximities to the source. Conversely, the decrease is less steep at greater distances, eventually reaching a point of either zero or a low background level with only occasional diseased plants.\nThe unique shapes of these gradients are largely influenced by mechanisms associated with the dispersal of the inoculum, which are contingent not only on the pathogen’s biological characteristics but also heavily upon environmental factors that can impact the pathogen’s dispersion.\nFrom this, we can categorize the resulting gradients into two types: primary and secondary. The primary gradient is generated solely from the initial source of the infection. On the other hand, the secondary gradient arises from the movement of inoculum that has been produced by plants previously infected due to the primary gradient. These secondary infections then spread to other plants situated at increasing distances from the initial source.\nAs the disease proliferates over time, it’s expected that a combination of both primary and secondary gradients will manifest. This interplay between the two gradient types contributes to the overall spread and severity of the disease within a given population and environment.\nAs an example of primary and secondary gradients, let’s visualize the gradients of Septoria leaf spot, caused by Septoria lycopersici, on tomato (Parker et al. 1997). The gradients were measured during two times, thus enabling a comparison of primary and secondary dispersal/disease gradients. More details of the study and experimental approach were provided in this tutorial. The data is entered below as a tribble and the plot produced using ggplot2.\n\nseptoria &lt;- \ntibble::tribble(\n ~d, ~date1, ~date4,\n 60,     75,    87,\n 120,    40,    78,\n 180,    30,    68,\n 240,    20,    62,\n 300,    15,    50,\n 360,    12,    27,\n 420,    10,    32,\n 480,    12,    12,\n 540,     8,    13,\n 600,     5,     5,\n 660,     4,     4\n                )\n\nseptoria |&gt; \n  pivot_longer(2:3, names_to = \"date\", \n               values_to = \"defoliation\") |&gt; \n  ggplot(aes(d, defoliation, color = date))+\n  theme_r4pde()+\n  geom_point()+\n  geom_line()+\n  scale_color_grey()+\n  annotate(geom = \"text\", x = 200, y = 12, \n           label = \"Primary gradient\", hjust = \"left\")+\n  annotate(geom = \"text\", x = 200, y = 72, \n           label = \"Secondary gradient\", hjust = \"left\")+\n  labs(x = \"Distance from focus (m)\",\n       y = \"Percent defoliation\",\n       color = \"Date\",\n       caption = \"Parker et al. (1997)\")\n\n\n\n\nFigure 11.3: Primary and secondary gradients of defoliation due to Septoria leaf spot on tomato\n\n\n\n\nWhen studying disease gradients, researchers need to make sure that there is a well-defined single source of inoculum. In gradients, this is called a focus (where foci are deemed the plural), from where the inoculum originates. Three types of foci can be defined: point, line or area sources. While the point source can be a plant or group of plants at any position in the plot or field (center or corner), line and area sources are usually defined as one or more rows of diseased plants at one side of the plot or field.\n\n\nCode\nlibrary(ggplot2)\n\nline &lt;- ggplot(data.frame(c(1:10),c(1:10)))+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 0, ymax = 10, fill = \"gray92\")+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 9.7, ymax = 10, color = \"black\", fill = \"#339966\")+\n  annotate(\"segment\", size = 2, x = 1, xend = 1, y = 9.5, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 3, xend = 3, y = 9.5, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 5, y = 9.5, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 7, xend = 7, y = 9.5, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 9, xend = 9, y = 9.5, yend = 2, arrow = arrow())+\n  ylim(0,10)+\n  xlim(0,10)+\n  coord_fixed()+\n  theme_void()+\n  labs(title = \"      Side line\")\n\narea &lt;- ggplot(data.frame(c(1:10),c(1:10)))+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 0, ymax = 10, fill = \"gray92\")+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 8.2, ymax = 10, color = \"black\", fill = \"#339966\")+\n  annotate(\"segment\", size = 2, x = 1, xend = 1, y = 8, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 3, xend = 3, y = 8, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 5, y = 8, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 7, xend = 7, y = 8, yend = 2, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 9, xend = 9, y = 8, yend = 2, arrow = arrow())+\n  ylim(0,10)+\n  xlim(0,10)+\n  coord_fixed()+\n  theme_void()+\n  labs(title = \"      Side area\")\n\npoint_central &lt;- ggplot(data.frame(c(1:10),c(1:10)))+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 0, ymax = 10, fill = \"gray92\")+\n  annotate(\"segment\", size = 2, x = 5, xend = 10, y = 5, yend = 10, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 10, y = 5, yend = 5, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 10, y = 5, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 0, y = 5, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 0, y = 5, yend = 5, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 0, y = 5, yend = 10, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 5, y = 5, yend = 10, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 5, xend = 5, y = 5, yend = 0, arrow = arrow())+\n   annotate(\"rect\", xmin = 5.5, xmax = 4.5, ymin = 4.5, ymax = 5.5, color = \"black\", fill = \"#339966\" )+\n  ylim(0,10)+\n  xlim(0,10)+\n  coord_fixed()+\n  theme_void()+\n  labs(title = \"    Central point/area\")\n\npoint_corner &lt;- ggplot(data.frame(c(1:10),c(1:10)))+\n  annotate(\"rect\", xmin = 0, xmax = 10, ymin = 0, ymax = 10, fill = \"gray92\")+\n  annotate(\"segment\", size = 2, x = 0, xend = 10, y = 10, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 6.6, y = 10, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 3.3, y = 10, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 0, y = 10, yend = 0, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 10, y = 10, yend = 3.3, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 10, y = 10, yend = 6.6, arrow = arrow())+\n  annotate(\"segment\", size = 2, x = 0, xend = 10, y = 10, yend = 10, arrow = arrow())+\n  annotate(\"rect\", xmin = 0, xmax = 1, ymin = 9, ymax = 10, color = \"black\", fill = \"#339966\")+\n  ylim(0,10)+\n  xlim(0,10)+\n  coord_fixed()+\n  theme_void()+\n  labs(title = \"    Corner point/area\")\n\nlibrary(patchwork)\np_gradients &lt;- (line | area)/\n(point_central | point_corner)\n\nggsave(\"imgs/gradients.png\", width =9, height =9, bg = \"white\")\n\n\n\n\n\nFigure 11.4: Example of location and size of inoculum sources for the study of disease gradients\n\n\n\n\n\n\nMikaberidze, A., Mundt, C. C., and Bonhoeffer, S. 2015. Data from: Invasiveness of plant pathogens depends on the spatial scale of host distribution. Available at: http://datadryad.org/stash/dataset/doi:10.5061/dryad.f2j8s.\n\n\nMundt, C. C., Ahmed, H. U., Finckh, M. R., Nieva, L. P., and Alfonso, R. F. 1999. Primary Disease Gradients of Bacterial Blight of Rice. Phytopathology®. 89:64–67 Available at: http://dx.doi.org/10.1094/phyto.1999.89.1.64.\n\n\nParker, S. K., Nutter, F. W., and Gleason, M. L. 1997. Directional Spread of Septoria Leaf Spot in Tomato Rows. Plant Disease. 81:272–276 Available at: http://dx.doi.org/10.1094/pdis.1997.81.3.272.\n\n\nSackett, K. E., and Mundt, C. C. 2005. Primary Disease Gradients of Wheat Stripe Rust in Large Field Plots. Phytopathology®. 95:983–991 Available at: http://dx.doi.org/10.1094/PHYTO-95-0983."
  },
  {
    "objectID": "spatial-models.html#exponential-model",
    "href": "spatial-models.html#exponential-model",
    "title": "12  Gradient models",
    "section": "12.1 Exponential model",
    "text": "12.1 Exponential model\nThe exponential model is also known as Kiyosawa & Shiyomi model. The differential of the exponential model is given by\n\\(\\frac{dy}{dx}\\) = \\(-b_{E}.y\\) ,\nwhere \\(b_{E}\\) is the exponential form of the rate of decline and \\(y\\) is the disease intensity. This model suggests that \\(y\\) (any disease intensity) is greater close to the source of inoculum, or at the distance zero. The integral form of the model is given by\n\\(y = a . e^{-b.x}\\) ,\nwhere \\(a\\) is the disease intensity at the distance zero and \\(b\\) is the rate of decline, in this case negative because disease intensity decreases with the increase of the distance from inoculum source. Let’s make a plot for two disease gradients of varying parameters for this model.\nFirst we need to load essential packages for programming, customizing the outputs and defining a global ggplot theme.\n\nlibrary(tidyverse)\nlibrary(r4pde)\ntheme_set(theme_r4pde()) # set global theme\n\nSet the parameters for the exponential model with two rates and the same inoculum level at the source:\n\na1 &lt;- 0.2 # y at distance zero for gradient 1\na2 &lt;- 0.2 # y at distance zero for gradient 2\nb1 &lt;- 0.1 # decline rate for gradient 1\nb2 &lt;- 0.05 # decline rate for gradient 2\nmax1 &lt;- 80 # maximum distance for gradient 1\nmax2 &lt;- 80 # maximum distance for gradient 2\ndat &lt;- data.frame(x = seq(1:max1), y = seq(0:a1))\n\nThe following code allows to visualize the model predictions.\n\ndat |&gt;\n  ggplot(aes(x, y)) +\n  theme_r4pde()+\n  stat_function(fun = function(x) a1 * exp(-b1 * x), linetype = 1) +\n  stat_function(fun = function(x) a2 * exp(-b2 * x), linetype = 2) +\n  ylim(0, a1) +\n  annotate(\"text\", x = 20, y = 0.04, label = \"b = 0.1\") +\n  annotate(\"text\", x = 20, y = 0.10, label = \"b = 0.05\") +\n  labs(x = \"Distance (m)\", y = \"Disease incidence (proportion)\"\n  )\n\n\n\n\nFigure 12.1: Exponential curves describing plant disease gradients"
  },
  {
    "objectID": "spatial-models.html#power-law-model",
    "href": "spatial-models.html#power-law-model",
    "title": "12  Gradient models",
    "section": "12.2 Power law model",
    "text": "12.2 Power law model\nAlso known as the modified Gregory’s model (Gregory was a pioneer in the use this model to describe plant disease gradients). In the power law model, \\(Y\\) is proportional to the power of the distance, and is given by:\n\\(Y = a_{P}.x - b_{P}\\)\nwhere \\(a_{P}\\) and \\(b_{P}\\) are the two parameters of the power law model. They differ from the exponential because as closer to \\(x\\) is to zero, \\(Y\\) is indefinitely large (not meaningful biologically). However, the model can still be useful because it produces realistic values at any distance \\(x\\) away from the source. The values of the \\(a_{P}\\) parameter should be interpreted in accord to the scale of \\(x\\), whether in centimeters or meters. If the distance between the source and the first measure away from the source is 0.5m, it is so more appropriate to record the distance in cm than in m or km.\nOnce \\(y\\) at the distance zero from the source is undefined when using the power law model, this is usually modified by the addition of a positive constant \\(C\\) in \\(x\\):\n\\(Y = a_{P}.(x + C) - b_{P}\\)\nFor this reason, the model is named as the modified power law. Here, the constant \\(C\\) is of the same unit of \\(x\\). At the distance zero, the positive constant is a term that express the size of the inoculum source. In other words, the \\(a\\) parameter is a theoretical value of \\(Y\\) at the distance \\(1-C\\) from the center of the inoculum source.\nLet’s plot two gradients with two rate parameters for the modified power law model:\n\nC &lt;- 0.5\na1 &lt;- 0.2 # y at zero distance for gradient 1\na2 &lt;- 0.2 # y at zero distance for gradient 2\nb1 &lt;- 0.5 # decline rate for gradient 1\nb2 &lt;- 0.7 # decline rate for gradient 2\nmax1 &lt;- 80 # maximum distance for gradient 1\nmax2 &lt;- 80 # maximum distance for gradient 2\ndat2 &lt;- data.frame(x = seq(1:max1), y = seq(0:a1))\n\n\ndat2 |&gt;\n  ggplot(aes(x, y)) +\n  theme_r4pde()+\n  stat_function(fun = function(x) a1 * ((x + C)^-b1), linetype = 1) +\n  stat_function(fun = function(x) a2 * ((x + C)^-b2), linetype = 2) +\n  ylim(0, a1 - 0.02) +\n  annotate(\"text\", x = 20, y = 0.03, label = \"b = 0.1\") +\n  annotate(\"text\", x = 20, y = 0.06, label = \"b = 0.05\") +\n  labs(x = \"Distance (m)\", y = \"Disease incidence\")\n\n\n\n\nFigure 12.2: Power law (modified) curves describing plant disease gradients\n\n\n\n\nThe differential equation of the power law model is given by:\n\\(\\frac{dy}{dx}\\) = \\(\\frac{-b_{P}.Y}{x - C}\\)\nSimilar to the exponential model, \\(\\frac{dy}{dx}\\) is proportional to \\(Y\\), meaning that the gradient is steeper (more negative) at the highest disease intensity value, usually closer to the source."
  },
  {
    "objectID": "spatial-models.html#linearization-of-the-models",
    "href": "spatial-models.html#linearization-of-the-models",
    "title": "12  Gradient models",
    "section": "12.3 Linearization of the models",
    "text": "12.3 Linearization of the models\n\n12.3.1 Transformations of y\nThe gradient models, again similar to the temporal disease models, are non linear in their parameters. The model is intrinsically linear if transformations are applied (according to the model) in both sides of the equations. The linear model in its generic state is given by\n\\(y* = a* + bx\\) ,\nwhere the asterisk in \\(a\\) indicated that one of the transformations was applied in \\(y\\) that produced the linear model. Note that \\(a*\\) is the transformed version of the initial disease intensity, which needs to be returned to the original scale according to the respective back-transformation. Follows the linearized form of the two most common gradient models.\n\\(ln(y) = ln(a_{E}) - b_{E}. x\\)\n\\(ln(y) = ln(a_{P}) - b_{E}. ln(x+C)\\)\n\n\n12.3.2 Plot for the linearized form of models\nLet’s visualize the linearization of the exponential model with two different slopes (gradient 1 and 2). Note that the transformation used was \\(ln(y)\\).\n\nC &lt;- 0.5\na1 &lt;- 0.2 # y at zero distance for gradient 1\na2 &lt;- 0.2 # y at zero distance for gradient 2\nb1 &lt;- 0.5 # decline rate for gradient 1\nb2 &lt;- 0.7 # decline rate for gradient 2\nmax1 &lt;- 80 # maximum distance for gradient 1\nmax2 &lt;- 80 # maximum distance for gradient 2\ndat2 &lt;- data.frame(x = seq(1:max1), y = seq(0:a1))\n\ndat2 |&gt;\n  ggplot(aes(x, y)) +\n  theme_r4pde()+\n  stat_function(fun = function(x) log(a1) - (b1 * x), linetype = 1) +\n  stat_function(fun = function(x) log(a2) - (b2 * x), linetype = 2) +\n  labs(x = \"log of distance (m)\", y = \"log of disease incidence\"\n  )\n\n\n\n\nFigure 12.3: Linearization of the exponential model describing plant disease gradients\n\n\n\n\nFollows the linearization of the modified power law model. Note that the transformation used was \\(ln(y)\\) and \\(ln(x+C)\\) .\n\nC &lt;- 0.5\na1 &lt;- 0.2 # y at zero distance for gradient 1\na2 &lt;- 0.2 # y at zero distance for gradient 2\nb1 &lt;- 0.5 # decline rate for gradient 1\nb2 &lt;- 0.7 # decline rate for gradient 2\nmax1 &lt;- log(80) # maximum distance for gradient 1\nmax2 &lt;- log(80) # maximum distance for gradient 2\ndat2 &lt;- data.frame(x = seq(1:max1), y = seq(0:a1))\n\ndat2 |&gt;\n  ggplot(aes(x, y)) +\n  theme_r4pde()+\n  stat_function(fun = function(x) log(a1) - (b1 * log(x + C)), linetype = 1) +\n  stat_function(fun = function(x) log(a2) - (b2 * log(x + C)), linetype = 2) +\n  labs(\n    title = \"Modified Power Law\",\n    subtitle = \"\",\n    x = \"log of distance (m)\",\n    y = \"log of disease incidence\"\n  )\n\n\n\n\nFigure 12.4: Linearization of the modified power law curves describing plant disease gradients"
  },
  {
    "objectID": "spatial-models.html#interactive-application",
    "href": "spatial-models.html#interactive-application",
    "title": "12  Gradient models",
    "section": "12.4 Interactive application",
    "text": "12.4 Interactive application\nA shiny app was developed to demonstrate these two models interactively. Click on the image below to get access to the app.\n\n\n\nFigure 12.5: Screenshot of the application to visualize the spatial disease gradient models by varying the model’s parameters"
  },
  {
    "objectID": "spatial-fitting.html#dataset",
    "href": "spatial-fitting.html#dataset",
    "title": "13  Fitting gradient models",
    "section": "13.1 Dataset",
    "text": "13.1 Dataset\nThe hypothetical data describe the gradient curve for the number of lesions counted at varying distances (in meters) from the source. Let’s create two vectors, one for the distances \\(x\\) and the other for the lesion count \\(Y\\), and then a data frame by combining the two vectors.\n\n# create the two vectors\nx &lt;- c(0.8, 1.6, 2.4, 3.2, 4, 7.2, 12, 15.2, 21.6, 28.8)\nY &lt;- c(184.9, 113.3, 113.3, 64.1, 25, 8, 4.3, 2.5, 1, 0.8)\ngrad1 &lt;- data.frame(x, Y) # create the dataframe\nknitr::kable(grad1) # show the gradient\n\n\n\n\nx\nY\n\n\n\n\n0.8\n184.9\n\n\n1.6\n113.3\n\n\n2.4\n113.3\n\n\n3.2\n64.1\n\n\n4.0\n25.0\n\n\n7.2\n8.0\n\n\n12.0\n4.3\n\n\n15.2\n2.5\n\n\n21.6\n1.0\n\n\n28.8\n0.8"
  },
  {
    "objectID": "spatial-fitting.html#visualize-the-gradient-curve",
    "href": "spatial-fitting.html#visualize-the-gradient-curve",
    "title": "13  Fitting gradient models",
    "section": "13.2 Visualize the gradient curve",
    "text": "13.2 Visualize the gradient curve\nThe gradient can be visualized using ggplot function.\n\ngrad1 |&gt; \n  ggplot(aes(x, Y))+\n  theme_r4pde(font_size = 16)+\n  geom_point(size = 2)+\n  geom_line()+\n  labs(y = \"Lesion count\",\n       x = \"Distance (m)\")\n\n\n\n\nFigure 13.1: Hypothetical gradient of lesion count over distances from the inoculum source"
  },
  {
    "objectID": "spatial-fitting.html#linear-regression",
    "href": "spatial-fitting.html#linear-regression",
    "title": "13  Fitting gradient models",
    "section": "13.3 Linear regression",
    "text": "13.3 Linear regression\nOne method to determine the best-fitting model for gradient data is through linear regression. Depending on the chosen model, the transformed \\(Y\\) variable is regressed against the distance (which could be either in its original form or transformed). By doing this, we can derive the model’s parameters and evaluate its fit using various statistics. Two primary ways to appraise the model’s fit are by visually inspecting the regression line and examining the coefficient of determination (often denoted as \\(R^2\\) ). Now, let’s proceed to fit each of the three models discussed in the previous chapter.\n\n13.3.1 Exponential model\nIn this model, the log of \\(Y\\) is taken and regressed against the (untransformed) distance from the focus. Let’s fit the model and examine the summary output of model fit.\n\nreg_exp &lt;- lm(log(Y) ~ x, data = grad1)\njtools::summ(reg_exp)\n\n\n\n\n\nObservations\n10\n\n\nDependent variable\nlog(Y)\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(1,8)\n57.39\n\n\nR²\n0.88\n\n\nAdj. R²\n0.86\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n4.58\n0.35\n13.00\n0.00\n\n\nx\n-0.20\n0.03\n-7.58\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nThe intercept \\(a\\) represents the natural logarithm (log) of the response variable when the predictor is at a distance of zero. The negative slope \\(-b\\) indicates the rate at which the response decreases as the predictor increases — this is the decline rate of the gradient. The adjusted R-squared value of 0.86 suggests that approximately 86% of the variability in the response variable can be explained by the predictor in the model. While this seems to indicate a good fit, it is essential to compare this coefficient with those from other models to determine its relative goodness of fit. Furthermore, visually inspecting a regression plot is crucial. By doing this, we can check for any patterns or residuals around the predicted line, which can provide insights into the model’s assumptions and potential areas of improvement\n\ngrad1 |&gt; \n  ggplot(aes(x, log(Y)))+\n  theme_r4pde(font_size = 16)+\n  geom_point(size = 2)+\n  geom_line()+\n  geom_abline(slope = coef(reg_exp)[[2]], intercept = coef(reg_exp)[[1]],\n              linewidth = 1, linetype = 2)+\n labs(y = \"Log of Lesion count\",\n       x = \"Distance (m)\")\n\n\n\n\nFigure 13.2: Fit of the exponential model to the log of lesion count over distances from the inoculum source\n\n\n\n\nFrom the aforementioned plot, it’s evident that the exponential model might not be the optimal choice. This inference is drawn from the noticeable patterns or residuals surrounding the regression fit line, suggesting that the model may not capture all the underlying structures in the data.\n\n\n13.3.2 Power law model\nFor the power law model, we employ a log-log transformation: the natural logarithm (log) of \\(Y\\) is regressed against the log of \\(X\\). Following this transformation, we apply the regression procedure to determine the model’s parameters. Additionally, we extract the relevant statistics to evaluate the model’s fit to the data\n\nreg_p &lt;- lm(log(Y) ~ log(x), data = grad1)\njtools::summ(reg_p)\n\n\n\n\n\nObservations\n10\n\n\nDependent variable\nlog(Y)\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(1,8)\n203.26\n\n\nR²\n0.96\n\n\nAdj. R²\n0.96\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n5.56\n0.25\n22.66\n0.00\n\n\nlog(x)\n-1.70\n0.12\n-14.26\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nThe plot presented below underscores the superiority of the power law model in comparison to the exponential model. One of the key indicators of this superior fit is the higher coefficient of determination, \\(R^2\\) for the power law model. A higher \\(R^2\\) value suggests that the model can explain a greater proportion of the variance in the dependent variable, making it a better fit for the data at hand.\n\ngrad1 |&gt; \n  ggplot(aes(log(x), log(Y)))+\n  theme_r4pde(font_size = 16)+\n  geom_point(size = 2)+\n  geom_line()+\n  geom_abline(slope = coef(reg_p)[[2]], intercept = coef(reg_p)[[1]],\n              linewidth = 1, linetype = 2)+\n labs(y = \"Log of Lesion count\",\n       x = \"Log of distance\")\n\n\n\n\nFigure 13.3: Fit of the power law model to the log of lesion count over log of the distance from the inoculum source\n\n\n\n\n\n\n13.3.3 Modified power law model\nIn the modified power model, a constant is added to \\(x\\).\n\nreg_pm &lt;- lm(log(Y) ~ log(x + 0.4), data = grad1)\njtools::summ(reg_pm)\n\n\n\n\n\nObservations\n10\n\n\nDependent variable\nlog(Y)\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(1,8)\n302.16\n\n\nR²\n0.97\n\n\nAdj. R²\n0.97\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n6.10\n0.23\n26.73\n0.00\n\n\nlog(x + 0.4)\n-1.88\n0.11\n-17.38\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\ngrad1 |&gt; \n  ggplot(aes(log(x+0.4), log(Y)))+\n  theme_r4pde(font_size = 16)+\n  geom_point(size = 2)+\n  geom_line()+\n  geom_abline(slope = coef(reg_pm)[[2]], intercept = coef(reg_pm)[[1]],\n              linewidth = 1, linetype = 2)+\n labs(y = \"Log of Lesion count\",\n       x = \"Log of distance + 0.4 (m)\")\n\n\n\n\nFigure 13.4: Fit of the modified power law model to the log of lesion count over log + 0.4 of the distances from the inoculum source\n\n\n\n\nAmong the models tested, the modified power law emerges as the most suitable choice based on its highest coefficient of determination, \\(R^2\\) . This conclusion is not only supported by the statistical metrics but also visibly evident when we examine the graphs of the fitted models.To further illustrate this, we’ll generate a gradient plot. On this plot, we’ll overlay the data with the best-fitting model — the modified power law. Remember, to accurately represent the data, we’ll need to back-transform the parameter \\(a\\) before plotting.\n\ngrad1 |&gt; \n  ggplot(aes(x, Y))+\n  theme_r4pde(font_size = 16)+\n  geom_point(size = 2)+\n  geom_line()+\n  stat_function(fun = function(x) intercept = exp(coef(reg_pm)[[1]]) * ((x + 0.4)^coef(reg_pm)[[2]]), linetype = 2) +\n  labs(y = \"Lesion count\",\n       x = \"Distance (m)\")"
  },
  {
    "objectID": "spatial-fitting.html#fit_gradients",
    "href": "spatial-fitting.html#fit_gradients",
    "title": "13  Fitting gradient models",
    "section": "13.4 fit_gradients",
    "text": "13.4 fit_gradients\nThe fit_gradients() function of the {r4pde} package is designed to take in a dataset consisting of two variables: distance (x) and some measure of the phenomenon (Y). Using this data, the function fits each of the three models and evaluates their performance by calculating the R-squared value for each fit. The higher the R-squared value, the better that particular model explains the variation in the data. Once the models are fit, the function returns a series of outputs:\n\nA table that summarizes the parameters and fit statistics of each model.\nDiagnostic plots that show how well each model fits the data in its transformed space.\nPlots that juxtapose the original, untransformed data against the fits from each of the three models.\n\nA notable feature is the addition of a constant (C) that can be adjusted in the modified power model. This provides flexibility in tweaking the model to better fit the data if necessary. By providing a comparative analysis of three gradient models, it enables users to quickly identify which model best represents the spatial patterns in their data.\nHere is how to use the function with our grad1 dataset. Then we show the table and two plots as outputs.\n\nlibrary(r4pde)\ntheme_set(theme_r4pde(font_size = 16))\n\nfit1 &lt;- fit_gradients(grad1, C = 0.4)\n\nknitr::kable(fit1$results_table) # display the table with coefficients and stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na.(Intercept)\nse_a\nsig_a\nb.x\nse_b\nsig_b\na_back.(Intercept)\nR2\n\n\n\n\nExponential\n4.577\n0.352\n**\n-0.201\n0.027\n**\n97.222\n0.878\n\n\nPower\n5.564\n0.246\n**\n-1.698\n0.119\n**\n260.864\n0.962\n\n\nModified_Power\n6.101\n0.228\n**\n-1.884\n0.108\n**\n446.304\n0.974\n\n\n\n\nlibrary(patchwork) # to place plots side by side\n(fit1$plot_power  |\n  fit1$plot_power_original)+\n  labs(title = \"\")\n\n\n\n\nEach plot can be further customized for publication purposes.\n\n fit1$plot_power_original +\n  labs(x = \"Distance from the focus (m)\",\n       y = \"Lesion count\",\n       title = \"\")"
  },
  {
    "objectID": "spatial-patterns.html#definitions",
    "href": "spatial-patterns.html#definitions",
    "title": "14  Spatial patterns",
    "section": "14.1 Definitions",
    "text": "14.1 Definitions\nA spatial disease pattern can be defined as the arrangement of diseased entities relative to each other and to the architecture of the host crop (Madden et al. 2007). Such arrangement is the realization of the underlying dispersal of the pathogen, from one or several sources within and/or outside the area of interest, under the influence of physical, biological and environmental factors.\nThe study of spatial patterns is conducted at a specific time or multiple times during the epidemic. When assessed multiple times, both spatial and temporal processes can be characterized. Because epidemics change over time, it is expected that spatial patterns are not constant but change over time as well. Usually, plant pathologists are interested in determining spatial patterns at one or various spatial scales, depending on the objective of the study. The scale of interest may be a leaf or root, plant, field, municipality, state, country or even intercontinental area. The diseased units observed may vary from lesions on a single leaf to diseased fields in a large production region.\nThe patterns can be classified into two main types that occur naturally: random or aggregated. The random pattern originates because the chances for the units (leaf, plant, crop) to be infected are equal and low, and are largely independent from each other. In aggregated spatial patterns, such chances are unequal and there is dependency among the units. For example, a healthy unit close to a diseased unit is at higher risk than more distant units.\nLet’s simulate in R two vectors (x,y) for the positions of diseased units that follow a random or an aggregated pattern. For the random pattern, we use runif, a function which generates random deviates from the uniform distribution.\n\nset.seed(123)          # for reproducibility\nx &lt;- runif(50, 0, 30)  # x vector\ny &lt;- runif(50, 0, 30)  # y vector\ndat &lt;- data.frame(x,y) # dataframe for plotting\n\nNow, the plot to visualize the random pattern.\n\nlibrary(tidyverse) \nlibrary(r4pde)\ntheme_set(theme_r4pde())\n\npr &lt;- dat |&gt; # R base pipe operator\n  ggplot(aes(x, y))+\n  theme_r4pde(font_size = 12)+\n  geom_point(size =3, \n             color = \"darkred\")+\n  ylim(0,30)+\n  xlim(0,30)+\n  coord_fixed()+\n  labs(x = \"Distance x\", y = \"Distance y\", \n       title = \"Random\")\npr\n\n\n\n\nFigure 14.1: Random pattern of a plant disease epidemic\n\n\n\n\nNow, we can generate new x and y vectors using rnbinom function which allows generating values for the negative binomial distribution (which should give rise to aggregated patterns) with parameters size and prob. Let’s simulate 50 values with mean 12 and size 20 as dispersal parameter.\n\nx &lt;- rnbinom(n = 50, mu = 12, size = 20)\ny &lt;- rnbinom(n = 50, mu = 5, size = 20)\ndat2 &lt;- data.frame(x, y)\n\nThis should give us an aggregated pattern.\n\npag &lt;- dat2 |&gt;\n  ggplot(aes(x, y))+\n  theme_r4pde(font_size = 12)+\n  geom_point(size = 3, color = \"darkred\")+\n  ylim(0,30)+\n  xlim(0,30)+\n  coord_fixed()+\n  labs(x = \"Distance x\", y = \"Distance y\", \n       title = \"Aggregated\")\npag\n\n\n\n\nFigure 14.2: Aggregated pattern of a plant disease epidemic\n\n\n\n\nA rare pattern found in nature is the regular pattern, but it may be generated artificially by the man when conducting experimentation. Follows a code to produce the regular pattern.\n\nx &lt;- rep(c(0,5,10,15,20, 25, 30, 35, 40, 45), 5) \ny &lt;- rep(c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45), each = 10)\ndat3 &lt;- data.frame(x, y)\n\npreg &lt;- dat3 |&gt;\n  ggplot(aes(x, y))+\n  theme_r4pde(font_size = 12)+\n  geom_point(size = 3, color = \"darkred\")+\n  ylim(0,30)+\n  xlim(0,30)+\n  coord_fixed()+\n  labs(x = \"Distance x\", y = \"Distance y\", \n       title = \"Regular\")\npreg\n\nWarning: Removed 51 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 14.3: Regular pattern of a plant disease epidemic\n\n\n\n\n\nlibrary(patchwork)\npreg + pr + pag\nggsave(\"imgs/spatial.png\", width = 10, height = 4)\n\n\n\n\nFigure 14.4: Patterns of a plant disease epidemic"
  },
  {
    "objectID": "spatial-patterns.html#spatiotemporal",
    "href": "spatial-patterns.html#spatiotemporal",
    "title": "14  Spatial patterns",
    "section": "14.2 Spatiotemporal",
    "text": "14.2 Spatiotemporal\nThe location of diseased plants can be assessed over time and so we can appraise both the progress and pattern of the epidemics. Let’s visualize spatial data collected from actual epidemics monitored (plant is diseased or not diseased) during six times during the epidemics. The data is available in the epiphy R package. Let’s use only one variety and one irrigation type.\n\nlibrary(epiphy)\ntswv_1928 &lt;- tomato_tswv$field_1928\n\ntswv_1928 |&gt;\n  filter(variety == \"Burwood-Prize\"&\n         irrigation == \"trenches\") |&gt; \n  ggplot(aes(x, y, fill= factor(i)))+\n  geom_tile(color = \"black\")+\n  coord_fixed()+\n  scale_fill_manual(values = c(\"grey70\", \"darkred\"))+\n  labs(fill = \"Status\", title = \"\")+\n  theme_void()+\n  theme(legend.position = \"bottom\")+\n  facet_wrap(~ t, nrow =1)\n\n\n\n\nFigure 14.5: Spatial patterns of tomato spotted wilt virus at six assessment times\n\n\n\n\nIn this other example, the severity of gummy stem blight (Didymella bryoniae) of watermelon (Café-Filho et al. 2010) was recorded in a 0-4 ordinal scale over time (days after planting) and space, in a naturally-infected rain-fed commercial field, to evaluate the effect of the distance of initial inoculum on the intensity of the disease. The dataset is included in the {r4pde} package that accompanies the book.\n\nlibrary(r4pde)\ndf &lt;- DidymellaWatermelon\n\ndf |&gt; \n  ggplot(aes(NS_col, EW_row, fill = severity))+\n  coord_fixed()+\n  geom_tile (color = \"white\")+\n  theme_void()+\n  theme(legend.position = c(0.9,0.25))+\n scale_fill_gradient(low = \"grey70\", high = \"darkred\")+\n  facet_wrap(~ dap, ncol = 4)\n\n\n\n\nFigure 14.6: Spatial patterns of the severity (0-4 ordinal scale) gummy stem blight of watermelon during seven times after the day of planting (Café-Filho et al. 2010)\n\n\n\n\nWe can see that the disease spread from the initial focus detected at 50 days after planting, taking the entire field 37 days later with various levels of severity."
  },
  {
    "objectID": "spatial-patterns.html#simulating-spatial-patterns",
    "href": "spatial-patterns.html#simulating-spatial-patterns",
    "title": "14  Spatial patterns",
    "section": "14.3 Simulating spatial patterns",
    "text": "14.3 Simulating spatial patterns\nTwo Shiny apps have been developed to allow simulating various spatial disease patterns. The first generates a disease- or pathogen-only data where the units are located in a scatter plot where the user can define the number of cells of the grid as well as the number of points to be plotted and the realized pattern: random or aggregated.\n\n\n\nFigure 14.7: Screenshot of a Shiny app to simulate disease-only data in a grid\n\n\nThe second app generates an artificial plantation with presence-absence data in a 2D map. The user can define the number of rows and number of plants per row and the realized pattern: random or aggregated. The latter pattern can start from the center or border of the plantation. The app calculates the number of foci and the final incidence (proportion of diseased plants).\n\n\n\nFigure 14.8: Screenshot of a Shiny app to simulate a presence-absence data in a 2D map\n\n\n\n\n\n\nCafé-Filho, A. C., Santos, G. R., and Laranjeira, F. F. 2010. Temporal and spatial dynamics of watermelon gummy stem blight epidemics. European Journal of Plant Pathology. 128:473–482 Available at: http://dx.doi.org/10.1007/s10658-010-9674-1.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007. Spatial aspects of epidemicsIII: Patterns of plant disease. In The American Phytopathological Society, p. 235–278. Available at: http://dx.doi.org/10.1094/9780890545058.009."
  },
  {
    "objectID": "spatial-tests.html#intensively-mapped",
    "href": "spatial-tests.html#intensively-mapped",
    "title": "15  Tests for patterns",
    "section": "15.1 Intensively mapped",
    "text": "15.1 Intensively mapped\n\n15.1.1 Binary data\nIn this situation the individual plants are mapped, meaning that their relative positions to one another are known. It is the case when a census is used to map presence/absence data. The status of each unit (usually a plant) is noted as a binary variable. The plant is either diseased (D or 1) or non-diseased or healthy (H or 0). Several statistical tests can be used to detect a deviation from randomness. The most commonly used tests are runs, doublets and join count.\n\n15.1.1.1 Runs test\nA run is defined as a succession of one or more diseased (D) or healthy (H) plants, which are followed and preceded by a plant of the other disease status or no plant at all (Madden 1982). There would be few runs if there is an aggregation of diseased or healthy plants and a large number of runs for a random mixing of diseased and healthy plants.\nLet’s create a vector of binary (0 = non-diseased; 1 = diseased) data representing a crop row with 32 plants and assign it to y. For plotting purposes, we make a data frame for more complete information.\n\nlibrary(tidyverse) \n\n\ny1 &lt;- c(1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,0,0)\nx1 &lt;- c(1:32) # position of each plant\nz1 &lt;- 1\nrow1 &lt;- data.frame(x1, y1, z1) # create a dataframe\n\nWe can then visualize the series using ggplot and count the number of runs as 8, aided by the different colors used to identify a run.\n\nlibrary(grid)\nruns2 &lt;- row1 |&gt;\n  ggplot(aes(x1, z1, label = x1, color = factor(y1))) +\n  geom_point(shape = 15, size = 10) +\n  scale_x_continuous(breaks = max(z1)) +\n  scale_color_manual(values = c(\"gray70\", \"darkred\")) +\n  geom_text(vjust = 0, nudge_y = 0.5) +\n  coord_fixed() +\n  ylim(0, 2.5) +\n  theme_void() +\n   theme(legend.position = \"top\")+\n  labs(color = \"Status\")\nggsave(\"imgs/runs2.png\", runs2, width = 12, height = 2, bg = \"white\")\n\n\n\n\nFigure 15.2: Sequence of diseased (dark red) or non-diseased (gray) units (plants). The numbers represent the position of the unit\n\n\nWe can obtain the number of runs and related statistics using the oruns_test() function of the {r4pde}.\n\nlibrary(r4pde)\noruns_test(row1$y1)\n\nOrdinary Runs Test of Data Sequence:\n -------------------------------------\n Total Number of Runs (U): 8\n Expected Number of Runs (EU): 16.75\n Standard Deviation of Runs (sU): 2.74\n Z-score: -3.20\n P-value: 0.0007\n\n Interpretation:\n Based on the Z-score, the sequence exhibits 'aggregation or clustering'.\n\n\n\n\n15.1.1.2 Join count\nIn a joint count statistics, two adjacent plants may be classified by the type of join that links them: D-D, H-H or H-D. The number of joins of the specified type in the orientation(s) of interest is then counted. The question is whether the observed join-count is large (or small) relative to that expected for a random pattern. The join-count statistics provides a basic measure of spatial autocorrelation. The expected number of join counts can defined under randomness and the corresponding standard errors can be estimated after constants are calculated based on the number of rows and columns of the matrix.\nLet’s use the join_count() function of the {r4pde} package to perform a join count test. The formulations in this function apply only for a rectangular array with no missing values, with the “rook” definition of proximity, and they were all presented in the book The Study of Plant Disease Epidemics, page 261 (Madden et al. 2007a).\nLet’s create a series of binary data from left to right and top to bottom. The data is displayed in Fig. 9.13 in page 260 of the book (Madden et al. 2007a). In the example, there are 5 rows and 5 columns. This will be informed later to compose the matrix which is the data format for analysis.\n\nm1 &lt;- c(1,0,1,1,0,\n       1,1,0,0,0,\n       1,0,1,0,0,\n       1,0,0,1,0,\n       0,1,0,1,1)\nmatrix1 &lt;- matrix(m1, 5, 5, byrow = TRUE)\n\nWe can visualize the two-dimensional array by converting to a raster.\n\n# Convert to raster \nmapS2 &lt;- terra::rast(matrix(matrix1, 5 , 5, byrow = TRUE))\n# Convert to data frame\nmapS3 &lt;- terra::as.data.frame(mapS2, xy = TRUE)\nmapS3 |&gt;\n  ggplot(aes(x, y, label = lyr.1, fill = factor(lyr.1))) +\n  geom_tile(color = \"white\", linewidth = 0.5) +\n  theme_void() +\n  coord_fixed()+\n  labs(fill = \"Status\") +\n  scale_fill_manual(values = c(\"gray70\", \"darkred\"))+\n  theme(legend.position = \"top\")\n\n\n\n\nFigure 15.3: Visualization of a matrix of presence/absence data representing a disease spatial pattern\n\n\n\n\nThe function join_count() calculates spatial statistics for a matrix based on the specifications and calculations shown in the book (Madden et al. 2007a). It identifies patterns of aggregation for values in a binary matrix based on join count statistics. The results determine whether the observed spatial arrangement is aggregated or non-aggregated (random) based on a standard normal distribution test statistic Z-score applied separately for HD or DD sequences. For HD, Z-score lower than - 1.64 (more negative) is taken as a basis for rejection of hypothesis of randomness (P = 0.05). For DD sequences, Z-score greater than 1.64 indicates aggregation.\n\nlibrary(r4pde)\njoin_count(matrix1)\n\nJoin Count Analysis of Spatial Patterns of Plant Diseases:\n ----------------------------------------------------------\n 1) 'HD' Sequences:\n    - Observed Count: 23\n    - Expected Count : 19.97\n    - Standard Deviation: 3.17\n    - Z-score: 0.80\n The pattern for 'HD' sequences is 'not aggregated'.\n\n 2) 'DD' Sequences:\n    - Observed Count: 7\n    - Expected Count: 9.22\n    - Standard Deviation: 4.23\n    - Z-score: -0.41\n The pattern for 'DD' sequences is 'not aggregated'.\n ----------------------------------------------------------\n\n\nIn the example above, the observed count of HD sequence was larger than the expected count, so the Z-score suggests non aggregation, or randomness. The DD sequence observed count was lower than expected count, confirming non aggregation. Let’s repeat the procedure using the second array of data shown in the book chapter, for which the result is different. In this case, there is evidence of aggregation of diseased plants, because the observed DD is greater than expected and observed HD is lower than expected. The Z-score for HD is less than -1.64 (P&lt; 0.05) and the Z-score for DD is greater than 1.64 (P &lt; 0.05).\n\nm2 &lt;- c(1,1,1,0,0,\n       1,1,1,0,0,\n       1,1,1,0,0,\n       1,1,1,0,0,\n       0,0,0,0,0)\nmatrix2 &lt;- matrix(m2, 5, 5, byrow = TRUE)\n\njoin_count(matrix2)\n\nJoin Count Analysis of Spatial Patterns of Plant Diseases:\n ----------------------------------------------------------\n 1) 'HD' Sequences:\n    - Observed Count: 7\n    - Expected Count : 19.97\n    - Standard Deviation: 3.17\n    - Z-score: -4.24\n The pattern for 'HD' sequences is 'aggregated'.\n\n 2) 'DD' Sequences:\n    - Observed Count: 17\n    - Expected Count: 9.22\n    - Standard Deviation: 4.23\n    - Z-score: 1.96\n The pattern for 'DD' sequences is 'aggregated'.\n ----------------------------------------------------------\n\n\nWe can apply these tests for a real example epidemic data provided by the {r4pde} R package. Let’s work with a portion of the intensively mapped data on the occurrence of gummy stem blight (GSB) of watermelon (Café-Filho et al. 2010).\n\nlibrary(r4pde)\ngsb &lt;- DidymellaWatermelon\ngsb \n\n# A tibble: 1,344 × 4\n     dap NS_col EW_row severity\n   &lt;int&gt;  &lt;int&gt;  &lt;int&gt;    &lt;int&gt;\n 1    50      1      1        0\n 2    50      1      2        0\n 3    50      1      3        0\n 4    50      1      4        0\n 5    50      1      5        0\n 6    50      1      6        0\n 7    50      1      7        0\n 8    50      1      8        0\n 9    50      1      9        0\n10    50      1     10        0\n# ℹ 1,334 more rows\n\n\nThe inspection of the data frame shows four variables where dap is the day after planting, NS_col is the north-south direction, EW_row is the east-west direction and severity is the severity in ordinal score (0-4). Let’s produce a map for the 65 and 74 dap, but first we need to create the incidence variable based on severity.\n\ngsb2 &lt;- gsb |&gt;\n  filter(dap %in% c(65, 74)) |&gt; \n  mutate(incidence = case_when(severity &gt; 0 ~ 1,\n                               TRUE ~ 0))\n\ngsb2 |&gt; \n  ggplot(aes(NS_col, EW_row, fill = factor(incidence)))+\n  geom_tile(color = \"white\") +\n  coord_fixed() +\n  scale_fill_manual(values = c(\"gray70\", \"darkred\")) +\n  facet_wrap( ~ dap) +\n  labs(fill = \"Status\")+\n  theme_void()+\n  theme(legend.position = \"top\")\n\n\n\n\nFigure 15.4: Incidence maps for gummy stem blight of watermelon\n\n\n\n\nNow w eshould check the number of rows (y) and columns (x) for further preparing the matrix for the join count statistics. The m65 object should be in the matrix format before running the join count test.\n\nmax(gsb2$NS_col)\n\n[1] 12\n\nmax(gsb2$EW_row)\n\n[1] 16\n\nm65 &lt;- gsb2 |&gt; \n  filter(dap == 65) |&gt; \n  pull(incidence) |&gt; \n  matrix(12, 16, byrow = TRUE)\n\njoin_count(m65)\n\nJoin Count Analysis of Spatial Patterns of Plant Diseases:\n ----------------------------------------------------------\n 1) 'HD' Sequences:\n    - Observed Count: 104\n    - Expected Count : 108.47\n    - Standard Deviation: 13.89\n    - Z-score: -0.36\n The pattern for 'HD' sequences is 'not aggregated'.\n\n 2) 'DD' Sequences:\n    - Observed Count: 15\n    - Expected Count: 12.52\n    - Standard Deviation: 4.76\n    - Z-score: 0.63\n The pattern for 'DD' sequences is 'not aggregated'.\n ----------------------------------------------------------\n\n\nWe can apply the join count test for 74 dap. The result shows that the pattern is now aggregated, differing from 64 dap.\n\n# Pull the binary sequence of time 2\nm74 &lt;- gsb2 |&gt; \n  filter(dap == 74) |&gt; \n  pull(incidence) |&gt; \n  matrix(12, 16, byrow = TRUE)\n\njoin_count(m74)\n\nJoin Count Analysis of Spatial Patterns of Plant Diseases:\n ----------------------------------------------------------\n 1) 'HD' Sequences:\n    - Observed Count: 110\n    - Expected Count : 155.67\n    - Standard Deviation: 11.91\n    - Z-score: -3.88\n The pattern for 'HD' sequences is 'aggregated'.\n\n 2) 'DD' Sequences:\n    - Observed Count: 58\n    - Expected Count: 37.12\n    - Standard Deviation: 8.85\n    - Z-score: 2.42\n The pattern for 'DD' sequences is 'aggregated'.\n ----------------------------------------------------------\n\n\n\n\n15.1.1.3 Foci analysis\nThe Analysis of Foci Structure and Dynamics (AFSD), introduced by (Nelson 1996) and further expanded by (Laranjeira et al. 1998), was used in several studies on citrus diseases in Brazil. In this analysis, the data come from incidence maps where both the diseased and no-diseased trees are mapped in the 2D plane (Jesus Junior and Bassanezi 2004; Laranjeira et al. 2004).\nHere is an example of an incidence map with four foci (adapted from (Laranjeira et al. 1998)). The data is organized in the wide format where the first column x is the index for the row and each column is the position of the plant within the row. The 0 and 1 represent the non-diseased and diseased plant, respectively.\n\nfoci &lt;- tibble::tribble(\n           ~x, ~`1`, ~`2`, ~`3`, ~`4`, ~`5`, ~`6`, ~`7`, ~`8`, ~`9`,\n           1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           2,   1,   1,   1,   0,   0,   0,   0,   1,   0,\n           3,   1,   1,   1,   0,   0,   0,   1,   1,   1,\n           4,   0,   1,   1,   0,   0,   0,   0,   1,   0,\n           5,   0,   1,   1,   0,   0,   0,   0,   0,   0,\n           6,   0,   0,   0,   1,   0,   0,   0,   0,   0,\n           7,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           8,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           9,   0,   0,   0,   0,   0,   1,   0,   1,   0,\n          10,   0,   0,   0,   0,   0,   0,   1,   0,   0,\n          11,   0,   1,   0,   0,   0,   1,   0,   1,   0,\n          12,   0,   0,   0,   0,   0,   0,   0,   0,   0\n          )\n\nSince the data frame is in the wide format, we need to reshape it to the long format using pivot_longer function of the tidyr package before plotting using ggplot2 package.\n\nlibrary(tidyr)\n\nfoci2 &lt;- foci |&gt; \n  pivot_longer(2:10, names_to = \"y\", values_to = \"i\")\nfoci2\n\n# A tibble: 108 × 3\n       x y         i\n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n 1     1 1         0\n 2     1 2         0\n 3     1 3         0\n 4     1 4         0\n 5     1 5         0\n 6     1 6         0\n 7     1 7         0\n 8     1 8         0\n 9     1 9         0\n10     2 1         1\n# ℹ 98 more rows\n\n\nNow we can make the plot.\n\nlibrary(ggplot2)\nfoci2 |&gt; \n  ggplot(aes(x, y, fill = factor(i)))+\n  geom_tile(color = \"black\")+\n  scale_fill_manual(values = c(\"grey70\", \"darkred\"))+\n  theme_void()+\n  coord_fixed()+\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 15.5: Examples of foci of plant diseases - see text for description\n\n\n\n\nIn the above plot, the upper left focus is composed of four diseased plants with a pattern of vertical and horizontal proximity to the central unit (or the Rook’s case). The upper right focus, also with four diseased plants denotes a pattern of longitudinal proximity to the central unit (or the Bishop’s case). The lower left focus is composed of 11 diseased plants with 4 rows and 6 columns occupied by the focus; the shape index of the focus (SIF) is 1.25 and the compactness index of the focus (CIF) is 0.55. The lower right is a single-unit focus.\nIn this analysis, several statistics can be summarized, both at the single focus and averaging across all foci in the area, including:\n\nNumber of foci (NF) and number of single focus (NSF)\nTo compare maps with different number of plants, NF and NSF can be normalized to 1000 plants as NF1000 and NSF1000\nNumber of plants in each focus i (NPFi)\nMaximum number of rows of the focus i (rfi) and maximum number of columns of the focus i (cfi)\nMean shape index of foci (meanSIF = [∑(fri / cfi)]/NF), where SIF values equal to 1.0 indicate isodiametrical foci; values greater than 1.0 indicate foci with greater length in the direction between the planting rows and values less than 1 indicate foci with greater length in the direction of the planting row.\nMean compactness index of foci (meanCIF = [∑(NPFi/rfi*cfi)]/NF), where CIF values close to 1.0 indicate a more compact foci, that is, greater aggregation and proximity among all the plants belonging to the focus\n\nWe can obtain the above-mentioned foci statistics using the AFSD function of the r4pde package. Let’s calculate for the foci2 dataset already loaded, but first we need to check whether all variables are numeric or integer.\n\nstr(foci2) # y was not numeric\n\ntibble [108 × 3] (S3: tbl_df/tbl/data.frame)\n $ x: num [1:108] 1 1 1 1 1 1 1 1 1 2 ...\n $ y: chr [1:108] \"1\" \"2\" \"3\" \"4\" ...\n $ i: num [1:108] 0 0 0 0 0 0 0 0 0 1 ...\n\nfoci2$y &lt;- as.integer(foci2$y) # transform to numeric\n\nlibrary(r4pde)\nresult_foci &lt;- AFSD(foci2)\n\nThe AFSD function returns a list of three data frames. The first is a summary statistics of this analysis, together with the disease incidence (DIS_INC), for the data frame in analysis.\n\nknitr::kable(result_foci[[1]])\n\n\n\n\nstats\nvalue\n\n\n\n\nNF\n4.0000000\n\n\nNF1000\n37.0370370\n\n\nNSF\n1.0000000\n\n\nNSF1000\n9.2592593\n\n\nDIS_INC\n0.2037037\n\n\nmean_SIF\n1.0625000\n\n\nmean_CIF\n0.6652778\n\n\n\n\n\nThe second object in the list is a data frame with statistics at the focus level, including the number of rows and columns occupied by each focus as well as the two indices for each focus: shape and compactness.\n\nknitr::kable(result_foci[[2]])\n\n\n\n\nfocus_id\nsize\nrows\ncols\nSIF\nCIF\n\n\n\n\n1\n11\n4\n5\n0.8\n0.5500000\n\n\n2\n5\n3\n3\n1.0\n0.5555556\n\n\n3\n5\n3\n3\n1.0\n0.5555556\n\n\n4\n1\n1\n1\n1.0\n1.0000000\n\n\n\n\n\nThe third object is the original data frame amended with the id for each focus which can be plotted and labelled (the focus ID) using the plot_AFSD() function.\n\nfoci_data &lt;- result_foci[[3]]\nfoci_data\n\n# A tibble: 22 × 4\n       x     y     i focus_id\n   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     2     1     1        1\n 2     2     2     1        1\n 3     2     3     1        1\n 4     2     8     1        2\n 5     3     1     1        1\n 6     3     2     1        1\n 7     3     3     1        1\n 8     3     7     1        2\n 9     3     8     1        2\n10     3     9     1        2\n# ℹ 12 more rows\n\n\nThe plot shows the ID for each focus.\n\nplot_AFSD(foci_data)+\n  theme_void()\n\n\n\n\nWe will now analyse the gummy stem blight dataset again focusing on 65 and 74 dap. We first need to prepare the data for the analysis by getting the x, y and i vectors in the dataframe.\n\nlibrary(r4pde)\ngsb65 &lt;- DidymellaWatermelon |&gt; \n  filter(dap %in% c(65)) |&gt; \n  mutate(incidence = case_when(severity &gt; 0 ~ 1,\n                               TRUE ~ 0)) |&gt; \n  select(EW_row, NS_col, incidence) |&gt; \n  rename(x = EW_row, y = NS_col, i = incidence)\n\nNow we can run the AFSD function and obtain the statistics.\n\nresult_df1 &lt;- AFSD(gsb65)\n\nknitr::kable(result_df1[[1]])\n\n\n\n\nstats\nvalue\n\n\n\n\nNF\n12.0000000\n\n\nNF1000\n62.5000000\n\n\nNSF\n2.0000000\n\n\nNSF1000\n10.4166667\n\n\nDIS_INC\n0.1875000\n\n\nmean_SIF\n1.0722222\n\n\nmean_CIF\n0.7162037\n\n\n\n\n\nThis analysis is usually applied to multiple maps and the statistics are visually related to the incidence in the area in a scatter plot. Let’s calculate the statistics for the two dap . We can split it by dap before applying the function. We can do it using the map function of the purrr package.\n\nlibrary(purrr)\ngsb65_74 &lt;- DidymellaWatermelon |&gt; \n  filter(dap %in% c(65, 74)) |&gt; \n  mutate(incidence = case_when(severity &gt; 0 ~ 1,\n                               TRUE ~ 0)) |&gt; \n  select(dap, EW_row, NS_col, incidence) |&gt; \n  rename(dap = dap, x = EW_row, y = NS_col, i = incidence)\n\n# Split the dataframe by 'time'\ndf_split &lt;- split(gsb65_74, gsb65_74$dap)\n\n# Apply the AFSD function to each split dataframe\nresults &lt;- map(df_split, AFSD)\n\nWe can check the summary results for time 2 and time 3.\n\ntime65 &lt;- data.frame(results[[1]][1])\nknitr::kable(time65)\n\n\n\n\nstats\nvalue\n\n\n\n\nNF\n12.0000000\n\n\nNF1000\n62.5000000\n\n\nNSF\n2.0000000\n\n\nNSF1000\n10.4166667\n\n\nDIS_INC\n0.1875000\n\n\nmean_SIF\n1.0722222\n\n\nmean_CIF\n0.7162037\n\n\n\n\ntime74 &lt;- data.frame(results[[2]][1])\nknitr::kable(time74)\n\n\n\n\nstats\nvalue\n\n\n\n\nNF\n10.0000000\n\n\nNF1000\n52.0833333\n\n\nNSF\n2.0000000\n\n\nNSF1000\n10.4166667\n\n\nDIS_INC\n0.3229167\n\n\nmean_SIF\n1.1338095\n\n\nmean_CIF\n0.7525022\n\n\n\n\n# Plot the results to see the two foci in time 1\nplot_AFSD(results[[1]][[3]])+\n  theme_void()+\n  coord_fixed()\n\n\n\n# Plot time 2\nplot_AFSD(results[[2]][[3]])+\n  theme_void()+\n  coord_fixed()\n\n\n\n\n\n\n\n15.1.2 Point pattern analysis\nPoint pattern analysis involves the study of the spatial arrangement of points in a two-dimensional space. In its simplest form, one can visualize this as a scatter plot on a map, where each point represents an event, object, or entity in space. For example, the points might represent the locations of diseased plants in a population.\nThe easiest way to visualize a 2-D point pattern is to produce a map of the locations, which is simply a scatter plot but with the provision that the axes are equally scaled. However, while the visualization can provide a basic understanding of the spatial distribution, the real power of point pattern analysis lies in the quantitative methods that allow one to analyze the distribution in a more detailed and systematic way. These methods help to identify whether the points are randomly distributed, clustered (points are closer together than expected by chance), or regularly spaced (points are more evenly spaced than expected by chance). This analysis can provide insights into underlying processes that might explain the observed patterns.\nLet’s work with two simulated datasets that were originally generated to produced a random or an aggregated (clustered) pattern.\n\nlibrary(r4pde)\nrand &lt;- SpatialRandom\naggr &lt;- SpatialAggregated\n\nLet’s produce 2-D map for each data frame.\n\nprand &lt;- rand |&gt; \n  ggplot(aes(x, y))+\n  geom_point()+\n  scale_color_manual(values = c(\"black\", NA))+\n  coord_fixed()+\n  coord_flip()+\n   theme_r4pde(font_size = 12)+\n  theme(legend.position = \"none\")+\n  labs (title = \"Random\", \n        x = \"Latitude\",\n        y = \"Longitude\",\n        caption = \"Source: r4pd R package\")\n\npaggr &lt;- aggr |&gt; \n  ggplot(aes(x, y))+\n  geom_point()+\n  scale_color_manual(values = c(\"black\", NA))+\n  coord_fixed()+\n  coord_flip()+\n  theme_r4pde(font_size = 12)+\n  theme(legend.position = \"none\")+\n  labs (title = \"Aggregated\", \n        x = \"Latitude\",\n        y = \"Longitude\",\n        caption = \"Source: r4pd R package\")\n\nlibrary(patchwork)\nprand | paggr\n\n\n\n\n\n15.1.2.1 Quadrat count\nIn the Quadrat Count method, the study region is divided into a regular grid of smaller, equally-sized rectangular or square subregions known as quadrats. For each quadrat, the number of points falling inside it is counted. If the points are uniformly and independently distributed across the region (i.e., random), then the number of points in each quadrat should follow a Poisson distribution. If the variance of the counts is roughly equal to the mean of the counts, then the pattern is considered to be random. If the variance is greater than the mean, it suggests that the pattern is aggregated or clumped.\nUsing the {spatstats} package, we first need to create a ppp object which represents a point pattern. This is the primary object type in spatstat for point patterns.\n\nlibrary(spatstat)\n\n### Create rectangular window around the points\nwindow_rand &lt;- bounding.box.xy(rand$x, rand$y)\n\n# create the point pattern object\nppp_rand &lt;- ppp(rand$x, rand$y, window_rand)\nplot(ppp_rand)\n\n\n\n\nUsing the quadratcount function, we can divide the study region into a grid and count the number of points in each cell:\n\n## Quadrat count 8 x 8\nqq &lt;- quadratcount(ppp_rand,8,8, keepempty = TRUE) \n\n# plot the quadrat count\nplot(qq)\n\n\n\n\nTo determine whether the observed distribution of points is consistent with a random Poisson process, we can use thequadrat.test function:\n\n# Quadrat test\nqt &lt;- quadrat.test(qq, alternative=\"clustered\", method=\"M\")\nqt\n\n\n    Conditional Monte Carlo test of CSR using quadrat counts\n    Test statistic: Pearson X2 statistic\n\ndata:  \nX2 = 47.04, p-value = 0.9345\nalternative hypothesis: clustered\n\nQuadrats: 8 by 8 grid of tiles\n\n\nThe result will give a Chi-squared statistic and a p-value. If the p-value is very low, then the pattern is likely not random. Keep in mind that the choice of the number and size of quadrats can affect the results. It’s often helpful to try a few different configurations to ensure robust conclusions.\nLets repeat this procedure for the situation of aggregated data.\n\n### Create a rectangular window around the points \nwindow_aggr &lt;- bounding.box.xy(aggr$x, aggr$y)\n\n# create the point pattern object\nppp_aggr &lt;- ppp(aggr$x, aggr$y, window_aggr)\nplot(ppp_aggr)\n\n\n\n## Quadrat count 8 x 8\nqq_aggr &lt;- quadratcount(ppp_aggr,8,8, keepempty=TRUE) \n\n# plot the quadrat count\nplot(qq_aggr)\n\n\n\n# Quadrat test\nqt_aggr &lt;- quadrat.test(qq_aggr, alternative=\"clustered\", method=\"M\")\nqt_aggr\n\n\n    Conditional Monte Carlo test of CSR using quadrat counts\n    Test statistic: Pearson X2 statistic\n\ndata:  \nX2 = 244.92, p-value = 5e-04\nalternative hypothesis: clustered\n\nQuadrats: 8 by 8 grid of tiles\n\n\n\n\n15.1.2.2 Spatial KS test\nThe Spatial Kolmogorov-Smirnov (KS) Test is a method to assess the goodness-of-fit of a given point pattern to the assumptions of Complete Spatial Randomness (CSR) Baddeley et al. (2005). Essentially, this means that it helps in determining whether a set of spatial points is distributed randomly or if there is some underlying pattern or interaction.\nHowever, unlike other goodness-of-fit tests in the spatial context, the Spatial KS Test leverages the values of a spatial covariate at the observed data points and contrasts them against the expected distribution of the same covariate under the assumption of CSR. The idea behind this test is to check if there’s any difference between the observed distribution of a spatial covariate’s values and the expected distribution if the points were distributed in a completely spatial random fashion.\nKey points of the test are:\n\nCovariate: For this test, a spatial covariate must be chosen. This is a spatially varying feature or value that might influence the point process. Examples include elevation, soil quality, or distance from a specific feature. The distribution of this covariate’s values at the observed data points is the crux of the test.\nComparison with CSR: The observed distribution of the spatial covariate’s values at the data points is juxtaposed with what would be expected under the CSR model. The CSR model posits that points are distributed purely by chance, without any underlying structure or influence.\nMethodology: The test employs a classical goodness-of-fit approach to contrast the observed and expected distributions. Specifically, it utilizes the Kolmogorov-Smirnov statistic, a non-parametric measure to gauge the similarity between two distributions.\n\nAs example, we may want to select spatial coordinates themselves (like x, y, or both) as the covariates. This means the test will assess how the observed distribution of x-coordinates (or y-coordinates or both) of the data points compares to what would be anticipated under CSR.\nLet’s test for the aggregated data.\n\n# y as covariate\nks_y &lt;- cdf.test(ppp_aggr, test=\"ks\", \"y\", jitter=FALSE)\nks_y\n\n\n    Spatial Kolmogorov-Smirnov test of CSR in two dimensions\n\ndata:  covariate 'y' evaluated at points of 'ppp_aggr' \n     and transformed to uniform distribution under CSR\nD = 0.070995, p-value = 0.2453\nalternative hypothesis: two-sided\n\nplot(ks_y)\n\n\n\n# x as covariate\nks_x &lt;- cdf.test(ppp_aggr, test=\"ks\", \"x\", jitter=FALSE)\nks_x\n\n\n    Spatial Kolmogorov-Smirnov test of CSR in two dimensions\n\ndata:  covariate 'x' evaluated at points of 'ppp_aggr' \n     and transformed to uniform distribution under CSR\nD = 0.093863, p-value = 0.0512\nalternative hypothesis: two-sided\n\nplot(ks_x)\n\n\n\n# x and y as covariates\nfun &lt;- function(x,y){2* x + y}\nks_xy &lt;- cdf.test(ppp_aggr, test=\"ks\", fun, jitter=FALSE)\nks_xy\n\n\n    Spatial Kolmogorov-Smirnov test of CSR in two dimensions\n\ndata:  covariate 'fun' evaluated at points of 'ppp_aggr' \n     and transformed to uniform distribution under CSR\nD = 0.12687, p-value = 0.002471\nalternative hypothesis: two-sided\n\nplot(ks_xy)\n\n\n\n\nAs shown above, we have sufficient evidence to reject the null hypothesis of complete spatial randomness.\n\n\n15.1.2.3 Distance based\n\n15.1.2.3.1 Ripley’s K\nA spatial point process is a set of irregularly distributed locations within a defined region which are assumed to have been generated by some form of stochastic mechanism. The K function, a.k.a. Ripley’s K-function, is a statistical measure used in spatial analysis to examine the spatial distribution of a single type of point in a given area. Named after its developer, the British statistician B.D. Ripley, the K-function measures the expected number of points within a given distance of an arbitrary point, assuming homogeneous intensity (a constant probability of a point occurring in a particular place).\nTo describe it simply: imagine you have a map of diseased trees in a forest, and you select a tree at random. The K-function helps you answer the question: “How many other diseased trees do I expect to find within a certain distance from the diseased tree I’ve chosen?”\nThe K-function is often used to identify and analyze patterns within spatial data, such as clustering, randomness, or regularity (dispersion). It is particularly useful because it looks at the distribution at all scales (distances) simultaneously. To interpret the results of Ripley’s K-function:\n\nRandom distribution: If the points (like trees in our example) are randomly distributed, the plot of the K-function will be a straight line at a 45-degree angle.\nClustered distribution: If the points are clustered (grouped closer together than you’d expect by chance), the plot will be above the 45-degree line of the random expectation.\nRegular or dispersed distribution: If the points are regularly spaced or dispersed (further apart than you’d expect by chance), the plot will be below the 45-degree line.\n\nRipley’s K checks the density of diseased units in each area by the variance as a function of radial distances (r) from the diseased unit, hence K(r). If the spatial localization of a diseased unit is independent, the process is random in space.\nLet’s use the Kest function of the spatstat package to obtain K(r).\n\nk_rand &lt;- Kest(ppp_rand)\nplot(k_rand)\n\n\n\nk_aggr &lt;- Kest(ppp_aggr)\nplot(k_aggr)\n\n\n\n\nThe envelope function performs simulations and computes envelopes of a summary statistic based on the simulations. The envelope can be used to assess the goodness-of-fit of a point process model to point pattern data (Baddeley et al. 2014). Let’s simulate the envelope and plot the values using ggplot. Because observed K(r) (solid line) lied outside the simulation envelope, aggregation was detected.\n\nke &lt;- envelope(ppp_aggr, fun = Kest)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\ndata.frame(ke) |&gt; \n  ggplot(aes(r, theo))+\n  geom_line(linetype =2)+\n  geom_line(aes(r, obs))+\n  geom_ribbon(aes(ymin = lo, ymax = hi),\n              fill = \"steelblue\", alpha = 0.5)+\n  labs(y = \"K(r)\", x = \"r\")+\n  theme_r4pde(font_size = 16)\n\n\n\n\nmad.test performs the ‘global’ or ‘Maximum Absolute Deviation’ test described by Ripley (1977, 1981). See (Baddeley et al. 2014). This performs hypothesis tests for goodness-of-fit of a point pattern data set to a point process model, based on Monte Carlo simulation from the model.\n\n# Maximum absolute deviation test\nmad.test(ppp_aggr, Kest)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\n    Maximum absolute deviation test of CSR\n    Monte Carlo test based on 99 simulations\n    Summary function: K(r)\n    Reference function: theoretical\n    Alternative: two.sided\n    Interval of distance values: [0, 49.477517555]\n    Test statistic: Maximum absolute deviation\n    Deviation = observed minus theoretical\n\ndata:  ppp_aggr\nmad = 2234.7, rank = 1, p-value = 0.01\n\nmad.test(ppp_rand, Kest)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\n    Maximum absolute deviation test of CSR\n    Monte Carlo test based on 99 simulations\n    Summary function: K(r)\n    Reference function: theoretical\n    Alternative: two.sided\n    Interval of distance values: [0, 44.352774865]\n    Test statistic: Maximum absolute deviation\n    Deviation = observed minus theoretical\n\ndata:  ppp_rand\nmad = 106.7, rank = 90, p-value = 0.9\n\n\n\n\n15.1.2.3.2 O-ring statistics\nAnother statistics that can be used is the O-ring statitics which are used in spatial analysis to quantify and test the degree of interaction between two types of spatial points (Wiegand and A. Moloney 2004). The name derives from the method of placing a series of concentric circles (O-rings) around each point of type 1 and counting how many points of type 2 fall within each ring. The plot generated by O-ring statistics is called an O-ring plot or an O-function plot. It plots the radius of the rings on the x-axis and the estimated intensity of points of type 2 around points of type 1 on the y-axis.\nInterpreting the plot is as follows:\n\nRandom pattern: If points of type 2 are randomly distributed around points of type 1, the O-ring plot will be a flat line. This means that the intensity of points of type 2 does not change with the distance to points of type 1.\nAggregation or clustering: If points of type 2 are aggregated around points of type 1, the O-ring plot will be an upward-sloping curve. This indicates that the intensity of points of type 2 increases with proximity to points of type 1.\nDispersion: If points of type 2 are dispersed away from points of type 1, the O-ring plot will be a downward-sloping curve. This shows that the intensity of points of type 2 decreases as you get closer to points of type 1.\n\nThe O-ring plot often includes a confidence envelope. If the O-ring statistic falls within this envelope, it suggests that the observed pattern could be the result of random spatial processes. If it falls outside the envelope, it suggests that the pattern is not random. Therefore, to decide whether a pattern is aggregated or random using O-ring statistics:\n\nLook at the shape of the O-ring plot.\nCompare the O-ring statistic to the confidence envelope.\n\nAn aggregated pattern will show an increasing curve that lies outside the confidence envelope, indicating that the density of type 2 points is higher close to type 1 points. On the other hand, a random pattern will show a flat line that lies within the confidence envelope, indicating no significant difference in the density of type 2 points around type 1 points at varying distances.\nIn R, we can use the estimate_o_ring() function of the onpoint package. We will use the point pattern object ppp_fw used in the previous examples\n\nlibrary(onpoint)\nplot(estimate_o_ring(ppp_rand))\n\n\n\nplot(estimate_o_ring(ppp_aggr))\n\n\n\n\nThe function can be used in combination with spatstat’s envelope() function.\n\noring_envelope &lt;- envelope(ppp_aggr, fun = estimate_o_ring, nsim = 199, verbose = FALSE)\nplot(oring_envelope)\n\n\n\n\nTo plot simulation envelopes using quantum plots (Esser et al. 2014), just pass an envelope object as input to plot_quantums().\n\nplot_quantums(oring_envelope, ylab = \"O-ring\")+\n  theme_r4pde()\n\n\n\n\n\n\n\n\n15.1.3 Grouped data\nIf the data are intensively mapped, meaning that the spatial locations of the sampling units are known, we are not limited to analyse presence/absence (incidence) only data at the unit level. The sampling units may be quadrats where the total number of plants and the number of disease plants (or number of pathogen propagules) are known. Alternatively, it could be a continuous measure of severity. The question here, similar to the previous section, is whether a plant being diseased makes it more (or less) likely that neighboring plants will be diseased. If that is the case, diseased plants are exhibiting spatial autocorrelation. The most common methods are:\n\nAutocorrelation (known as Moran’s I)\nSemivariance\nSADIE (an alternative approach to autocorrelation.)\n\n\n15.1.3.1 Autocorrelation\nSpatial autocorrelation analysis provides a quantitative assessment of whether a large value of disease intensity in a sampling unit makes it more (positive autocorrelation) or less (negative auto- correlation) likely that neighboring sampling units tend to have a large value of disease intensity (Madden et al. 2007a).\nWe will illustrate the basic concept by reproducing the example provided in page 264 of the chapter on spatial analysis (Madden et al. 2007a), which was extracted from table 11.3 of Campbell and Madden. L. (1990). The data represent a single transect with the number of Macrophomia phaseolina propagules per 10 g air-dry soil recorded in 16 contiguous quadrats across a field.\n\nmp &lt;- data.frame(\n  i = c(1:16),\n  y = c(41, 60, 81, 22, 8, 20, 28, 2, 0, 2, 2, 8, 0, 43, 61, 50)\n)\nmp\n\n    i  y\n1   1 41\n2   2 60\n3   3 81\n4   4 22\n5   5  8\n6   6 20\n7   7 28\n8   8  2\n9   9  0\n10 10  2\n11 11  2\n12 12  8\n13 13  0\n14 14 43\n15 15 61\n16 16 50\n\n\nWe can produce a plot to visualize the number of propagules across the transect.\n\nmp |&gt;\n  ggplot(aes(i, y)) +\n  geom_col(fill = \"darkred\") +\n  theme_r4pde()+\n  labs(\n    x = \"Relative position within a transect\",\n    y = \"Number of propagules\",\n    caption = \"Source: Campbell and Madden (1990)\"\n  )\n\n\n\n\nFigure 15.6: Number of propagules of Macrophomina phaseolina in the soil at various positions within a transect\n\n\n\n\nTo calculate the autocorrelation coefficient in R, we can use the ac() function of the tseries package.\n\nlibrary(tseries)\nac_mp &lt;- acf(mp$y, lag = 5, pl = FALSE)\nac_mp\n\n\nAutocorrelations of series 'mp$y', by lag\n\n     0      1      2      3      4      5 \n 1.000  0.586  0.126 -0.033 -0.017 -0.181 \n\n\nLet’s store the results in a data frame to facilitate visualization.\n\nac_mp_dat &lt;- data.frame(index = ac_mp$lag, ac_mp$acf)\nac_mp_dat\n\n  index   ac_mp.acf\n1     0  1.00000000\n2     1  0.58579374\n3     2  0.12636306\n4     3 -0.03307249\n5     4 -0.01701392\n6     5 -0.18092810\n\n\nAnd now the plot known as autocorrelogram.\n\nac_mp_dat |&gt;\n  ggplot(aes(index, ac_mp.acf, label = round(ac_mp.acf, 3))) +\n  geom_col(fill = \"darkred\") +\n  theme_r4pde()+\n  geom_text(vjust = 0, nudge_y = 0.05) +\n  scale_x_continuous(n.breaks = 6) +\n  geom_hline(yintercept = 0) +\n  labs(x = \"Distance lag\", y = \"Autocorrelation coefficient\")\n\n\n\n\nFigure 15.7: Autocorrelogram for the spatial distribution of Macrophomina phaseolina in soil\n\n\n\n\nThe values we obtained here are not the same but quite close to the values reported in Madden et al. (2007b). For the transect data, the calculated coefficients in the book example for lags 1, 2 and 3 are 0.625, 0.144, and - 0.041. The conclusion is the same, the smaller the distance between sampling units, the stronger is the correlation between the count values.\n\n15.1.3.1.1 Moran’s I\nThe method above is usually referred to Moran’s I (Moran 1950), a widely-used statistic to measure spatial autocorrelation in spatial datasets. The presence of spatial autocorrelation implies that nearby locations exhibit more similar (positive autocorrelation) or dissimilar (negative autocorrelation) values than would be expected under random arrangements.\nLet’s use another example dataset from the book to calculate the Moran’s I in R. The data is shown in page 269 of the book. The data represent the number of diseased plants per quadrat (out of a total of 100 plants in each) in 144 quadrats. It was based on an epidemic generated using the stochastic simulator of Xu and Madden (2004). The data is stored in a CSV file.\n\nepi &lt;- read_csv(\"https://raw.githubusercontent.com/emdelponte/epidemiology-R/main/data/xu-madden-simulated.csv\")\nepi1 &lt;- epi |&gt;\n  pivot_longer(2:13,\n               names_to = \"y\",\n               values_to = \"n\") |&gt;\n  pull(n)\n\nThe {spdep} package in R provides a suite of functions for handling spatial dependence in data, and one of its main functions for assessing spatial autocorrelation is moran(). The moran() function calculates Moran’s I statistic for a given dataset. It requires two primary inputs: 1) A Numeric Vector: This vector represents the values for which spatial autocorrelation is to be measured. Typically, this could be a variable like population density, temperature, or any other spatial attribute; and 2) A Spatial Weights Matrix: This matrix represents the spatial relationships between the observations. It can be thought of as defining the “neighbors” for each observation. The {spdep} package provides functions to create various types of spatial weights, such as contiguity-based weights or distance-based weights. Let’s load the library and start with the procedures.\n\n\nset.seed(100)\nlibrary(spdep)\n\nThe cell2nb() function creates the neighbor list with 12 rows and 12 columns, which is how the 144 quadrats are arranged.\n\nnb &lt;- cell2nb(12, 12, type = \"queen\", torus = FALSE)\n\nThe nb2listw() function supplements a neighbors list with spatial weights for the chosen coding scheme. We use the default W, which is the row standardized (sums over all links to n). We then create the col.W neighbor list.\n\ncol.W &lt;- nb2listw(nb, style = \"W\")\n\nThe Moran’s I statistic is given by the moran() function\n\nmoran(x = epi1, # numeric vector\n      listw = col.W, # the nb list\n      n = 12, # number of zones\n      S0 = Szero(col.W)) # global sum of weights\n\n$I\n[1] 0.05818595\n\n$K\n[1] 2.878088\n\n\nThe $I is Moran’s I and $K output is the sample kurtosis of x, or a measure of the “tailedness” of the probability distribution of a real-valued random variable.\nThe interpretations for Moran’s I is as follows: A positive Moran’s I indicates positive spatial autocorrelation. Nearby locations tend to have similar values, while a Negative Moran’s I suggests negative spatial autocorrelation. Neighboring locations have dissimilar values. Moran’s close to zero indicates no spatial autocorrelation. The distribution appears random.\nIn the context of Moran’s I and spatial statistics, kurtosis of the data (x) can provide additional insights. For instance, if the data is leptokurtic ($K &gt; 3), it might suggest that there are some hotspots or cold spots (clusters of high or low values) in the spatial dataset. On the other hand, platykurtic ($K &lt; 3) data might indicate a more even spread without pronounced clusters. This information can be useful when interpreting the results of spatial autocorrelation tests and in understanding the underlying spatial structures.\n\n\n15.1.3.1.2 Moran’s test\nThe Moran’s I test is a cornerstone of spatial statistics, used to detect spatial autocorrelation in data. In the realm of the spdep package in R, the moran.test() function is employed to perform this test. The Moran’s test for spatial autocorrelation uses spatial weights matrix in weights list form. The key inputs are:\n\nx: This is a numeric vector containing the values we wish to test for spatial autocorrelation. It could be anything like population densities, number of diseased unites or severity, in the context of plant disease.\nlistw: This represents the spatial weights, and it’s in list form. The spatial weights matrix is essential for the Moran’s test because it defines the “relationship” between different observations. How we define these relationships can vary: it might be based on distance (e.g., closer observations have higher weights), contiguity (e.g., observations that share a border), or other criteria.\n\n\nmoran.test(x = epi1, \n           listw = col.W)\n\n\n    Moran I test under randomisation\n\ndata:  epi1  \nweights: col.W    \n\nMoran I statistic standard deviate = 15.919, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.698231416      -0.006993007       0.001962596 \n\n\nThe function will return the Moran’s I statistic value, an expected value under the null hypothesis of no spatial autocorrelation, the variance, and a p-value. The p-value can be used to determine the statistical significance of the observed Moran’s I value. The interpretation is as follows:\n\nA significant positive Moran’s I value indicates positive spatial autocorrelation, meaning that similar values cluster together in the spatial dataset.\nA significant negative Moran’s I value suggests negative spatial autocorrelation, implying that dissimilar values are adjacent to one another.\nIf Moran’s I is not statistically significant (based on the p-value), it suggests that the spatial pattern might be random, and there’s no evidence of spatial autocorrelation.\n\nIn conclusion, the moran.test() function offers a structured way to investigate spatial autocorrelation in datasets. Spatial weights play a crucial role in this analysis, representing the spatial relationships between observations.\nAs before, we can construct a correlogram using the output of the sp.correlogram() function. Note that the figure below is very similar to the one shown in Figure 91.5 in page 269 of the book chapter (Madden et al. 2007a). Let’s store the results in a data frame.\n\ncorrel_I &lt;- sp.correlogram(nb, epi1, \n                           order = 10,\n                           method = \"I\",  \n                           zero.policy = TRUE)\n\n\ndf_correl &lt;- data.frame(correl_I$res) |&gt; \n  mutate(lag = c(1:10))\n# Show the spatial autocorrelation for 10 distance lags\nround(df_correl$X1,3)\n\n [1]  0.698  0.340  0.086 -0.002 -0.009 -0.024 -0.090 -0.180 -0.217 -0.124\n\n\nThen, we can generate the plot using ggplot.\n\ndf_correl |&gt;\n  ggplot(aes(lag, X1)) +\n  geom_col(fill = \"darkred\") +\n  theme_r4pde()+\n  scale_x_continuous(n.breaks = 10) +\n  labs(x = \"Distance lag\", y = \"Spatial autocorrelation\")\n\n\n\n\nFigure 15.8: Autocorrelogram for the spatial distribution of simulated epidemics\n\n\n\n\n\n\n\n15.1.3.2 Semivariance\nSemi-variance is a key quantity in geostatistics. This differs from spatial autocorrelation because distances are usually measured in discrete spatial lags. The semi-variance can be defined as half the variance of the differences between all possible points spaced a constant distance apart.\nThe semi-variance at a distance d = 0 will be zero, because there are no differences between points that are compared to themselves. However, as points are compared to increasingly distant points, the semi-variance increases. At some distance, called the Range, the semi-variance will become approximately equal to the variance of the whole surface itself. This is the greatest distance over which the value at a point on the surface is related to the value at another point. In fact, when the distance between two sampling units is small, the sampling units are close together and, usually, variability is low. As the distance increases, so (usually) does the variability.\nResults of semi-variance analysis are normally presented as a graphical plot of semi-variance against distance, which is referred to as a semi-variogram. The main characteristics of the semi-variogram of interest are the nugget, the range and the sill, and their estimations are usually based on an appropriate (non-linear) model fitted to the data points representing the semi-variogram.\nFor the semi-variance, we will use the variog() function of the geoR package. We need the data in the long format (x, y and z). Let’s reshape the data to the long format and store it in epi2 dataframe.\n\nepi2 &lt;- epi |&gt;\n  pivot_longer(2:13,\n               names_to = \"y\",\n               values_to = \"n\") |&gt;\n  mutate(y = as.numeric(y))\n\nhead(epi2)\n\n# A tibble: 6 × 3\n      x     y     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     2\n2     1     2     2\n3     1     3     3\n4     1     4    33\n5     1     5     4\n6     1     6     0\n\n\n\nlibrary(geoR)\n# the coordinates are x and y and the data is the n\nv1 &lt;- variog(coords = epi2[,1:2], data = epi2[,3])\n\nvariog: computing omnidirectional variogram\n\n\n\nv2 &lt;- variofit(v1, ini.cov.pars = c(1200, 12), \n               cov.model = \"exponential\", \n               fix.nugget = F)\n\nvariofit: covariance model used is exponential \nvariofit: weights used: npairs \nvariofit: minimisation function used: optim \n\n# Plotting \nplot(v1, xlim = c(0,15))\nlines(v2, lty = 1, lwd = 2)\n\n\n\n\nFigure 15.9: Semivariance plot for the spatial distribution simulated epidemic\n\n\n\n\n\n\n15.1.3.3 SADIE\nSADIE (spatial analysis by distance indices) is an alternative to autocorrelation and semi-variance methods described previously, which has found use in plant pathology (Madden et al. 2007a; Xu and Madden 2004; Li et al. 2011). Similar to those methods, the spatial coordinates for the disease intensity (count of diseased individuals) or pathogen propagules values should be provided.\nSADIE quantifies spatial pattern by calculating the minimum total distance to regularity. That is, the distance that individuals must be moved from the starting point defined by the observed counts to the end point at which there is the same number of individuals in each sampling unit. Therefore, if the data are highly aggregated, the distance to regularity will be large, but if the data are close to regular to start with, the distance to regularity will be smaller.\nThe null hypothesis to test is that the observed pattern is random. SADIE calculates an index of aggregation (Ia). When this is equal to 1, the pattern is random. If this is greater than 1, the pattern is aggregated. Hypothesis testing is based on the randomization procedure. The null hypothesis of randomness, with an alternative hypothesis of aggregation.\nAn extension was made to quantify the contribution of each sampling unit count to the observed pattern. Regions with large counts are defined as patches and regions with small counts are defined as gaps. For each sampling unit, a clustering index is calculated and can be mapped.\nIn R, we can use the sadie() function of the epiphy package (Gigot 2018). The function computes the different indices and probabilities based on the distance to regularity for the observed spatial pattern and a specified number of random permutations of this pattern. To run the analysis, the dataframe should have only three columns: the first two must be the x and y coordinates and the third one the observations. Let’s continue working with the simulated epidemic dataset named epi2. We can map the original data as follows:\n\nepi2 |&gt;\n  ggplot(aes(x, y, label = n, fill = n)) +\n  geom_tile() +\n  geom_text(size = 5, color = \"white\") +\n  theme_void() +\n  coord_fixed() +\n  scale_fill_gradient(low = \"gray70\", high = \"darkred\")\n\n\n\n\nFigure 15.10: Spatial map for the number of diseased plants per quadrat (n = 144) in simulated epidemic\n\n\n\n\n\nlibrary(epiphy)\nsadie_epi2 &lt;- sadie(epi2)\n\nComputation of Perry's indices:\n\nsadie_epi2\n\nSpatial Analysis by Distance IndicEs (sadie)\n\nCall:\nsadie.data.frame(data = epi2)\n\nIa: 2.4622 (Pa = &lt; 2.22e-16)\n\n\nThe simple output shows the Ia value and associated P-value. As suggested by the low value of the P-value, the pattern is highly aggregated. The summary() function provides a more complete information such as the overall inflow and outflow measures. A data frame with the clustering index for each sampling unit is also provided using the summary() function.\n\nsummary(sadie_epi2)\n\n\nCall:\nsadie.data.frame(data = epi2)\n\nFirst 6 rows of clustering indices:\n  x y  i cost_flows      idx_P idx_LMX prob\n1 1 1  2 -11.382725 -7.2242617      NA   NA\n2 1 2  2  -9.461212 -6.2258877      NA   NA\n3 1 3  3  -7.299482 -5.3390880      NA   NA\n4 1 4 33   1.000000  0.8708407      NA   NA\n5 1 5  4  -5.830952 -3.6534511      NA   NA\n6 1 6  0  -5.301329 -2.9627172      NA   NA\n\nSummary indices:\n                      overall    inflow  outflow\nPerry's index        2.495346 -2.811023 2.393399\nLi-Madden-Xu's index       NA        NA       NA\n\nMain outputs:\nIa: 2.4622 (Pa = &lt; 2.22e-16)\n\n'Total cost': 201.6062\nNumber of permutations: 100\n\n\nThe plot() function allows to map the clustering indices and so to identify regions of patches (red, outflow) and gaps (blue, inflow).\n\nplot(sadie_epi2) \n\n\n\n\nFigure 15.11: Map of the SADIE clustering indices where red identifiy patches (outflow) and blue identify gaps (inflow)\n\n\n\n\nA isocline plot can be obtained by setting the isocline argument as TRUE.\n\nplot(sadie_epi2, isoclines = TRUE)\n\n\n\n\nFigure 15.12: Map of the SADIE clustering indices"
  },
  {
    "objectID": "spatial-tests.html#sparsely-sampled-data",
    "href": "spatial-tests.html#sparsely-sampled-data",
    "title": "15  Tests for patterns",
    "section": "15.2 Sparsely sampled data",
    "text": "15.2 Sparsely sampled data\nSparsely sampled data differs significantly from intensively mapped data. While the latter provides intricate details about the spatial location of each unit, sparsely sampled data lacks this granularity. This absence means that the spatial location of individual units isn’t factored into the analysis of sparsely sampled data.\nIn the realm of analyzing such data, particularly in understanding disease intensity, researchers often aim to characterize the range of variability in the average level of disease intensity for each sampling unit, as indicated by (Madden et al. 2007a). The methodology to analyze sparsely sampled data, especially in relation to the spatial patterns seen in plant disease epidemics, can be broadly categorized into two primary approaches:\n\nGoodness of Fit to Statistical Distributions: This approach focuses on testing how well the observed data matches expected statistical probability distributions. By doing so, researchers can assess the likelihood that the observed data is a good fit for a particular statistical model or distribution.\nIndices of Aggregation Calculation: This involves computing various metrics that can help determine the degree to which data points, or disease incidents in this case, tend to cluster or aggregate in certain areas or patterns.\n\nFurthermore, the choice of analysis method often hinges on the nature of the data in question. For instance, data can either be in the form of raw counts or as incidences (proportions). Depending on this distinction, specific statistical distributions are presumed to best represent and describe the data. Subsequent sections will delve deeper into these methods, distinguishing between count and incidence data to provide clarity on the best analytical approach for each type.\n\n15.2.1 Count data\n\n15.2.1.1 Fit to distributions\nTwo statistical distributions can be adopted as reference for the description of random or aggregated patterns of disease data in the form of counts of infection within sampling units. Take the count of lesions on a leaf, or the count of diseased plants on a quadrat, as an example. If the presence of a lesion/diseased plant does not increase or decrease the chance that other lesions/diseased plants will occur, the Poisson distribution describes the distribution of lesions on the leaf. Otherwise, the negative binomial provides a better description.\nLet’s work with the previous simulation data of 144 quadrats with a variable count of diseased plants per quadrat (in a maximum of 100). Notice that we won’t consider the location of each quadrat as in the previous analyses of intensively mapped data. We only need the vector with the number of infected units per sampling unit.\nThe epiphy package provides a function called fit_two_distr(), which allows fitting these two distribution for count data. In this case, either randomness assumption (Poisson distributions) or aggregation assumption (negative binomial) are made, and then, a goodness-of-fit comparison of both distributions is performed using a log-likelihood ratio test. The function requires a data frame created using the count() function where the number of infection units is designated as i. It won’t work with a single vector of numbers. We create the data frame using:\n\ndata_count &lt;- epi2 |&gt; \n  mutate(i = n) |&gt;  # create i vector\n  epiphy::count()   # create the map object of count class\n\nWe can now run the function that will look fo the the vector i. The function returns a list of four components including the outputs of the fitting process for both distribution and the result of the log-likelihood ratio test, the llr.\n\nfit_data_count &lt;- fit_two_distr(data_count)\nsummary(fit_data_count)\n\nFitting of two distributions by maximum likelihood\nfor 'count' data.\nParameter estimates:\n\n(1) Poisson (random):\n       Estimate  Std.Err Z value    Pr(&gt;z)    \nlambda 27.85417  0.43981  63.333 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(2) Negative binomial (aggregated):\n       Estimate    Std.Err Z value    Pr(&gt;z)    \nk     0.6327452  0.0707846  8.9390 &lt; 2.2e-16 ***\nmu   27.8541667  2.9510198  9.4388 &lt; 2.2e-16 ***\nprob  0.0222118  0.0033463  6.6378 3.184e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nfit_data_count$llr\n\nLikelihood ratio test\n\n               LogLik Df  Chisq Pr(&gt;Chisq)    \nrandom :     -2654.71                         \naggregated :  -616.51  1 4076.4  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe very low value of the P-value of the LLR test suggest that the negative binomial provides a better fit to the data. The plot() function allows for visualizing the expected random and aggregated frequencies together with the observed frequencies. The number of breaks can be adjusted as indicated.\n\nplot(fit_data_count, breaks = 5) \n\n\n\n\nFigure 15.13: Frequencies of the observed and expected aggregated and random distributions\n\n\n\n\nSee below another way to plot by extracting the frequency data (and pivoting from wide to long format) from the generated list and using ggplot. Clearly, the negative binomial is a better description for the observed count data.\n\ndf &lt;- fit_data_count$freq |&gt;\n  pivot_longer(2:4, names_to = \"pattern\", values_to = \"value\")\n\ndf |&gt;\n  ggplot(aes(category, value, fill = pattern)) +\n  geom_col(position = \"dodge\", width = 2) +\n  scale_fill_manual(values = c(\"gray70\", \"darkred\", \"steelblue\")) +\n  theme_r4pde()+\n  theme(legend.position = \"top\")\n\n\n\n\nFigure 15.14: Frequencies of the observed and expected aggregated and random distributions\n\n\n\n\n\n\n15.2.1.2 Aggregation indices\n\nidx &lt;- agg_index(data_count, method = \"fisher\")\nidx\n\nFisher's index of dispersion:\n(Version for count data)\n34.25\n\nchisq.test(idx)\n\n\n    Chi-squared test for (N - 1)*index following a chi-squared\n    distribution (df = N - 1)\n\ndata:  idx\nX-squared = 4897.2, df = 143, p-value &lt; 2.2e-16\n\nz.test(idx)\n\n\n    One-sample z-test\n\ndata:  idx\nz = 82.085, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n# Lloyd index\n\nidx_lloyd &lt;- agg_index(data_count, method = \"lloyd\")\nidx_lloyd\n\nLloyd's index of patchiness:\n2.194\n\nidx_mori &lt;- agg_index(data_count, method = \"morisita\")\nidx_mori\n\nMorisita's coefficient of dispersion:\n(Version for count data)\n2.186\n\n# Using the vegan package\nlibrary(vegan)\nz &lt;- data_count$data$i\nmor &lt;- dispindmorisita(z)\nmor\n\n      imor     mclu      muni      imst pchisq\n1 2.185591 1.008728 0.9922162 0.5041152      0\n\n\n\n\n15.2.1.3 Power law\nWhen we have a collection of count data sets at the sampling unit scale the Taylor’s power law (TPL) can be used to assess the overall degree of heterogeneity.\n\n\n\n15.2.2 Incidence data\n\n15.2.2.1 Fit to distributions\n\ntas &lt;-\n  read.csv(\n    \"https://www.apsnet.org/edcenter/disimpactmngmnt/topc/EcologyAndEpidemiologyInR/SpatialAnalysis/Documents/tasmania_test_1.txt\",\n    sep = \"\"\n  )\nhead(tas,10)\n\n   quad group_size count\n1     1          6     4\n2     2          6     6\n3     3          6     6\n4     4          6     6\n5     5          6     6\n6     6          6     6\n7     7          6     6\n8     8          6     6\n9     9          6     4\n10   10          6     6\n\n# Create incidence object for epiphy\ndat_tas &lt;- tas |&gt;\n  mutate(n = group_size, i = count) |&gt;\n  epiphy::incidence()\n\n## Fit to two distributions\nfit_tas &lt;- fit_two_distr(dat_tas)\nsummary(fit_tas)\n\nFitting of two distributions by maximum likelihood\nfor 'incidence' data.\nParameter estimates:\n\n(1) Binomial (random):\n     Estimate Std.Err Z value    Pr(&gt;z)    \nprob  0.90860 0.01494  60.819 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(2) Beta-binomial (aggregated):\n      Estimate  Std.Err Z value    Pr(&gt;z)    \nalpha 1.923479 0.869621  2.2119  0.026976 *  \nbeta  0.181337 0.075641  2.3973  0.016514 *  \nprob  0.913847 0.023139 39.4943 &lt; 2.2e-16 ***\nrho   0.322080 0.096414  3.3406  0.000836 ***\ntheta 0.475101 0.209789  2.2647  0.023534 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfit_tas$llr\n\nLikelihood ratio test\n\n              LogLik Df  Chisq Pr(&gt;Chisq)    \nrandom :     -75.061                         \naggregated : -57.430  1 35.263   2.88e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(fit_tas)\n\n\n\n\n\n\n15.2.2.2 Aggregation indices\nglm model\n\nbinom.tas = glm(cbind(count, group_size - count) ~ 1,\n                family = binomial,\n                data = tas)\nsummary(binom.tas)\n\n\nCall:\nglm(formula = cbind(count, group_size - count) ~ 1, family = binomial, \n    data = tas)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-3.447   1.073   1.073   1.073   1.073  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.2967     0.1799   12.77   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 117.76  on 61  degrees of freedom\nResidual deviance: 117.76  on 61  degrees of freedom\nAIC: 152.12\n\nNumber of Fisher Scoring iterations: 5\n\nlibrary(performance)\ncheck_overdispersion(binom.tas)\n\n# Overdispersion test\n\n       dispersion ratio =   2.348\n  Pearson's Chi-Squared = 143.206\n                p-value = &lt; 0.001\n\n\nepiphy(c-alpha test)\n\nlibrary(epiphy)\ntas2 &lt;- tas |&gt;\n  mutate(i = count,\n         n = group_size) |&gt;  # create i vector\n  epiphy::incidence()\n\nt &lt;- agg_index(tas2, flavor = \"incidence\")\nt\n\nFisher's index of dispersion:\n(Version for incidence data)\n2.348\n\n\n\ncalpha.test(t)\n\n\n    C(alpha) test\n\ndata:  t\nz = 7.9886, p-value = 1.365e-15\n\n\n\n\n15.2.2.3 Binary power law\nWhen dealing with incidence data sets at the sampling unit scale, one often aims to discern patterns or tendencies in the data, especially with regards to the distribution of diseased individuals. The binary form of the power law is a statistical method used to assess the overall degree of heterogeneity in such data sets. In essence, this method compares the observed variance in the number of diseased individuals in the data set to the variance one would expect under a random distribution.\nFor incidence data, which deals with proportions, the expected random distribution is typically the Binomial distribution. The Binomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of binary experiments. In the context of disease incidence, a “success” can be interpreted as the occurrence of a disease in a given sampling unit.\nBy comparing the observed variance to the variance under the Binomial distribution, researchers can determine if the disease is spreading in a manner that is consistent with random chance, or if there are significant deviations that might suggest other underlying factors at play.\n\n\n\n\nBaddeley, A., Diggle, P. J., Hardegen, A., Lawrence, T., Milne, R. K., and Nair, G. 2014. On tests of spatial pattern based on simulation envelopes. Ecological Monographs. 84:477–489 Available at: http://dx.doi.org/10.1890/13-2042.1.\n\n\nBaddeley, A., Turner, R., Moller, J., and Hazelton, M. 2005. Residual analysis for spatial point processes (with discussion). Journal of the Royal Statistical Society: Series B (Statistical Methodology). 67:617–666 Available at: http://dx.doi.org/10.1111/j.1467-9868.2005.00519.x.\n\n\nCafé-Filho, A. C., Santos, G. R., and Laranjeira, F. F. 2010. Temporal and spatial dynamics of watermelon gummy stem blight epidemics. European Journal of Plant Pathology. 128:473–482 Available at: http://dx.doi.org/10.1007/s10658-010-9674-1.\n\n\nCampbell, C. L., and Madden. L., V. 1990. Introduction to plant disease epidemiology. Wiley.\n\n\nEsser, D. S., Leveau, J. H. J., Meyer, K. M., and Wiegand, K. 2014. Spatial scales of interactions among bacteria and between bacteria and the leaf surface. FEMS Microbiology Ecology. 91 Available at: http://dx.doi.org/10.1093/femsec/fiu034.\n\n\nGigot, C. 2018. Epiphy: Analysis of plant disease epidemics.\n\n\nJesus Junior, W. C. de, and Bassanezi, R. B. 2004. Análise da dinâmica e estrutura de focos da morte súbita dos citros. Fitopatologia Brasileira. 29:399–405 Available at: http://dx.doi.org/10.1590/S0100-41582004000400007.\n\n\nLaranjeira, F. F., Bergamin Filho, A. R., and Amorim, L. I. 1998. Dinâmica e estrutura de focos da clorose variegada dos citros (CVC). Fitopatologia Brasileira. 23:36–41.\n\n\nLaranjeira, F. F., Bergamin Filho, A., Amorim, L., and Gottwald, T. R. 2004. Dinâmica espacial da clorose variegada dos citros em três regiões do estado de são paulo. Fitopatologia Brasileira. 29:56–65 Available at: http://dx.doi.org/10.1590/S0100-41582004000100009.\n\n\nLi, B., Madden, L. V., and Xu, X. 2011. Spatial analysis by distance indices: an alternative local clustering index for studying spatial patterns. Methods in Ecology and Evolution. 3:368–377 Available at: http://dx.doi.org/10.1111/j.2041-210x.2011.00165.x.\n\n\nMadden, L. V. 1982. Evaluation of tests for randomness of infected plants. Phytopathology. 72:195 Available at: http://dx.doi.org/10.1094/phyto-72-195.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007a. Spatial aspects of epidemicsIII: Patterns of plant disease. In The American Phytopathological Society, p. 235–278. Available at: http://dx.doi.org/10.1094/9780890545058.009.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007b. The study of plant disease epidemics. The American Phytopathological Society. Available at: http://dx.doi.org/10.1094/9780890545058.\n\n\nMoran, P. A. P. 1950. Notes on continuous stochastic phenomena. Biometrika. 37:17.\n\n\nNelson, S. C. 1996. A simple analysis of disease foci. Phytopathology. 86:432–439.\n\n\nWiegand, T., and A. Moloney, K. 2004. Rings, circles, and null-models for point pattern analysis in ecology. Oikos. 104:209–229 Available at: http://dx.doi.org/10.1111/j.0030-1299.2004.12497.x.\n\n\nXu, X.-M., and Madden, L. V. 2004. Use of SADIE statistics to study spatial dynamics of plant disease epidemics. Plant Pathology. 53:38–49 Available at: http://dx.doi.org/10.1111/j.1365-3059.2004.00949.x."
  },
  {
    "objectID": "yieldloss-concepts.html#introduction",
    "href": "yieldloss-concepts.html#introduction",
    "title": "16  Definitions and concepts",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\nPlant disease epidemics significantly impact agricultural production, particularly affecting crop yield - the measurable produce such as seed, fruit, leaves, roots or tubers - and quality, which includes factors such as blemishes on fruit and toxins in grain. Studying these impacts is crucial to understanding the overall repercussions of plant diseases on agriculture.\nThe yield of some crops can be severely diminished if they host a pathogen for a prolonged period of time. The plant’s physiology is dynamically and negatively affected as the crop grows, leading to an increase in biomass and advancement through phenological stages. For some diseases that primarily infect the end product, like grains, yield is directly impacted with a reduction in size and weight of the affected plant part.\nCertain plant diseases cause visual damage to the product, such as fruit or tubers, which may not result in a reduction in yield, but the presence of the symptoms can adversely affect sales due to decreased marketability. Furthermore, the presence of toxins in the product caused by some diseases can significantly downgrade its value, posing both health risks and economic losses.\nLosses due to plant diseases can be categorized as direct, affecting the farm itself, or indirect, having broader impacts on society. Direct losses on the farm due to plant diseases are primarily due to reductions in the quantity and quality of yield, as well as the costs associated with disease control. These are classified as primary losses.\nSecondary losses on the farm are indirect consequences of disease epidemics, such as the buildup of inoculum in the soil, which can lead to subsequent disease outbreaks. Other secondary impacts include the reduced efficacy of disease control methods due to the emergence of resistance to chemicals within the pathogen population over time.\nIn addition to these on-farm losses, plant diseases can have significant indirect impacts on society. These can include increased food prices due to reduced supply, loss of export markets due to trade restrictions, and environmental damage due to increased use of pesticides. Understanding the full spectrum of losses caused by plant diseases is critical for developing effective disease management strategies and policies.\n\n\n\nFigure 16.1: Tipology of losses caused by plant diseases\n\n\nThe famous epidemics in the ancient history, such as the late blight of potatoes, serve us as a remind of worst-case scenarios of major impact of epidemics causing both direct and indirect losses. However, crop losses due to diseases occur regularly and at levels that depend on the intensity of epidemics (Madden et al. 2007). Expert opinion estimates have indicated that around 20% (on average) of the yield of major crops like wheat, rice, maize, potato and soybean is lost due to the pests and pathogens globally (Savary et al. 2019)."
  },
  {
    "objectID": "yieldloss-concepts.html#crop-loss-assessment",
    "href": "yieldloss-concepts.html#crop-loss-assessment",
    "title": "16  Definitions and concepts",
    "section": "16.2 Crop loss assessment",
    "text": "16.2 Crop loss assessment\nAccording to Madden et al. (2007), knowledge about the disease:yield relationship falls within crop loss assessment, a general branch of epidemiology that study the relationship between the attack by harmful organisms and the resulting yield (or yield loss) of crops. In fact, the study (analysis and modeling) of crop losses is considered central to plant pathology as no plant protection scientific reasoning could be possible without a measure of crop loss (Savary et al. 2006).\nThe concept of yield levels is important to recognize as a framework to study crop losses. There are three levels (from higher to lower) of yield: theoretical, attainable and actual.\n\nTheoretical (also known as potential) yield is determined mainly by defining factors such as the genotype of the crop grown under ideal conditions. It can be obtained in experimental plots managed with high input of fertilizers and pesticides.\nAttainable yield is obtained in commercial crops managed with a full range of modern technology to maximize yield. It considers the presence of limiting factors such as water and fertilizers.\nActual yield is generally less than or equal to attainable yield, and is obtained under the effect of reducing factors such as those caused by pest (disease, insects, weeds) injuries - defined as measurable symptom caused by a harmful organism. It is the crop yield actually harvested in a farmer’s field.\n\nYield loss (expressed in absolute or relative terms) is the difference between the attainable and the actual yield. Yield loss studies are only possible when reliable field data are collected in sufficient number to allow the development of statistical (empirical) models as well as the validation of mechanistic simulation yield loss models.\n\n\nCode\nlibrary(tidyverse)\nyl &lt;- tibble::tribble(\n  ~yield, ~value, ~class,\n  \"Theoretical\", 25,1,\n  \"Attainable\", 20,2,\n  \"Actual\", 15,3,\n  \"\", 0, 4\n)\nyl |&gt; \n  ggplot(aes(reorder(yield,-value), value, fill = class))+\n  geom_col(width = 0.5)+\n  r4pde::theme_r4pde(font_size = 14)+\n  ggthemes::scale_fill_gradient_tableau(palette = \"Green\")+\n  geom_hline(yintercept = 25, linetype = 2, color = \"gray60\")+\n  geom_hline(yintercept = 20, linetype = 2, color = \"gray60\")+\n  geom_hline(yintercept = 15, linetype = 2, color = \"gray60\")+\n  theme(legend.position = \"none\", \n        axis.text.y=element_blank(),\n        axis.ticks.x = element_blank())+\n  annotate(geom = \"text\", x = 1.8, y = 22, label =\"Defining factors \n           (genotype and environment)\")+\n  annotate(geom = \"text\", x = 2.8, y = 17.5, label =\"Limiting factors\n        (fertilizers, water)\")+\n  annotate(geom = \"text\", x = 3.8, y = 12, label =\"Reducing factors\n           (pest, weeds, diseases)\")+\n  annotate(\"segment\", x = 4, y = 20, xend = 4, yend = 15,\n         arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\")))+\n  annotate(geom = \"text\", x = 4.3, y = 17, label =\"Yield loss\")+\n  labs(x = \"\", y = \"\")\n\n\n\n\n\nFigure 16.2: Yield levels"
  },
  {
    "objectID": "yieldloss-concepts.html#generating-diseaseyield-data",
    "href": "yieldloss-concepts.html#generating-diseaseyield-data",
    "title": "16  Definitions and concepts",
    "section": "16.3 Generating disease:yield data",
    "text": "16.3 Generating disease:yield data\nThe datasets utilized to characterize a disease-yield relationship should ideally encompass a broad spectrum of yield and disease values. There are primarily two approaches to acquiring such data:\n\nconducting experiments in controlled environments such as fields or greenhouses\nconducting surveys in commercial fields that are naturally infected.\n\nIn experimental setups, researchers rely on different treatments that are designed to result in varying disease epidemics, under the assumption that the disease has an impact on yield. These treatments often include manipulating the level of inoculum when the disease is expected to be minimal. This is achieved through inoculations with different amounts of the pathogen. Conversely, when the disease is expected to be severe, researchers might use fungicides at different rates, frequencies, or timings. An alternative strategy is to use different host genotypes, preferably isolines or near-isolines, which exhibit varying degrees of susceptibility to the disease. Another method is to manipulate the environment, for example by altering the irrigation levels."
  },
  {
    "objectID": "yieldloss-concepts.html#damage-curves",
    "href": "yieldloss-concepts.html#damage-curves",
    "title": "16  Definitions and concepts",
    "section": "16.4 Damage curves",
    "text": "16.4 Damage curves\nIn any case, the relationship between a measure of yield (either absolute or relative) and the disease variable can be evaluated using scatter plots that depict a “damage curve” (Madden et al. 2007). The disease variable most commonly represents the assessment of the disease at a singular critical point. However, sometimes data obtained from multiple assessments throughout the disease epidemic is used to calculate the area under the disease progress curve, which is then used to represent the disease variable. This offers a more comprehensive view of the disease’s impact over time, and can better capture the complex relationships between disease progression and yield loss (Madden et al. 2007).\nLet’s work with actual data on the incidence of white mold disease and yield of soybean determined across different locations and years in Brazil (Lehner et al. 2016). The variation in disease and yield was obtained by applying different fungicides that varied in efficacy, thus resulting in variable final disease incidence. The data was made freely available in this repository and was included the the package that accompanies the book.\n\nlibrary(tidyverse)\nlibrary(r4pde)\nwm &lt;- WhiteMoldSoybean\n\nglimpse(wm, 60)\n\nRows: 382\nColumns: 17\n$ study           &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9…\n$ treat           &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,…\n$ season          &lt;chr&gt; \"2009/2010\", \"2009/2010\", \"2009/20…\n$ harvest_year    &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010…\n$ location        &lt;chr&gt; \"Agua Fria\", \"Agua Fria\", \"Agua Fr…\n$ state           &lt;chr&gt; \"GO\", \"GO\", \"GO\", \"GO\", \"GO\", \"GO\"…\n$ country         &lt;chr&gt; \"Brazil\", \"Brazil\", \"Brazil\", \"Bra…\n$ elevation       &lt;dbl&gt; 891, 891, 891, 891, 891, 891, 891,…\n$ region          &lt;chr&gt; \"Northern\", \"Northern\", \"Northern\"…\n$ elevation_class &lt;chr&gt; \"low\", \"low\", \"low\", \"low\", \"low\",…\n$ inc_check       &lt;dbl&gt; 37.7, 37.7, 37.7, 37.7, 37.7, 37.7…\n$ inc_class       &lt;chr&gt; \"low\", \"low\", \"low\", \"low\", \"low\",…\n$ yld_check       &lt;dbl&gt; 3729, 3729, 3729, 3729, 3729, 3729…\n$ yld_class       &lt;chr&gt; \"high\", \"high\", \"high\", \"high\", \"h…\n$ inc             &lt;dbl&gt; 37.7, 11.6, 33.5, 1.0, 5.6, 1.0, 3…\n$ scl             &lt;dbl&gt; 5092, 6154, 200, 180, 1123, 641, 1…\n$ yld             &lt;dbl&gt; 3729, 3739, 3863, 3904, 4471, 4313…\n\n\nAs seen above, the full data set has 17 variables. Let’s reduce the data set to a few variables of interest (study, inc and yld) and the trials number 1 to 4.\n\nwm2 &lt;- wm |&gt; \n  select(study, inc, yld) |&gt; \n  filter(study %in% c(1,2, 3, 4)) \n\nwm2\n\n# A tibble: 52 × 3\n   study   inc   yld\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1    76  2265\n 2     1    53  2618\n 3     1    42  2554\n 4     1    37  2632\n 5     1    29  2820\n 6     1    42  2799\n 7     1    55  2503\n 8     1    40  2967\n 9     1    26  2965\n10     1    18  3088\n# ℹ 42 more rows\n\n\nWe can now produce the damage curves for each study. As it can be seen, the relationship can be adequately described by a straight line.\n\nwm2 |&gt; \n  ggplot(aes(inc, yld, \n             group = study))+\n  geom_point(size = 2)+\n   theme_r4pde(font_size= 14)+\n  geom_smooth(method = \"lm\", se = F, color = \"black\", fullrange = T)+\n  ylim(1800, 3500)+\n  facet_wrap(~study)+\n  labs(x = \"White mold incidence (%)\",\n       y = \"Soybean yield (kg/ha)\",\n       color = \"Study\")\n\n\n\n\nFigure 16.3: Relationship between soybean yield and incidence of white mold in two experiments\n\n\n\n\nWe can plot the relationships for all studies combined, which will resemble a “spaghetti” plot after adding the individual regression lines.\n\np1 &lt;- wm |&gt; \n  ggplot(aes(inc, yld, group = study))+\n  theme_r4pde(font_size = 14)+\n  geom_point(size = 2, alpha = 0.5)+\n  ylim(0, 5000)+\n  labs(x = \"White mold incidence (%)\",\n       y = \"Soybean yield (kg/ha)\")\n\np2 &lt;- wm |&gt; \n  ggplot(aes(inc, yld, group = study))+\n  theme_r4pde(font_size = 14)+\n  geom_smooth(method = \"lm\", se = F, fullrange = T, color = \"black\")+\n  ylim(0, 5000)+\n  labs(x = \"White mold incidence (%)\",\n       y = \"Soybean yield (kg/ha)\")\n\nlibrary(patchwork)\np1 | p2\n\n\n\n\nFigure 16.4: Relationship between soybean yield and incidence of white mold across trials. Observed (left) and fitted regression lines (right)\n\n\n\n\n\n\n\n\nLehner, M. S., Pethybridge, S. J., Meyer, M. C., and Del Ponte, E. M. 2016. Meta-analytic modelling of the incidenceyield and incidencesclerotial production relationships in soybean white mould epidemics. Plant Pathology. 66:460–468 Available at: http://dx.doi.org/10.1111/ppa.12590.\n\n\nMadden, L. V., Hughes, G., and Bosch, F. van den, eds. 2007. CHAPTER 12: Epidemics and crop yield. In The American Phytopathological Society, p. 353–388. Available at: http://dx.doi.org/10.1094/9780890545058.012.\n\n\nSavary, S., Teng, P. S., Willocquet, L., and Nutter, F. W. 2006. Quantification and Modeling of Crop Losses: A Review of Purposes. Annual Review of Phytopathology. 44:89–112 Available at: http://dx.doi.org/10.1146/annurev.phyto.44.070505.143342.\n\n\nSavary, S., Willocquet, L., Pethybridge, S. J., Esker, P., McRoberts, N., and Nelson, A. 2019. The global burden of pathogens and pests on major food crops. Nature Ecology & Evolution. 3:430–439 Available at: http://dx.doi.org/10.1038/s41559-018-0793-y."
  },
  {
    "objectID": "yieldloss-regression-models.html#introduction",
    "href": "yieldloss-regression-models.html#introduction",
    "title": "17  Statistical models",
    "section": "17.1 Introduction",
    "text": "17.1 Introduction\nWhen we aim to empirically describe the relationship between yield (either in absolute terms represented as [YLD], or in relative terms such as relative yield [RY] or relative yield loss [RL]) and disease intensity (denoted as y), the most straightforward approach is to employ a single metric of disease intensity. This could be in the form of disease incidence, severity, electronic measurements, among others. A linear model, as suggested by (Madden et al. 2007), often proves to be a suitable choice for this purpose. Such models are frequently termed as single-point or critical-point models. The nomenclature stems from the fact that the assessment of the disease occurs at one pivotal moment during the epidemic. This moment is typically chosen because it’s when the disease’s impact correlates with yield outcomes.\nA version of the equation, for the absolute quantity of yield (YLD), is written as:\n\\(YLD = 𝛽_0 - 𝛽_1y\\)\nwhere \\(𝛽_0\\) and \\(𝛽_1\\) are parameters and y is a disease measure. For this particular case, the intercept is a yield property of a given plant genoptye in a given environment when the disease of interest is absent (or the attainable yield). The (negative) slope represents the change (reduction) in yield with change (increase) in disease intensity.\nThe interpretation of the model parameters depends on the specific yield variable being related to disease. If relative yield loss is related to disease, the slope will be positive, because the loss (relative decrease in yield) will increase with the increase in disease intensity assessed at the critical time. Anyway, the differences in the intercept reflect the environmental effect on yield when disease is absent, while varying slopes reflect the environmental effect on yield response to disease (Madden et al. 2007)."
  },
  {
    "objectID": "yieldloss-regression-models.html#example-1-single-study",
    "href": "yieldloss-regression-models.html#example-1-single-study",
    "title": "17  Statistical models",
    "section": "17.2 Example 1: single study",
    "text": "17.2 Example 1: single study\nWe demonstrate the fitting of linear regression models (assuming a straight line relationship) to crop loss data on the effects of white mold on soybean yield (Lehner et al. 2016). This is the same data illustrated in the previous chapter. Let’s load the data and assign them to a data frame named as wm.\n\nlibrary(r4pde)\nwm &lt;- WhiteMoldSoybean\n\nFirst, we will work with data from a single trial (trial 1).\n\nlibrary(tidyverse)\nwm1 &lt;- wm |&gt; \n  dplyr::select(study, inc, yld) |&gt; \n  filter(study %in% c(1)) \nhead(wm1, 13)\n\n# A tibble: 13 × 3\n   study   inc   yld\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1    76  2265\n 2     1    53  2618\n 3     1    42  2554\n 4     1    37  2632\n 5     1    29  2820\n 6     1    42  2799\n 7     1    55  2503\n 8     1    40  2967\n 9     1    26  2965\n10     1    18  3088\n11     1    27  3044\n12     1    28  2925\n13     1    36  2867"
  },
  {
    "objectID": "yieldloss-regression-models.html#linear-regression",
    "href": "yieldloss-regression-models.html#linear-regression",
    "title": "17  Statistical models",
    "section": "17.3 Linear regression",
    "text": "17.3 Linear regression\nAssuming a linear relationship between yld and inc, we can employ a linear regression model for trial 1 using the lm() function.\n\nlm1 &lt;-  lm(yld ~ inc, data = wm1) \njtools::summ(lm1)\n\n\n\n\n\nObservations\n13\n\n\nDependent variable\nyld\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(1,11)\n46.86\n\n\nR²\n0.81\n\n\nAdj. R²\n0.79\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n3329.14\n86.84\n38.33\n0.00\n\n\ninc\n-14.21\n2.08\n-6.85\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nIn the summary output, we can notice that the model explains a statistically significant and substantial proportion of variance. The model’s intercept, corresponding to inc = 0, is at 3329.14. The effect of inc is statistically significant and negative ( beta = -14.21). In another words, 140.21 kg is lost for each 10 percent point increase in incidence, given the attainable yield of 3,329.14 kg."
  },
  {
    "objectID": "yieldloss-regression-models.html#damage-coefficients",
    "href": "yieldloss-regression-models.html#damage-coefficients",
    "title": "17  Statistical models",
    "section": "17.4 Damage coefficients",
    "text": "17.4 Damage coefficients\nDamage curves offer a visual representation of how plant diseases can impact a given crop in terms of yield loss. When we want to normalize these effects to better compare across different systems or conditions, it is useful to express these curves in relative terms rather than absolute ones. To achieve this, we can adjust the derived slope by dividing it by the intercept. This step essentially scales the rate of damage in relation to the baseline or the starting point (when there’s no damage). By subsequently multiplying the result by 100, we convert this value into a percentage. This percentage is termed the “relative damage coefficient”. What makes this coefficient particularly useful is its ability to standardize the measurement of damage, facilitating comparisons across diverse pathosystems.\nTwo plots can be produced, one that shows the effect of the disease on the relative yield and the other on the effect on yield loss (which in this case represents a positive slope). Both representations can be found in the literature. Let’s use the estimated coefficients and produce these two plots.\n\n# Extract the coefficients from model fit\ndc &lt;- (lm1$coefficients[2]/lm1$coefficients[1])*100\ndc \n\n      inc \n-0.426775 \n\n# Plot the relative damage curve\nx = seq(0,100,0.1)\ny = seq(0,100,0.1)\ndat &lt;- data.frame(x,y)\n\np1 &lt;- dat |&gt; \n  ggplot(aes(x,y))+\n  theme_r4pde(font_size = 14)+\n  geom_point(color = \"NA\")+  \n  scale_y_continuous(expand = c(0,0))+\n  scale_x_continuous(expand = c(0,0))+\n  geom_abline(aes(intercept = 100, slope = dc))+\n  labs(x = \"Incidence (%)\", y = \"Yield (%)\")+\n  annotate(geom = \"text\", x = 60, y = 60, label = \"DC = -0.42\")\n\np1 \n\n\n\n# Plot for the relative yield decrease\ndc2 &lt;- (-lm1$coefficients[2]/lm1$coefficients[1])*100\ndc2 \n\n     inc \n0.426775 \n\ndat &lt;- data.frame(x,y)\np2 &lt;- dat |&gt; \n  ggplot(aes(x,y))+\n  theme_r4pde(font_size = 14)+\n  geom_point(color = \"NA\")+  \n  scale_y_continuous(expand = c(0,0))+\n  scale_x_continuous(expand = c(0,0))+\n  geom_abline(aes(intercept = 0, slope = dc2))+\n  labs(x = \"Incidence (%)\", y = \"Yield loss (%)\")+\n  annotate(geom = \"text\", x = 60, y = 60, label = \"DC = 0.42\")\np2"
  },
  {
    "objectID": "yieldloss-regression-models.html#example-2-multiple-studies",
    "href": "yieldloss-regression-models.html#example-2-multiple-studies",
    "title": "17  Statistical models",
    "section": "17.5 Example 2: Multiple studies",
    "text": "17.5 Example 2: Multiple studies\n\n17.5.1 Introduction\nWhen managing data sourced from multiple studies (or experiments), a naive and straightforward approach is to pool all the data and fit a “global” linear regression. This option, while efficient, operates under the assumption that the different studies share common underlying characteristics, which might not always be the case.\nAnother simplistic methodology entails running independent linear regressions for each trial and subsequently averaging the intercepts and slopes. While this provides a glimpse into the general behavior of the data, it potentially sidesteps important variations that exist between individual trials. This variation is crucial, as different trials could have unique environments, treatments, or experimental conditions, all of which can influence results.\nNeglecting these variations can mask important nuances and lead to overgeneralized or inaccurate conclusions. In order to accommodate the inherent variability and structure present in multitrial data, researchers often turn to more refined analytical frameworks. Two such frameworks stand out in their ability to provide a nuanced view.\nThe first is a meta-analytic modelling. This approach synthesizes results across multiple studies to arrive at a more comprehensive understanding Madden and Paul (2011). In the context of linear regressions across multiple trials, the coefficients (both intercepts and slopes) from each trial can be treated as effect sizes Dalla Lana et al. (2015). The standard error of these coefficients, which reflects the precision of the estimate, can then be used to weight each coefficient, ensuring that more reliable estimates have greater influence on the overall mean. This method can also provide insights into heterogeneity across trials, and if required, moderators can be explored to account for this variability Lehner et al. (2016).\nA second approach is to fit a random coefficients (mixed effects) models. This approach allows the intercepts and slopes to vary across different trials, treating them as random effects Madden and Paul (2009). By modeling the coefficients as random effects, we’re assuming they come from a distribution, and we aim to estimate the parameters of this distribution. This structure acknowledges that variability exists between different trials and allows for the sharing of information across trials, thereby improving the estimation.\nBoth methods have their strengths and are appropriate under different circumstances. The choice largely depends on the research question, data structure, and the assumptions one is willing to make.\n\n\n17.5.2 Global regression\nLet’s begin by fitting a “global regression”, but we might want to first inspect the damage curve withe the fit of the global regression model visually.\n\nggplot(wm, aes(inc, yld))+\n      theme_r4pde(font_size = 12)+\n       geom_point(shape = 1)+\n       stat_smooth(method = lm, fullrange=TRUE, se = TRUE, col = \"black\")+\n       ylab(\"Yield (kg/ha)\")+\n       xlab(\"White mold incidence (%)\")\n\n\n\n\nA “global” regression can be performed using:\n\nlibrary(broom)\nfit_global &lt;- wm%&gt;%\n  do(tidy(lm(.$yld ~ .$inc), conf.int=TRUE))\nfit_global\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  3300.       56.5      58.5  5.14e-192   3189.    3411.  \n2 .$inc          -9.26      2.11     -4.39 1.45e-  5    -13.4     -5.12\n\n\nThe global intercept and slope were estimated as 3,299 kg/ha and -9.26 kg/p.p. (percent point), respectively.\n\n\n17.5.3 Individual regressions\nNow we can fit separate regressions for each trial.\n\nggplot(wm, aes(inc, yld, group = study))+\n      theme_r4pde(font_size = 12)+\n       geom_point(shape = 1)+\n       stat_smooth(method = lm, se = F, col = \"black\")+\n       ylab(\"Yield (kg/ha)\")+\n       xlab(\"White mold incidence (%)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n      #facet_wrap(~ study, ncol = 7, scales = \"fixed\") \n\nTo fit separate regression lines for each study and extract the coefficients, we can use the group_by() function along with do() and of the {dplyr} package and tidy() from {broom} package.\nThe data is first grouped by the study column. Then, for each study, a linear regression is fitted with yld as the response variable and inc as the predictor. The tidy function from the broom package is then used to extract the coefficients and confidence intervals for each regression line. The resulting output should give us a tidy dataframe with coefficients for each trial.\n\nfit_all &lt;- wm%&gt;%\n  group_by(study) |&gt; \n  do(broom::tidy(lm(.$yld ~ .$inc), conf.int=TRUE))\nfit_all\n\n# A tibble: 70 × 8\n# Groups:   study [35]\n   study term        estimate std.error statistic  p.value conf.low conf.high\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1     1 (Intercept)  3329.       86.8      38.3  4.60e-13   3138.    3520.  \n 2     1 .$inc         -14.2       2.08     -6.85 2.78e- 5    -18.8     -9.64\n 3     2 (Intercept)  2682.       48.6      55.2  8.55e-15   2575.    2789.  \n 4     2 .$inc          -6.93      1.49     -4.66 6.89e- 4    -10.2     -3.66\n 5     3 (Intercept)  4017.       61.6      65.2  1.37e-15   3882.    4153.  \n 6     3 .$inc         -18.6       1.71    -10.9  3.11e- 7    -22.4    -14.9 \n 7     4 (Intercept)  2814.      151.       18.6  1.15e- 9   2481.    3147.  \n 8     4 .$inc         -43.5      16.8      -2.58 2.56e- 2    -80.5     -6.38\n 9     5 (Intercept)  3317.      234.       14.2  2.07e- 8   2802.    3832.  \n10     5 .$inc         -21.2       5.69     -3.72 3.36e- 3    -33.7     -8.67\n# ℹ 60 more rows\n\n\nNow we can plot the distributions of each coefficient.\n\np3 &lt;- fit_all |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  ggplot(aes(x = estimate))+\n  geom_histogram(bins = 8, color = \"white\", fill = \"gray50\")+\n  theme_r4pde()+\n  labs(x = \"Intercept\", y = \"Frequency\")\n\np4 &lt;- fit_all |&gt; \n  filter(term == \".$inc\") |&gt; \n  ggplot(aes(x = estimate))+\n  geom_histogram(bins = 8, color = \"white\", fill = \"gray50\")+\n  theme_r4pde()+\n  labs(x = \"Slope\", y = \"Frequency\")\n\n\nlibrary(patchwork)\np3 | p4\n\n\n\n\nLet’s summarize the data on the slopes and intercept, using the summary() function.\n\nfit_all |&gt; \n  filter(term == \"(Intercept)\") |&gt;\n  ungroup() |&gt; \n  dplyr::select(estimate) |&gt; \n  summary()\n\n    estimate   \n Min.   :1760  \n 1st Qu.:2863  \n Median :3329  \n Mean   :3482  \n 3rd Qu.:4080  \n Max.   :4923  \n\nfit_all |&gt; \n  filter(term == \".$inc\") |&gt;\n  ungroup() |&gt; \n  dplyr::select(estimate) |&gt; \n  summary()\n\n    estimate      \n Min.   :-43.455  \n 1st Qu.:-27.676  \n Median :-16.926  \n Mean   :-19.529  \n 3rd Qu.:-13.054  \n Max.   :  2.712  \n\n\n\n\n17.5.4 Meta-analysis\nHere the objective is to combine the estimates from multiple studies or trials into a single overall estimate using a random-effects meta-analysis using {metafor} R package Viechtbauer (2010). The goal is to capture both the central tendency (i.e., the overall effect) and the variability (heterogeneity) among those individual estimates. Let’s first prepare data for analysis and then run separate meta-analysis for the intercepts and slopes.\n\n# data preparation\nIntercepts &lt;- fit_all |&gt; \n  filter(term == \"(Intercept)\")\n\nSlopes &lt;-  fit_all |&gt; \n  filter(term == \".$inc\") \n  \n\n# Model for the intercepts\nlibrary(metafor)\nma1 &lt;- rma(yi = estimate, sei = std.error, data = Intercepts)\nsummary(ma1)\n\n\nRandom-Effects Model (k = 35; tau^2 estimator: REML)\n\n   logLik   deviance        AIC        BIC       AICc   \n-274.9958   549.9916   553.9916   557.0444   554.3787   \n\ntau^2 (estimated amount of total heterogeneity): 607939.3750 (SE = 150941.5658)\ntau (square root of estimated tau^2 value):      779.7047\nI^2 (total heterogeneity / total variability):   98.88%\nH^2 (total variability / sampling variability):  89.38\n\nTest for Heterogeneity:\nQ(df = 34) = 3402.9633, p-val &lt; .0001\n\nModel Results:\n\n estimate        se     zval    pval      ci.lb      ci.ub      \n3479.3087  133.3611  26.0894  &lt;.0001  3217.9258  3740.6917  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Model for the slopes\nma2 &lt;- rma(yi = estimate, sei = std.error, data = Slopes)\nsummary(ma2)\n\n\nRandom-Effects Model (k = 35; tau^2 estimator: REML)\n\n   logLik   deviance        AIC        BIC       AICc   \n-127.4587   254.9174   258.9174   261.9701   259.3045   \n\ntau^2 (estimated amount of total heterogeneity): 65.0917 (SE = 22.6013)\ntau (square root of estimated tau^2 value):      8.0679\nI^2 (total heterogeneity / total variability):   82.28%\nH^2 (total variability / sampling variability):  5.64\n\nTest for Heterogeneity:\nQ(df = 34) = 151.3768, p-val &lt; .0001\n\nModel Results:\n\nestimate      se      zval    pval     ci.lb     ci.ub      \n-18.1869  1.6648  -10.9245  &lt;.0001  -21.4499  -14.9240  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the output above, we can notice that there is considerable heterogeneity in both the intercepts and slopes (P &lt; 0.01 for Q statistics), meaning that there may be moderators variables that, if added to the model, could potentially reduce the variance. The estimates for the intercept and slopes were 3479.30 kg/ha and -18.18 kg/p.p. (percentage point), respectively.\n\n\n17.5.5 Random coefficients model\nAmong several options in the R ecosystem, the {lme4} package in R offers a comprehensive suite of tools to fit linear mixed-effects models Bates et al. (2015). When analyzing data from multiple trials using this package, it allows for both intercepts and slopes to vary across the trials by treating them as random effects. By doing so, the inherent assumption is that these coefficients (intercepts and slopes) are drawn from certain distributions, and the goal becomes estimating the parameters of these distributions. This modeling technique acknowledges and captures the variability present across different trials. Importantly, by treating the coefficients as random effects, this enables the sharing of information across trials. This not only provides a more holistic understanding of the underlying data structure but also enhances the precision and robustness of the coefficient estimates.\nThe lmer() function allows to fit a mixed model with both the fixed (inc) and random effects (inc | study).\n\nlibrary(lme4)\nrc1 &lt;- lmer(yld ~ inc + (inc |study), data = wm, \n            REML = F)\nsummary(rc1)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: yld ~ inc + (inc | study)\n   Data: wm\n\n     AIC      BIC   logLik deviance df.resid \n  5319.4   5343.1  -2653.7   5307.4      376 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7078 -0.5991 -0.0295  0.5077  3.2364 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n study    (Intercept) 557573.08 746.708       \n          inc             36.85   6.071  -0.29\n Residual              37228.73 192.947       \nNumber of obs: 382, groups:  study, 35\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 3455.432    128.063   26.98\ninc          -17.236      1.451  -11.88\n\nCorrelation of Fixed Effects:\n    (Intr)\ninc -0.300\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.416806 (tol = 0.002, component 1)\n\n\n\n\n17.5.6 Conclusion\nResults from various regression methods show that the calculated damage coefficients fall within the range of -0.28 to -0.56 (see table below). This range of values represents the extent to which damage is influenced by the method in consideration. The most straightforward method, often referred to as the naive approach, produced the most conservative estimate, positioning the damage coefficient at the lower bound of the observed range. In stark contrast, computing the mean values from multiple individual regressions yielded a dc that topped the range, signifying a greater estimated impact.\n\nTable: Intercept, slope and damage coefficients for the four regression approaches to summarize the relationship between soybean yield and white mold incidence.\n\n\nModel\nintercept\nslope\ndamage coefficient\n\n\n\n\nGlobal regression\n3299.6\n-9.261\n-0.28\n\n\nMean of regressions\n3482\n-19.529\n-0.56\n\n\nmeta-analysis\n3479.3\n-18.1869\n-0.52\n\n\nmixed-models\n3455.43\n-17.236\n-0.49\n\n\n\nHowever, it’s worth noting that the coefficients derived from the more advanced techniques - meta-analysis and mixed-models - were quite congruent. Their close alignment suggests that both methodologies, while operating on different principles, capture the underlying dynamics of the data in somewhat analogous ways. A prominent advantage of employing these advanced techniques is their ability to encapsulate and quantify uncertainty. This capability is crucial in scientific analyses as it provides a clearer understanding of the confidence levels associated with the derived coefficients. By being able to measure and articulate this uncertainty, researchers can ensure their interpretations and subsequent decisions are founded on a solid empirical base.\n\n\n\n\nBates, D., Mächler, M., Bolker, B., and Walker, S. 2015. Fitting Linear Mixed-Effects Models Usinglme4. Journal of Statistical Software. 67 Available at: http://dx.doi.org/10.18637/jss.v067.i01.\n\n\nDalla Lana, F., Ziegelmann, P. K., Maia, A. de H. N., Godoy, C. V., and Del Ponte, E. M. 2015. Meta-Analysis of the Relationship Between Crop Yield and Soybean Rust Severity. Phytopathology®. 105:307–315 Available at: http://dx.doi.org/10.1094/PHYTO-06-14-0157-R.\n\n\nLehner, M. S., Pethybridge, S. J., Meyer, M. C., and Del Ponte, E. M. 2016. Meta-analytic modelling of the incidenceyield and incidencesclerotial production relationships in soybean white mould epidemics. Plant Pathology. 66:460–468 Available at: http://dx.doi.org/10.1111/ppa.12590.\n\n\nMadden, L. V., Hughes, G., and Bosch, F. van den, eds. 2007. CHAPTER 12: Epidemics and crop yield. In The American Phytopathological Society, p. 353–388. Available at: http://dx.doi.org/10.1094/9780890545058.012.\n\n\nMadden, L. V., and Paul, P. A. 2009. Assessing Heterogeneity in the Relationship Between Wheat Yield and Fusarium Head Blight Intensity Using Random-Coefficient Mixed Models. Phytopathology®. 99:850–860 Available at: http://dx.doi.org/10.1094/PHYTO-99-7-0850.\n\n\nMadden, L. V., and Paul, P. A. 2011. Meta-Analysis for Evidence Synthesis in Plant Pathology: An Overview. Phytopathology®. 101:16–30 Available at: http://dx.doi.org/10.1094/PHYTO-03-10-0069.\n\n\nViechtbauer, W. 2010. Conducting Meta-Analyses inRwith themetaforPackage. Journal of Statistical Software. 36 Available at: http://dx.doi.org/10.18637/jss.v036.i03."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agrios, G. N. 2005a. INTRODUCTION. In Elsevier, p. 3–75. Available at:\nhttp://dx.doi.org/10.1016/b978-0-08-047378-9.50007-5.\n\n\nAgrios, G. N. 2005b. Plant disease epidemiology. In Elsevier, p.\n265–291. Available at: http://dx.doi.org/10.1016/b978-0-08-047378-9.50014-2.\n\n\nAlves, K. S., and Del Ponte, E. M. 2021. Analysis and simulation of\nplant disease progress curves in R: introducing the epifitter package.\nPhytopathology Research. 3 Available at: http://dx.doi.org/10.1186/s42483-021-00098-7.\n\n\nAlves, K. S., Guimarães, M., Ascari, J. P., Queiroz, M. F., Alfenas, R.\nF., Mizubuti, E. S. G., et al. 2021. RGB-based phenotyping of foliar\ndisease severity under controlled conditions. Tropical Plant Pathology.\n47:105–117 Available at: http://dx.doi.org/10.1007/S40858-021-00448-Y.\n\n\nBaddeley, A., Diggle, P. J., Hardegen, A., Lawrence, T., Milne, R. K.,\nand Nair, G. 2014. On tests of spatial pattern based on simulation\nenvelopes. Ecological Monographs. 84:477–489 Available at: http://dx.doi.org/10.1890/13-2042.1.\n\n\nBaddeley, A., Turner, R., Moller, J., and Hazelton, M. 2005. Residual\nanalysis for spatial point processes (with discussion). Journal of the\nRoyal Statistical Society: Series B (Statistical Methodology).\n67:617–666 Available at: http://dx.doi.org/10.1111/j.1467-9868.2005.00519.x.\n\n\nBarnhart, H. X., Haber, M., and Song, J. 2002. Overall Concordance\nCorrelation Coefficient for Evaluating Agreement Among Multiple\nObservers. Biometrics. 58:1020–1027 Available at: http://dx.doi.org/10.1111/j.0006-341x.2002.01020.x.\n\n\nBates, D., Mächler, M., Bolker, B., and Walker, S. 2015. Fitting Linear\nMixed-Effects Models Usinglme4. Journal of\nStatistical Software. 67 Available at: http://dx.doi.org/10.18637/jss.v067.i01.\n\n\nBock, C. H., Chiang, K.-S., and Del Ponte, E. M. 2021a. Plant disease\nseverity estimated visually: a century of research, best practices, and\nopportunities for improving methods and practices to maximize accuracy.\nTropical Plant Pathology. 47:25–42 Available at: http://dx.doi.org/10.1007/s40858-021-00439-z.\n\n\nBock, C. H., Pethybridge, S. J., Barbedo, J. G. A., Esker, P. D.,\nMahlein, A.-K., and Del Ponte, E. M. 2021b. A phytopathometry glossary\nfor the twenty-first century: towards consistency and precision in\nintra- and inter-disciplinary dialogues. Tropical Plant Pathology.\n47:14–24 Available at: http://dx.doi.org/10.1007/s40858-021-00454-0.\n\n\nBrooks, M. E., Kristensen, K., van Benthem, K. J., Magnusson, A., Berg,\nC. W., Nielsen, A., et al. 2017. glmmTMB balances speed and flexibility among\npackages for zero-inflated generalized linear mixed modeling. The R\nJournal. 9:378–400.\n\n\nBrown, V. A. 2021. An Introduction to Linear Mixed-Effects Modeling in\nR. Advances in Methods and Practices in Psychological Science.\n4:251524592096035 Available at: http://dx.doi.org/10.1177/2515245920960351.\n\n\nCafé-Filho, A. C., Santos, G. R., and Laranjeira, F. F. 2010. Temporal\nand spatial dynamics of watermelon gummy stem blight epidemics. European\nJournal of Plant Pathology. 128:473–482 Available at: http://dx.doi.org/10.1007/s10658-010-9674-1.\n\n\nCampbell, C. L., and Madden. L., V. 1990. Introduction to plant\ndisease epidemiology. Wiley.\n\n\nChester, K. S. 1950. Plant disease losses : Their appraisal and\ninterpretation /. Available at: http://dx.doi.org/10.5962/bhl.title.86198.\n\n\nChiang, K.-S., and Bock, C. H. 2021. Understanding the ramifications of\nquantitative ordinal scales on accuracy of estimates of disease severity\nand data analysis in plant pathology. Tropical Plant Pathology. 47:58–73\nAvailable at: http://dx.doi.org/10.1007/s40858-021-00446-0.\n\n\nChiang, K.-S., Chang, Y. M., Liu, H. I., Lee, J. Y., El Jarroudi, M.,\nand Bock, C. 2023. Survival Analysis as a Basis to Test Hypotheses When\nUsing Quantitative Ordinal Scale Disease Severity Data. Phytopathology®.\nAvailable at: http://dx.doi.org/10.1094/PHYTO-02-23-0055-R.\n\n\nChiang, K.-S., Liu, S.-C., Bock, C. H., and Gottwald, T. R. 2014. What\nInterval Characteristics Make a Good Categorical Disease Assessment\nScale? Phytopathology®. 104:575–585 Available at: http://dx.doi.org/10.1094/phyto-10-13-0279-r.\n\n\nCruz, C. D., and Valent, B. 2017. Wheat blast disease: danger on the\nmove. Tropical Plant Pathology. 42:210–222 Available at: http://dx.doi.org/10.1007/s40858-017-0159-z.\n\n\nDalla Lana, F., Ziegelmann, P. K., Maia, A. de H. N., Godoy, C. V., and\nDel Ponte, E. M. 2015. Meta-Analysis of the Relationship Between Crop\nYield and Soybean Rust Severity. Phytopathology®. 105:307–315 Available\nat: http://dx.doi.org/10.1094/PHYTO-06-14-0157-R.\n\n\nDel Ponte, E. M., Cazón, L. I., Alves, K. S., Pethybridge, S. J., and\nBock, C. H. 2022. How much do standard area diagrams improve accuracy of\nvisual estimates of the percentage area diseased? A systematic review\nand meta-analysis. Tropical Plant Pathology. 47:43–57 Available at: http://dx.doi.org/10.1007/s40858-021-00479-5.\n\n\nDel Ponte, E. M., Pethybridge, S. J., Bock, C. H., Michereff, S. J.,\nMachado, F. J., and Spolti, P. 2017. Standard Area Diagrams for Aiding\nSeverity Estimation: Scientometrics, Pathosystems, and Methodological\nTrends in the Last 25 Years. Phytopathology®. 107:1161–1174 Available\nat: http://dx.doi.org/10.1094/PHYTO-02-17-0069-FI.\n\n\nDuffeck, M. R., Santos Alves, K. dos, Machado, F. J., Esker, P. D., and\nDel Ponte, E. M. 2020. Modeling Yield Losses and Fungicide Profitability\nfor Managing Fusarium Head Blight in Brazilian Spring Wheat.\nPhytopathology®. 110:370–378 Available at: http://dx.doi.org/10.1094/PHYTO-04-19-0122-R.\n\n\nEsser, D. S., Leveau, J. H. J., Meyer, K. M., and Wiegand, K. 2014.\nSpatial scales of interactions among bacteria and between bacteria and\nthe leaf surface. FEMS Microbiology Ecology. 91 Available at: http://dx.doi.org/10.1093/femsec/fiu034.\n\n\nFranceschi, V. T., Alves, K. S., Mazaro, S. M., Godoy, C. V., Duarte, H.\nS. S., and Del Ponte, E. M. 2020. A new standard area diagram set for\nassessment of severity of soybean rust improves accuracy of estimates\nand optimizes resource use. Plant Pathology. 69:495–505 Available at: http://dx.doi.org/10.1111/ppa.13148.\n\n\nFrancl, L. J. 2001. The..disease triangle: A plant pathological paradigm\nrevisited. The Plant Health Instructor. Available at: http://dx.doi.org/10.1094/PHI-T-2001-0517-01.\n\n\nGigot, C. 2018. Epiphy: Analysis of plant disease epidemics.\n\n\nGodoy, C. V., Seixas, C. D. S., Soares, R. M., Marcelino-Guimarães, F.\nC., Meyer, M. C., and Costamilan, L. M. 2016. Asian soybean rust in\nbrazil: Past, present, and future. Pesquisa Agropecuária Brasileira.\n51:407–421 Available at: http://dx.doi.org/10.1590/S0100-204X2016000500002.\n\n\nGonzález-Domínguez, E., Martins, R. B., Del Ponte, E. M., Michereff, S.\nJ., García-Jiménez, J., and Armengol, J. 2014. Development and\nvalidation of a standard area diagram set to aid assessment of severity\nof loquat scab on fruit. European Journal of Plant Pathology. Available\nat: http://dx.doi.org/10.1007/s10658-014-0400-2.\n\n\nHebert, T. T. 1982. The rationale for the horsfall-barratt plant disease\nassessment scale. Phytopathology. 72:1269 Available at: http://dx.doi.org/10.1094/phyto-72-1269.\n\n\nIslam, M. T., Kim, K.-H., and Choi, J. 2019. Wheat Blast in Bangladesh:\nThe Current Situation and Future Impacts. The Plant Pathology Journal.\n35:1–10 Available at: http://dx.doi.org/10.5423/ppj.rw.08.2018.0168.\n\n\nJeger, M. J., and Viljanen-Rollinson, S. L. H. 2001. The use of the area\nunder the disease-progress curve (AUDPC) to assess quantitative disease\nresistance in crop cultivars. Theoretical and Applied Genetics.\n102:32–40 Available at: http://dx.doi.org/10.1007/s001220051615.\n\n\nJesus Junior, W. C. de, and Bassanezi, R. B. 2004. Análise da dinâmica e\nestrutura de focos da morte súbita dos citros. Fitopatologia Brasileira.\n29:399–405 Available at: http://dx.doi.org/10.1590/S0100-41582004000400007.\n\n\nLaranjeira, F. F., Bergamin Filho, A. R., and Amorim, L. I. 1998.\nDinâmica e estrutura de focos da clorose variegada dos\ncitros (CVC). Fitopatologia Brasileira. 23:36–41.\n\n\nLaranjeira, F. F., Bergamin Filho, A., Amorim, L., and Gottwald, T. R.\n2004. Dinâmica espacial da clorose variegada dos citros em três regiões\ndo estado de são paulo. Fitopatologia Brasileira. 29:56–65 Available at:\nhttp://dx.doi.org/10.1590/S0100-41582004000100009.\n\n\nLehner, M. S., Pethybridge, S. J., Meyer, M. C., and Del Ponte, E. M.\n2016. Meta-analytic modelling of the\nincidenceyield and incidencesclerotial\nproduction relationships in soybean white mould epidemics. Plant\nPathology. 66:460–468 Available at: http://dx.doi.org/10.1111/ppa.12590.\n\n\nLi, B., Madden, L. V., and Xu, X. 2011. Spatial analysis by distance\nindices: an alternative local clustering index for studying spatial\npatterns. Methods in Ecology and Evolution. 3:368–377 Available at: http://dx.doi.org/10.1111/j.2041-210x.2011.00165.x.\n\n\nLi, F., Upadhyaya, N. M., Sperschneider, J., Matny, O., Nguyen-Phuc, H.,\nMago, R., et al. 2019. Emergence of the Ug99 lineage of the wheat stem\nrust pathogen through somatic hybridisation. Nature Communications. 10\nAvailable at: http://dx.doi.org/10.1038/s41467-019-12927-7.\n\n\nLin, L. I.-K. 1989. A concordance correlation coefficient to evaluate\nreproducibility. Biometrics. 45:255 Available at: http://dx.doi.org/10.2307/2532051.\n\n\nLiu, H. I., Tsai, J. R., Chung, W. H., Bock, C. H., and Chiang, K. S.\n2019. Effects of Quantitative Ordinal Scale Design on the Accuracy of\nEstimates of Mean Disease Severity. Agronomy. 9:565 Available at: http://dx.doi.org/10.3390/agronomy9090565.\n\n\nMadden, L. V. 1982. Evaluation of tests for randomness of infected\nplants. Phytopathology. 72:195 Available at: http://dx.doi.org/10.1094/phyto-72-195.\n\n\nMadden, L. V., Esker, P. D., and Pethybridge, S. J. 2021. Forrest W.\nNutter, Jr.: a career in phytopathometry. Tropical Plant Pathology.\n47:5–13 Available at: http://dx.doi.org/10.1007/s40858-021-00469-7.\n\n\nMadden, L. V., Hughes, G., and Bosch, F. van den, eds. 2007a. CHAPTER\n12: Epidemics and crop yield. In The American Phytopathological Society,\np. 353–388. Available at: http://dx.doi.org/10.1094/9780890545058.012.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007b. Spatial aspects\nof epidemicsIII: Patterns of plant disease. In The American\nPhytopathological Society, p. 235–278. Available at: http://dx.doi.org/10.1094/9780890545058.009.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007c. Temporal\nanalysis i: Quantifying and comparing epidemics. In The American\nPhytopathological Society, p. 63–116. Available at: http://dx.doi.org/10.1094/9780890545058.004.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F. 2007d. The study of\nplant disease epidemics. The American Phytopathological Society.\nAvailable at: http://dx.doi.org/10.1094/9780890545058.\n\n\nMadden, L. V., and Paul, P. A. 2009. Assessing Heterogeneity in the\nRelationship Between Wheat Yield and Fusarium Head Blight Intensity\nUsing Random-Coefficient Mixed Models. Phytopathology®. 99:850–860\nAvailable at: http://dx.doi.org/10.1094/PHYTO-99-7-0850.\n\n\nMadden, L. V., and Paul, P. A. 2011. Meta-Analysis for Evidence\nSynthesis in Plant Pathology: An Overview. Phytopathology®. 101:16–30\nAvailable at: http://dx.doi.org/10.1094/PHYTO-03-10-0069.\n\n\nMalaker, P. K., Barma, N. C. D., Tiwari, T. P., Collis, W. J.,\nDuveiller, E., Singh, P. K., et al. 2016. First Report of Wheat Blast\nCaused by Magnaporthe oryzae Pathotype\ntriticum in Bangladesh. Plant Disease.\n100:2330–2330 Available at: http://dx.doi.org/10.1094/pdis-05-16-0666-pdn.\n\n\nMikaberidze, A., Mundt, C. C., and Bonhoeffer, S. 2015. Data from:\nInvasiveness of plant pathogens depends on the spatial scale of host\ndistribution. Available at: http://datadryad.org/stash/dataset/doi:10.5061/dryad.f2j8s.\n\n\nMoran, P. A. P. 1950. Notes on\ncontinuous stochastic phenomena. Biometrika. 37:17.\n\n\nMoreira, R. R., Silva Silveira Duarte, H. da, and De Mio, L. L. M. 2018.\nImproving accuracy, precision and reliability of severity estimates of\nGlomerella leaf spot on apple leaves using a new standard area diagram\nset. European Journal of Plant Pathology. 153:975–982 Available at: http://dx.doi.org/10.1007/s10658-018-01610-0.\n\n\nMundt, C. C., Ahmed, H. U., Finckh, M. R., Nieva, L. P., and Alfonso, R.\nF. 1999. Primary Disease Gradients of Bacterial Blight of Rice.\nPhytopathology®. 89:64–67 Available at: http://dx.doi.org/10.1094/phyto.1999.89.1.64.\n\n\nNelson, S. C. 1996. A simple analysis of disease foci. Phytopathology.\n86:432–439.\n\n\nNutter, F. W., and Esker, P. D. 2006. The Role of Psychophysics in\nPhytopathology: The WeberFechner Law Revisited. European\nJournal of Plant Pathology. 114:199–213 Available at: http://dx.doi.org/10.1007/s10658-005-4732-9.\n\n\nNutter, F. W., Esker, P. D., and Netto, R. A. C. 2006. Disease\nAssessment Concepts and the Advancements Made in Improving the Accuracy\nand Precision of Plant Disease Data. European Journal of Plant\nPathology. 115:95–103 Available at: http://dx.doi.org/10.1007/s10658-005-1230-z.\n\n\nOlivoto, T. 2022. Lights, camera, pliman! An R package for plant image\nanalysis. Methods in Ecology and Evolution. 13:789–798 Available at: http://dx.doi.org/10.1111/2041-210X.13803.\n\n\nOlivoto, T., Andrade, S. M. P., and M. Del Ponte, E. 2022. Measuring\nplant disease severity in R: introducing and evaluating the pliman\npackage. Tropical Plant Pathology. 47:95–104 Available at: http://dx.doi.org/10.1007/s40858-021-00487-5.\n\n\nOnofri, A., Piepho, H.-P., and Kozak, M. 2018. Analysing censored data\nin agricultural research: A review with examples and software tips.\nAnnals of Applied Biology. 174:3–13 Available at: http://dx.doi.org/10.1111/aab.12477.\n\n\nParker, S. K., Nutter, F. W., and Gleason, M. L. 1997. Directional\nSpread of Septoria Leaf Spot in Tomato Rows. Plant Disease. 81:272–276\nAvailable at: http://dx.doi.org/10.1094/pdis.1997.81.3.272.\n\n\nPereira, W. E. L., Andrade, S. M. P. de, Del Ponte, E. M., Esteves, M.\nB., Canale, M. C., Takita, M. A., et al. 2020. Severity assessment in\nthe Nicotiana tabacum-Xylella fastidiosa subsp. pauca pathosystem:\ndesign and interlaboratory validation of a standard area diagram set.\nTropical Plant Pathology. 45:710–722 Available at: http://dx.doi.org/10.1007/s40858-020-00401-5.\n\n\nSackett, K. E., and Mundt, C. C. 2005. Primary Disease Gradients of\nWheat Stripe Rust in Large Field Plots. Phytopathology®. 95:983–991\nAvailable at: http://dx.doi.org/10.1094/PHYTO-95-0983.\n\n\nSavary, S., Teng, P. S., Willocquet, L., and Nutter, F. W. 2006.\nQuantification and Modeling of Crop Losses: A Review of Purposes. Annual\nReview of Phytopathology. 44:89–112 Available at: http://dx.doi.org/10.1146/annurev.phyto.44.070505.143342.\n\n\nSavary, S., Willocquet, L., Pethybridge, S. J., Esker, P., McRoberts,\nN., and Nelson, A. 2019. The global burden of pathogens and pests on\nmajor food crops. Nature Ecology & Evolution. 3:430–439 Available\nat: http://dx.doi.org/10.1038/s41559-018-0793-y.\n\n\nScott, P. R., and Hollins, T. W. 1974. Effects of eyespot on the yield\nof winter wheat. Annals of Applied Biology. 78:269–279 Available at: http://dx.doi.org/10.1111/j.1744-7348.1974.tb01506.x.\n\n\nShah, D. A., and Madden, L. V. 2004. Nonparametric Analysis of Ordinal\nData in Designed Factorial Experiments. Phytopathology®. 94:33–43\nAvailable at: http://dx.doi.org/10.1094/PHYTO.2004.94.1.33.\n\n\nShrout, P. E., and Fleiss, J. L. 1979. Intraclass correlations: Uses in\nassessing rater reliability. Psychological Bulletin. 86:420–428\nAvailable at: http://dx.doi.org/10.1037/0033-2909.86.2.420.\n\n\nSimko, I., and Piepho, H.-P. 2012. The Area Under the Disease Progress\nStairs: Calculation, Advantage, and Application. Phytopathology®.\n102:381–389 Available at: http://dx.doi.org/10.1094/phyto-07-11-0216.\n\n\nTembo, B., Mulenga, R. M., Sichilima, S., M’siska, K. K., Mwale, M.,\nChikoti, P. C., et al. 2020. Detection and characterization of fungus\n(Magnaporthe oryzae pathotype Triticum) causing wheat blast disease on\nrain-fed grown wheat (Triticum aestivum L.) in Zambia ed. Zonghua Wang.\nPLOS ONE. 15:e0238724 Available at: http://dx.doi.org/10.1371/journal.pone.0238724.\n\n\nThresh, J. M. 1998. In memory of James Edward Vanderplank\n19091997. Plant Pathology. 47:114–115 Available at: http://dx.doi.org/10.1046/j.1365-3059.2998.00220.x.\n\n\nVanderplank, J. 1963. Plant disease epidemics and control.\nElsevier. Available at: http://dx.doi.org/10.1016/C2013-0-11642-X.\n\n\nViechtbauer, W. 2010. Conducting Meta-Analyses\ninRwith\nthemetaforPackage. Journal of Statistical\nSoftware. 36 Available at: http://dx.doi.org/10.18637/jss.v036.i03.\n\n\nWiegand, T., and A. Moloney, K. 2004. Rings, circles, and null-models\nfor point pattern analysis in ecology. Oikos. 104:209–229 Available at:\nhttp://dx.doi.org/10.1111/j.0030-1299.2004.12497.x.\n\n\nXu, X.-M., and Madden, L. V. 2004. Use of SADIE statistics\nto study spatial dynamics of plant disease epidemics. Plant Pathology.\n53:38–49 Available at: http://dx.doi.org/10.1111/j.1365-3059.2004.00949.x.\n\n\nYadav, N. V. S., Vos, S. M. de, Bock, C. H., and Wood, B. W. 2012.\nDevelopment and validation of standard area diagrams to aid assessment\nof pecan scab symptoms on fruit. Plant Pathology. 62:325–335 Available\nat: http://dx.doi.org/10.1111/j.1365-3059.2012.02641.x.\n\n\nYorinori, J. T., Paiva, W. M., Frederick, R. D., Costamilan, L. M.,\nBertagnolli, P. F., Hartman, G. E., et al. 2005. Epidemics of Soybean\nRust (Phakopsora pachyrhizi) in Brazil and\nParaguay from 2001 to 2003. Plant Disease. 89:675–677 Available at: http://dx.doi.org/10.1094/PD-89-0675.\n\n\nZadoks, J. C., and Schein, R. D. 1988. James Edward Vanderplank:\nMaverick* and Innovator. Annual Review of Phytopathology. 26:31–37\nAvailable at: http://dx.doi.org/10.1146/annurev.py.26.090188.000335."
  },
  {
    "objectID": "warning-systems.html#introduction",
    "href": "warning-systems.html#introduction",
    "title": "18  Warning systems",
    "section": "18.1 Introduction",
    "text": "18.1 Introduction\nOne practical application of plant disease epidemiology is to predict disease occurrences to guide timely management interventions, reducing crop damage and rationalizing pesticide use. Since the early 1900s, warning systems (synonyms: disease forecaster, predictor) have advanced considerably. More comprehensively, these systems have evolved to a Decision Support System (DSS), when they integrate expert input, models, and databases for more nuanced management recommendations, transcending simple prediction to encompass various goals within computerized frameworks. In fact, technological progress in recent decades has boosted the development and automation of decision support system (DSS), now widely available in public and private sectors, providing direct, sophisticated guidance to crop advisors and growers (Figure 18.1).\n\n\n\nFigure 18.1: Core elements of a decision support system that provides risk information for plant disease management\n\n\nIn its core, a DSS for plant disease management are based on a disease model, or a simplified, often mathematical, representation of a the system (here the pathosystem) used for making predictions or suggesting management decisions based on the risk information they provide. The models range from basic rules (e.g. if-then) and static thresholds to sophisticated simulation models covering entire disease epidemics. Usually these models depend on weather information, that are collected on site in automatic weather stations or, more recently, from remote sensing or data reanalysis sources, as input and are improved and tested with field data."
  },
  {
    "objectID": "warning-systems.html#when-is-a-dws-needed",
    "href": "warning-systems.html#when-is-a-dws-needed",
    "title": "18  Warning systems",
    "section": "18.2 When is a DWS needed?",
    "text": "18.2 When is a DWS needed?\nDisease warning systems for operational use must fulfill four criteria to be practical:\n\nThey should target diseases that are prevalent and economically damaging, affecting yield or quality. Merely having severe economic consequences doesn’t guarantee a system’s practical use or attention from growers and advisors.\nThe diseases should exhibit variability across seasons in terms of onset, epidemic growth rate, severity, or another aspect, creating uncertainty in decision-making. Diseases with predictable patterns provide minimal information and little management advantage, reducing the relevance of warning systems.\nUsers should be capable of acting on the system’s alerts, necessitating available and effective control measures and sufficient response time to prevent crop damage. Systems are unhelpful if practitioners can’t adapt their strategies promptly.\nThe system must encompass comprehensive knowledge about the disease, synthesizing accurate risk estimates. Understanding the specific interactions between host, pathogen, and environment is crucial for the system’s effectiveness and relevance."
  },
  {
    "objectID": "warning-systems.html#what-types-dwss-are-there",
    "href": "warning-systems.html#what-types-dwss-are-there",
    "title": "18  Warning systems",
    "section": "18.2 What types DWSs are there?",
    "text": "18.2 What types DWSs are there?\nThese systems vary significantly in structure and design, reflecting the multitude of plant diseases, objectives, available data, control strategies, developer preferences, and operational infrastructures. They range from basic guidelines and static thresholds to sophisticated simulation models covering entire disease epidemics. They might leverage diverse inputs beyond weather, like host, pathogen, and economic factors, catering to the complexities of disease prediction. The underlying models in these systems are either empirical, based on statistical correlations with disease data, or mechanistic (process-based), constructed around understood cause-effect relationships within disease progression.\n\n18.2.1 Disease thresholds\nDisease thresholds, integral to integrated pest management in entomology, serve as basic disease warning systems. They involve economic injury levels, denoting pest abundance that equates control costs with incurred losses, and economic thresholds, indicating when action is necessary to avoid reaching injury levels. These concepts, while straightforward, are complex in practical scenarios, adapting to factors like crop stages and environmental conditions. Though less prevalent than in arthropod management, thresholds guide actions like fungicide application in plant diseases, especially those directly impacting yield through photosynthetic area reduction. However, their application is challenging for rapid, recurrent diseases affecting high-value crops or quality-centric produce, requiring prompt intervention even at minimal disease presence. With potato late blight, for instance, the first fungicide application may need to be applied by the time disease severity reaches 0.1% of the foliage.\n\n\n18.2.2 Warnings based on weather\nDisease warning systems frequently predict conditions conducive to pathogenesis, with wetness and temperature being key variables for many foliar diseases. BLITECAST, the first computerized system, exemplifies this in its monitoring of late blight in potatoes and tomatoes. While initial inoculum is often undetectable, the presence of pathogens like Phytophthora infestans is assumed. Accurate predictions of an outbreak are possible by tracking environmental conditions favorable for the pathogen’s development.\nWeather-driven disease warning systems, like FAST, TOMCAST, and PLAM, continuously monitor moisture and temperature for various crop diseases. These systems calculate weather favorability or severity values from environmental data, predicting infection and disease severity. They guide growers on optimal spraying schedules, initiating treatments or determining application intervals based on these accumulated severity values.\n\n18.2.2.1 Wallin model\nJ.R. Wallin developed a model in the mid-20th century focusing on forecasting potato late blight, detailed across several publications. The model tracks hourly relative humidity and temperature, emphasizing periods with relative humidity of 90% or more. It calculates the number of high-humidity hours and corresponding minimum and maximum temperatures. By accumulating ‘disease severity values’ from plant emergence throughout the season, based on humidity and temperature metrics, the model predicts the initial onset and subsequent spread of potato late blight.\nRelationship of temperature and relative humidity (RH) periods as used in the Wallin late blight forecasting system to predict Disease Severity Values (DSVs) from 0 to 4.\n\n\n\nAverage Temperature Range\n0\n1\n2\n3\n4\n\n\n7.2 - 11.6 C\n15\n16-18\n19-21\n22-24\n&gt;25\n\n\n11.7 - 15.0 C\n12\n13-15\n16-18\n19-21\n&gt;22\n\n\n15.1 - 26.6 C\n9\n10-12\n13-15\n16-18\n&gt;19\n\n\n\nLet’s download hourly weather data from NASA Power project using nasapower r package for the locality of Viçosa, MG, Brazil during the month of March 2022.\n\nlibrary(nasapower)\nweather &lt;- get_power(\n      community = \"ag\",\n      lonlat = c(-42.88, -20.7561),\n      pars = c(\"RH2M\", \"T2M\"),\n      dates = c(\"2022-03-02\", \"2022-03-31\"),\n      temporal_api = \"hourly\"\n    )\nhead(weather)\n\nNASA/POWER CERES/MERRA2 Native Resolution Hourly Data  \n Dates (month/day/year): 03/02/2022 through 03/31/2022  \n Location: Latitude  -20.7561   Longitude -42.88  \n Elevation from MERRA-2: Average for 0.5 x 0.625 degree lat/lon region = 665.27 meters \n The value for missing source data that cannot be computed or is outside of the sources availability range: NA  \n Parameter(s):  \n \n Parameters: \n RH2M     MERRA-2 Relative Humidity at 2 Meters (%) ;\n T2M      MERRA-2 Temperature at 2 Meters (C)  \n \n# A tibble: 6 × 8\n    LON   LAT  YEAR    MO    DY    HR  RH2M   T2M\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -42.9 -20.8  2022     3     1    21  93.6  20.6\n2 -42.9 -20.8  2022     3     1    22  94.4  20.2\n3 -42.9 -20.8  2022     3     1    23  95.4  19.7\n4 -42.9 -20.8  2022     3     2     0  96.3  19.4\n5 -42.9 -20.8  2022     3     2     1  97.2  19.0\n6 -42.9 -20.8  2022     3     2     2  98.1  18.8\n\n\nIn order to prepare the data to calculate the DSVs, we first need to obtain the leaf wetness duration variable (LW) based on hours of relative humidity &gt;90% and then the average temperature during the LW for each day. We group the variables by year, month and day and use mutate() and summarise().\n\nlibrary(tidyverse)\nweather2 &lt;- weather |&gt; \n      group_by(YEAR, MO, DY) |&gt; \n      mutate(LW = case_when(RH2M &gt; 90 ~ 1,\n                            TRUE ~ 0)) |&gt; \n      filter(LW &gt; 0) |&gt;\n      summarise(Air_LWD = mean(T2M, na.rm = TRUE),\n                LWD = n())\n\nNow we are ready to calculate the daily DSV based on rules on the table and inspect the first 6 rows of the new table called df_wallin.\n\ndf_wallin &lt;- weather2 |&gt; \n      mutate(\n        DSV = case_when(\n          # Temperature Range: 7.2 - 11.6 C\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &lt;= 15 ~ 0,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 15 & LWD &lt;= 18 ~ 1,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 18 & LWD &lt;= 21 ~ 2,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 21 & LWD &lt;= 24 ~ 3,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 24 ~ 4,\n          \n          # Temperature Range: 11.7 - 15.0 C\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &lt;= 12 ~ 0,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 12 & LWD &lt;= 15 ~ 1,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 15 & LWD &lt;= 18 ~ 2,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 18 & LWD &lt;= 21 ~ 3,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 21 ~ 4,\n          \n          # Temperature Range: 15.1 - 26.6 C\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &lt;= 9 ~ 0,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 9 & LWD &lt;= 12 ~ 1,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 12 & LWD &lt;= 15 ~ 2,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 15 & LWD &lt;= 18 ~ 3,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 18 ~ 4,\n          \n          # Default (For temperatures out of the specified ranges or any other scenarios)\n          TRUE ~ 0  # Assigning a default value of 0\n        )\n      )\n    head(df_wallin)\n\n# A tibble: 6 × 6\n# Groups:   YEAR, MO [1]\n   YEAR    MO    DY Air_LWD   LWD   DSV\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1  2022     3     1    20.2     3     0\n2  2022     3     2    19.2    10     1\n3  2022     3     3    19.7    12     1\n4  2022     3     4    19.9    10     1\n5  2022     3     5    19.5     7     0\n6  2022     3     6    19.0     9     0\n\n\nWe can visualize the daily and cumulative DSV for the monthly period. The dashed line indicates the action threshold of 20 cumulative DSV, or when a spray should be applied. Please not that in the real system, the DSV is reduced to zero and another counting is initiated.\n\ndf_wallin2 &lt;- df_wallin |&gt; \n  mutate(DSV2 = cumsum(DSV),\n  date = as.Date(sprintf('%04d-%02d-%02d', YEAR, MO, DY)))\n\ndf_wallin2 |&gt; \n  ggplot(aes(date, DSV))+\n  geom_col(fill = \"#339966\")+\n  geom_line(aes(date, DSV2))+\n  geom_hline(yintercept = 20, linetype = 2)+\n  annotate(geom = \"text\", x = as.Date(\"2022-03-04\"), y = 20.5, label = \"Action threshold\")+\n  r4pde::theme_r4pde()+\n  labs(x = \"Date\", y = \"Daily and cumulative DSV\")"
  },
  {
    "objectID": "warning-systems.html#section",
    "href": "warning-systems.html#section",
    "title": "18  Warning systems",
    "section": "18.2 ",
    "text": "18.2"
  },
  {
    "objectID": "warning-systems.html#what-types-systems-are-there",
    "href": "warning-systems.html#what-types-systems-are-there",
    "title": "18  Warning systems",
    "section": "18.2 What types systems are there?",
    "text": "18.2 What types systems are there?\nThese systems vary significantly in structure and design, reflecting the multitude of plant diseases, objectives, available data, control strategies, developer preferences, and operational infrastructures. They range from basic guidelines and static thresholds to sophisticated simulation models covering entire disease epidemics. They might leverage diverse inputs beyond weather, like host, pathogen, and economic factors, catering to the complexities of disease prediction. The underlying models in these systems are either empirical, based on statistical correlations with disease data, or mechanistic (process-based), constructed around understood cause-effect relationships within disease progression.\n\n18.2.1 Disease thresholds\nDisease thresholds, integral to integrated pest management in entomology, serve as basic disease warning systems. They involve economic injury levels, denoting pest abundance that equates control costs with incurred losses, and economic thresholds, indicating when action is necessary to avoid reaching injury levels. These concepts, while straightforward, are complex in practical scenarios, adapting to factors like crop stages and environmental conditions.\nThough less prevalent than in arthropod management, thresholds guide actions like fungicide application in plant diseases, especially those directly impacting yield through photosynthetic area reduction. However, their application is challenging for rapid, recurrent diseases affecting high-value crops or quality-centric produce, requiring prompt intervention even at minimal disease presence. With potato late blight, for instance, the first fungicide application may need to be applied by the time disease severity reaches as low as 0.1% of the foliage.\nThe concept of economic damage threshold (EDT) may be used as a criteria do warn fungicide spray. By definition, EDT is the amount of disease intensity (e.g. severity when dealing with foliar diseases) that corresponds to an economic loss that equals the control cost to combat the disease. A formula for the EDT was proposed by Mumford and Norton (Mumford and Norton 1984) further modified by Reis (Reis et al. 2002) for use in foliar fungal diseases, as described in Equation 3:\n\\(EDT = \\frac{F_C}{S_P . D_C} . C_e\\) ,\nwhere EDT is the disease intensity, \\(F_C\\) is the fungicide cost (USD/ha), \\(S_P\\) is the soybean price (USD/ton), \\(D_C\\) is the damage coefficient (calculated based on the potential yield) and \\(C_e\\) is the control efficacy of the fungicide (%). In practice, sprays should be applied prior to reaching the EDT, which gives rise to the ADT (action disease threshold).\nIn a study on northern corn leaf blight in Argentina, the following values were used by the authors to calculate the EDT (De Rossi et al. 2022). Note that in the study the authors adjusted the Dc to potential yield by multiplying by the potential yield value in metric tons, since the Dc is normalized to metric tons. More about Dc here in the dedicated chapter on yield loss. The action disease threshold (ADT) was defined as 20% reduction of the EDT in that study.\n\ncalculate_EDT &lt;- function(Fc, Sp, Dc, Ec) {\n  EDT &lt;- (Fc / (Sp * Dc)) * Ce\n  return(EDT)\n}\n\nFc &lt;- 30      # fixed cost of control is 30 USD/ha.\nSp &lt;- 112     # fixed soybean price is 112 USD/ton.\nDc &lt;- 0.1712  # for potential yield of 8.5 t/ha so 8.5 x 0.02015 = 0.1712.\nCe &lt;- 0.70    # control efficacy of fungicide is 70%.\nEDT_value &lt;- calculate_EDT(Fc, Sp, Dc, Ce)\nprint(EDT_value)\n\n[1] 1.09521\n\nADT = EDT_value * 0.80\nADT\n\n[1] 0.8761682\n\n\n\n\n18.2.2 Warnings based on weather\nDisease warning systems frequently predict conditions conducive to pathogenesis, with wetness and temperature being key variables for many foliar diseases. BLITECAST, the first computerized system, exemplifies this in its monitoring of late blight in potatoes and tomatoes. While initial inoculum is often undetectable, the presence of pathogens like Phytophthora infestans is assumed. Accurate predictions of an outbreak are possible by tracking environmental conditions favorable for the pathogen’s development.\nWeather-driven disease warning systems, like FAST, TOMCAST, and PLAM, continuously monitor moisture and temperature for various crop diseases. These systems calculate weather favorability or severity values from environmental data, predicting infection and disease severity. They guide growers on optimal spraying schedules, initiating treatments or determining application intervals based on these accumulated severity values.\n\n18.2.2.1 Wallin model\nJ.R. Wallin developed a model in the mid-20th century focusing on forecasting potato late blight, detailed across several publications. The model tracks hourly relative humidity and temperature, emphasizing periods with relative humidity of 90% or more. It calculates the number of high-humidity hours and corresponding minimum and maximum temperatures. By accumulating ‘disease severity values’ from plant emergence throughout the season, based on humidity and temperature metrics, the model predicts the initial onset and subsequent spread of potato late blight. The table below summarizes the way the DSVs are obtained based on combinations of hours of relative humdity &gt; 90% and the air temperature within that period.\nTable. Relationship of temperature and relative humidity (RH) periods as used in the Wallin late blight forecasting system to predict Disease Severity Values (DSVs)\n\n\n\n\nDaily\nseverity\nvalue\n\n\n\n\nAverage Temperature Range\n0\n1\n2\n3\n4\n\n\n7.2 - 11.6 C\n15\n16-18\n19-21\n22-24\n&gt;25\n\n\n11.7 - 15.0 C\n12\n13-15\n16-18\n19-21\n&gt;22\n\n\n15.1 - 26.6 C\n9\n10-12\n13-15\n16-18\n&gt;19\n\n\n\nLet’s download hourly weather data from NASA Power project using nasapower r package for the locality of Viçosa, MG, Brazil during the month of March 2022.\n\nlibrary(nasapower)\nweather &lt;- get_power(\n      community = \"ag\",\n      lonlat = c(-42.88, -20.7561),\n      pars = c(\"RH2M\", \"T2M\"),\n      dates = c(\"2022-03-02\", \"2022-03-31\"),\n      temporal_api = \"hourly\"\n    )\nhead(weather)\n\nNASA/POWER CERES/MERRA2 Native Resolution Hourly Data  \n Dates (month/day/year): 03/02/2022 through 03/31/2022  \n Location: Latitude  -20.7561   Longitude -42.88  \n Elevation from MERRA-2: Average for 0.5 x 0.625 degree lat/lon region = 665.27 meters \n The value for missing source data that cannot be computed or is outside of the sources availability range: NA  \n Parameter(s):  \n \n Parameters: \n RH2M     MERRA-2 Relative Humidity at 2 Meters (%) ;\n T2M      MERRA-2 Temperature at 2 Meters (C)  \n \n# A tibble: 6 × 8\n    LON   LAT  YEAR    MO    DY    HR  RH2M   T2M\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -42.9 -20.8  2022     3     1    21  93.6  20.6\n2 -42.9 -20.8  2022     3     1    22  94.4  20.2\n3 -42.9 -20.8  2022     3     1    23  95.4  19.7\n4 -42.9 -20.8  2022     3     2     0  96.3  19.4\n5 -42.9 -20.8  2022     3     2     1  97.2  19.0\n6 -42.9 -20.8  2022     3     2     2  98.1  18.8\n\n\nIn order to prepare the data to calculate the DSVs, we first need to obtain the leaf wetness duration variable (LW) based on hours of relative humidity &gt;90% and then the average temperature during the LW for each day. We group the variables by year, month and day and use mutate() and summarise().\n\nlibrary(tidyverse)\nweather2 &lt;- weather |&gt; \n      group_by(YEAR, MO, DY) |&gt; \n      mutate(LW = case_when(RH2M &gt; 90 ~ 1,\n                            TRUE ~ 0)) |&gt; \n      filter(LW &gt; 0) |&gt;\n      summarise(Air_LWD = mean(T2M, na.rm = TRUE),\n                LWD = n())\n\nNow we are ready to calculate the daily DSV based on rules on the table and inspect the first 6 rows of the new table called df_wallin.\n\ndf_wallin &lt;- weather2 |&gt; \n      mutate(\n        DSV = case_when(\n          # Temperature Range: 7.2 - 11.6 C\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &lt;= 15 ~ 0,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 15 & LWD &lt;= 18 ~ 1,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 18 & LWD &lt;= 21 ~ 2,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 21 & LWD &lt;= 24 ~ 3,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 24 ~ 4,\n          \n          # Temperature Range: 11.7 - 15.0 C\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &lt;= 12 ~ 0,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 12 & LWD &lt;= 15 ~ 1,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 15 & LWD &lt;= 18 ~ 2,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 18 & LWD &lt;= 21 ~ 3,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 21 ~ 4,\n          \n          # Temperature Range: 15.1 - 26.6 C\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &lt;= 9 ~ 0,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 9 & LWD &lt;= 12 ~ 1,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 12 & LWD &lt;= 15 ~ 2,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 15 & LWD &lt;= 18 ~ 3,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 18 ~ 4,\n          \n          # Default (For temperatures out of the specified ranges or any other scenarios)\n          TRUE ~ 0  # Assigning a default value of 0\n        )\n      )\n    head(df_wallin)\n\n# A tibble: 6 × 6\n# Groups:   YEAR, MO [1]\n   YEAR    MO    DY Air_LWD   LWD   DSV\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1  2022     3     1    20.2     3     0\n2  2022     3     2    19.2    10     1\n3  2022     3     3    19.7    12     1\n4  2022     3     4    19.9    10     1\n5  2022     3     5    19.5     7     0\n6  2022     3     6    19.0     9     0\n\n\nWe can visualize the daily and cumulative DSV for the monthly period. The dashed line indicates the action threshold of 20 cumulative DSV, or when a spray should be applied. Please not that in the real system, the DSV is reduced to zero and another counting is initiated.\n\ndf_wallin2 &lt;- df_wallin |&gt; \n  mutate(DSV2 = cumsum(DSV),\n  date = as.Date(sprintf('%04d-%02d-%02d', YEAR, MO, DY)))\n\ndf_wallin2 |&gt; \n  ggplot(aes(date, DSV))+\n  geom_col(fill = \"#339966\")+\n  geom_line(aes(date, DSV2))+\n  geom_hline(yintercept = 20, linetype = 2)+\n  annotate(geom = \"text\", x = as.Date(\"2022-03-04\"), y = 20.5, label = \"Action threshold\")+\n  r4pde::theme_r4pde()+\n  labs(x = \"Date\", y = \"Daily and cumulative DSV\")\n\n\n\n\n\n\n\n\nDe Rossi, R. L., Guerra, F. A., Plazas, M. C., Vuletic, E. E., Brücher, E., Guerra, G. D., et al. 2022. Crop damage, economic losses, and the economic damage threshold for northern corn leaf blight. Crop Protection. 154:105901 Available at: http://dx.doi.org/10.1016/j.cropro.2021.105901.\n\n\nMumford, J. D., and Norton, G. A. 1984. Economics of Decision Making in Pest Management. Annual Review of Entomology. 29:157–174 Available at: http://dx.doi.org/10.1146/annurev.en.29.010184.001105.\n\n\nReis, E. M., Hoffmann, L. L., and Blum, M. 2002. Modelo de ponto crítico para estimar os danos causados pelo oídio em cevada. Fitopatologia Brasileira. 27:644–646."
  },
  {
    "objectID": "warning-systems.html#what-types-of-systems-are-there",
    "href": "warning-systems.html#what-types-of-systems-are-there",
    "title": "18  Warning systems",
    "section": "18.4 What types of systems are there?",
    "text": "18.4 What types of systems are there?\nThese systems vary significantly in structure and design, reflecting the multitude of plant diseases, objectives, available data, control strategies, developer preferences, and operational infrastructures. Usually, warning system as based on weather inputs, but they might leverage other inputs like host, pathogen, and economic factors, catering to the complexities of disease prediction.\nDisease warning systems can include simple rules of thumb (e.g. if-then), static or dynamic disease thresholds, direct detection of inoculum, empirically-derived risk models, or complex simulation models that estimate all phases of an epidemic. The underlying models in these systems can be grouped into two different basic types: i) Empirical, based on statistical correlations with disease data; ii) Mechanistic (process-based), constructed around understood cause-effect relationships within disease progression. Let’s see some examples of these systems together with their implementation in R.\n\n18.4.1 Disease threshold\nDamaging thresholds, integral to integrated pest management in entomology, can serve as basic disease warning system. They involve economic injury levels, denoting pest abundance that equates control costs with incurred losses, and economic or action thresholds, indicating when action is necessary to avoid reaching injury levels. These concepts, while straightforward, can be quite complex in practical scenarios.\nThough less prevalent than in arthropod management, thresholds guide actions like fungicide application in plant diseases, especially those directly impacting yield through photosynthetic area reduction. However, their application is challenging for rapid, recurrent diseases affecting high-value crops, requiring prompt intervention even at minimal disease levels. With potato late blight, for instance, the first fungicide application may need to be applied by the time disease severity reaches as low as 0.1% of the foliage. Hence, disease monitoring for detection and quantification is vital for this system. In reality, for such rapid spreading and highly damaging diseases one cannot wait to “see” the disease to start protecting the crops, for which yield protection is best when applications are made preventatively.\nHowever, the concept of economic damage threshold (EDT) may be used as a criteria do indicate when to start with fungicide sprays. By definition, EDT is the amount of disease intensity (e.g. severity when dealing with foliar diseases) that corresponds to an economic loss that equates the control cost to combat the disease. A formula for the EDT was proposed by Mumford and Norton (Mumford and Norton 1984) further modified by Reis (Reis et al. 2002) for use in foliar fungal diseases, as described in Equation 3:\n\\(EDT = \\frac{F_C}{C_P . D_C} . C_e\\) ,\nwhere EDT is the disease intensity, \\(F_C\\) is the fungicide cost (USD/ha), \\(C_P\\) is the crop selling price (USD/ton), \\(D_C\\) is the damage coefficient (adjusted to potential yield) and \\(C_e\\) is the control efficacy of the fungicide (proportion of disease reduction relative to non-treated). In practice, sprays should be applied prior to reaching the EDT, which gives rise to the ADT (action damage threshold).\nIn a study on northern corn leaf blight in Argentina, the following values were used to calculate the EDT (De Rossi et al. 2022). Note that the authors adjusted the Dc to potential yield by multiplying by the potential yield value (8.5 ton.ha) in metric tons, since the Dc is normalized to one metric ton. More about Dc in the dedicated chapter on yield loss. The action damage threshold (ADT) was defined in that study as 20% reduction of the EDT.\n\ncalculate_EDT &lt;- function(Fc, Cp, Dc, Ec) {\n  EDT &lt;- (Fc / (Cp * Dc)) * Ce\n  return(EDT)\n}\nFc &lt;- 30      # fixed cost of control is 30 USD/ha.\nCp &lt;- 112     # fixed crop price is 112 USD/ton.\nDc &lt;- 0.1712  # for potential yield of 8.5 t/ha so 8.5 x 0.02015 = 0.1712.\nCe &lt;- 0.70    # control efficacy of fungicide is 70%.\nEDT_value &lt;- calculate_EDT(Fc, Cp, Dc, Ce)\nprint(EDT_value)\n\n[1] 1.09521\n\nADT = EDT_value * 0.80\nADT\n\n[1] 0.8761682\n\n\n\n\n18.4.2 Monitoring within-season weather\nDisease warning systems frequently predict conditions conducive to infection of the plant by the pathogen, with wetness and temperature being key variables for many foliar diseases. BLITECAST, the first computerized system (Krause et al. 1975), exemplifies this in its monitoring of late blight in potatoes and tomatoes. While initial inoculum is often undetectable, the presence of inoculum of the pathogen is assumed. Accurate predictions of an outbreak are possible by tracking environmental conditions favorable for disease development.\nWeather-driven disease warning systems, like FAST and Wallin (Madden 1978; Wallin 1962), continuously monitor moisture and temperature for various crop diseases. These systems calculate weather favorability or severity values from environmental data, predicting infection and disease severity. These systems guide growers on optimal spraying schedules, initiating treatments or determining application intervals based on accumulated severity values over time.\n\n18.4.2.1 Wallin model\nJ.R. Wallin developed a model in the mid-20th century focusing on forecasting potato late blight, detailed across several publications (Wallin 1962). The model tracks hourly relative humidity and temperature, emphasizing periods with relative humidity of 90% or more. It calculates the number of high-humidity hours and corresponding minimum and maximum temperatures. By accumulating ‘disease severity values’ (DSV) from plant emergence throughout the season, based on humidity and temperature measures, the model predicts the initial onset and subsequent spread of potato late blight. The table below summarizes the way the DSVs are obtained based on combinations of hours of relative humdity &gt; 90% and the air temperature within the wet period.\nTable. Relationship of temperature and relative humidity (RH) periods as used in the Wallin late blight forecasting system to predict disease severity values (0 to 4).\n\n\n\n\nDaily\nseverity\nvalue\n\n\n\n\nAverage Temperature Range\n0\n1\n2\n3\n4\n\n\n7.2 - 11.6 C\n15\n16-18\n19-21\n22-24\n&gt;25\n\n\n11.7 - 15.0 C\n12\n13-15\n16-18\n19-21\n&gt;22\n\n\n15.1 - 26.6 C\n9\n10-12\n13-15\n16-18\n&gt;19\n\n\n\nLet’s calculate DVS in R based on Wallin’s system. But first we need to download hourly weather data from NASA Power project using nasapower r package for the locality of Viçosa, MG, Brazil during the month of March 2022.\n\nlibrary(nasapower)\nweather &lt;- get_power(\n      community = \"ag\",\n      lonlat = c(-42.88, -20.7561),\n      pars = c(\"RH2M\", \"T2M\"),\n      dates = c(\"2022-03-02\", \"2022-03-31\"),\n      temporal_api = \"hourly\"\n    )\nhead(weather)\n\nNASA/POWER CERES/MERRA2 Native Resolution Hourly Data  \n Dates (month/day/year): 03/02/2022 through 03/31/2022  \n Location: Latitude  -20.7561   Longitude -42.88  \n Elevation from MERRA-2: Average for 0.5 x 0.625 degree lat/lon region = 665.27 meters \n The value for missing source data that cannot be computed or is outside of the sources availability range: NA  \n Parameter(s):  \n \n Parameters: \n RH2M     MERRA-2 Relative Humidity at 2 Meters (%) ;\n T2M      MERRA-2 Temperature at 2 Meters (C)  \n \n# A tibble: 6 × 8\n    LON   LAT  YEAR    MO    DY    HR  RH2M   T2M\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -42.9 -20.8  2022     3     1    21  93.6  20.6\n2 -42.9 -20.8  2022     3     1    22  94.4  20.2\n3 -42.9 -20.8  2022     3     1    23  95.4  19.7\n4 -42.9 -20.8  2022     3     2     0  96.3  19.4\n5 -42.9 -20.8  2022     3     2     1  97.2  19.0\n6 -42.9 -20.8  2022     3     2     2  98.1  18.8\n\n\nWe now need to obtain the leaf wetness duration variable (LW) based on hours of relative humidity &gt;90% and then the average temperature during the LW for each day. We can obtain these by grouping the variables by year, month and day and use mutate() and summarise().\n\nlibrary(tidyverse)\nweather2 &lt;- weather |&gt; \n      group_by(YEAR, MO, DY) |&gt; \n      mutate(LW = case_when(RH2M &gt; 90 ~ 1,\n                            TRUE ~ 0)) |&gt; \n      filter(LW &gt; 0) |&gt;\n      summarise(Air_LWD = mean(T2M, na.rm = TRUE),\n                LWD = n())\n\nNow we are ready to calculate the daily DSV based on Wallin’s rules on the table and inspect the first 6 rows of the new table called df_wallin.\n\ndf_wallin &lt;- weather2 |&gt; \n      mutate(\n        DSV = case_when(\n          # Temperature Range: 7.2 - 11.6 C\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &lt;= 15 ~ 0,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 15 & LWD &lt;= 18 ~ 1,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 18 & LWD &lt;= 21 ~ 2,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 21 & LWD &lt;= 24 ~ 3,\n          Air_LWD &gt;= 7.2 & Air_LWD &lt;= 11.7 & LWD &gt; 24 ~ 4,\n          \n          # Temperature Range: 11.7 - 15.0 C\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &lt;= 12 ~ 0,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 12 & LWD &lt;= 15 ~ 1,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 15 & LWD &lt;= 18 ~ 2,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 18 & LWD &lt;= 21 ~ 3,\n          Air_LWD &gt; 11.7 & Air_LWD &lt;= 15.1 & LWD &gt; 21 ~ 4,\n          \n          # Temperature Range: 15.1 - 26.6 C\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &lt;= 9 ~ 0,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 9 & LWD &lt;= 12 ~ 1,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 12 & LWD &lt;= 15 ~ 2,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 15 & LWD &lt;= 18 ~ 3,\n          Air_LWD &gt; 15.1 & Air_LWD &lt;= 26.6 & LWD &gt; 18 ~ 4,\n          \n          # Default (For temperatures out of the specified ranges or any other scenarios)\n          TRUE ~ 0  # Assigning a default value of 0\n        )\n      )\n    head(df_wallin)\n\n# A tibble: 6 × 6\n# Groups:   YEAR, MO [1]\n   YEAR    MO    DY Air_LWD   LWD   DSV\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1  2022     3     1    20.2     3     0\n2  2022     3     2    19.2    10     1\n3  2022     3     3    19.7    12     1\n4  2022     3     4    19.9    10     1\n5  2022     3     5    19.5     7     0\n6  2022     3     6    19.0     9     0\n\n\nWe can visualize the daily and cumulative DSV for the monthly period after transforming the date format using as.Date() function. The dashed horizontal line in the plot indicates the action threshold of 20 cumulative DSV, or when a spray should be applied. Please note that in real systems, the DSV is reduced to zero and another DSV counting is initiated after the spray.\n\ndf_wallin2 &lt;- df_wallin |&gt; \n  mutate(DSV2 = cumsum(DSV),\n  date = as.Date(sprintf('%04d-%02d-%02d', YEAR, MO, DY)))\n\ndf_wallin2 |&gt; \n  ggplot(aes(date, DSV))+\n  geom_col(fill = \"#339966\")+\n  geom_line(aes(date, DSV2))+\n  geom_hline(yintercept = 20, linetype = 2)+\n  annotate(geom = \"text\", x = as.Date(\"2022-03-04\"), y = 20.5, label = \"Action threshold\")+\n  r4pde::theme_r4pde()+\n  labs(x = \"Date\", y = \"Daily and cumulative DSV\")"
  },
  {
    "objectID": "warning-systems.html#footnotes",
    "href": "warning-systems.html#footnotes",
    "title": "18  Warning systems",
    "section": "",
    "text": "More comprehensively, Decision Support Systems (DSS), integrate expert input, simulations, and databases for more nuanced management recommendations, transcending simple prediction to encompass various goals within computerized frameworks.↩︎"
  },
  {
    "objectID": "warning-systems.html#risk-assessment-framework",
    "href": "warning-systems.html#risk-assessment-framework",
    "title": "18  Warning systems",
    "section": "18.2 Risk assessment framework",
    "text": "18.2 Risk assessment framework\nThere are basically two types of decisions related to plant disease management: tactical and strategical and these can be related to the distinct input data and time frames of information (i.e., historical data, pre-season, growing season, and future seasons) used in risk assessment. A framework with associated terminology can be proposed based on such relations (Figure 18.2) and its components are explained next.\n\n\n\nFigure 18.2: Framework and associated terminology for plant disease risk assessment\n\n\nHistorical data refers to those obtained from or simulated for previous years or seasons. This can include past observed weather patterns, observed or simulated disease outbreaks. In the pre-season period,data or predictions are made before the actual planting or growing season starts. This could be early predictions of weather patterns, disease risks, or market demands. During the growing season, real-time data or short-term predictions are made for risk assessment. This is the period when crops are in the fields and are actively monitored. Finally, for future seasons, forecasts or predictions (e.g. climate change scenarios) are made for subsequent years or planting seasons.\nStrategical are decisions that have a long-term impact and are typically based on historical data and forecasts for future seasons. Examples might include choosing where to plant the crops in the next years, which cultivar to plant, and making infrastructure investments. Tactical are short-term decisions typically based on real-time data or short-term predictions, especially those relevant to the current growing season. A key example include the application of fungicide sprays.\nRisk Analysis is a comprehensive assessment of potential disease risks, considering both historical data and forecasts. The goal is to understand and mitigate potential threats to agricultural production. Risk Prediction can be broken down into a) Outlook: Broad predictions or estimations about potential risks, often based on pre-season data. Example: El Niño Southern Oscilation (ENSO) information; b) Forecasting: Short-term predictions, typically for the growing season, about specific risks such as critical weather events; and c) Warning: Immediate alerts about imminent risks, like an upcoming risk of plant infection, crucial for tactical decisions.\nIn this chapter we will focus on warning system that make use of seasonal weather to provide risk information for tactical decisions."
  },
  {
    "objectID": "warning-systems.html#when-is-a-warning-system-needed",
    "href": "warning-systems.html#when-is-a-warning-system-needed",
    "title": "18  Warning systems",
    "section": "18.3 When is a warning system needed?",
    "text": "18.3 When is a warning system needed?\nThe figure and the box below provide some information on their utility. In the figure, risk analysis is defined as an approach for pathogens/diseases that are not present in a target region and for which modeling can be used for risk estimation if the disease is highly damaging were it occurs. Warning systems can be used if the disease is more erratic but still damaging when it occurs. If the epidemics are too frequent but no effective and economic control is available (for example, some nematodes and virus), plant host resistance is the way to go. If there is an effective and economic control (e.g. fungicide sprays) during the season, farmers should apply scheduled treatments given the high risk of the disease.\n\n\n\n\n\nflowchart\n    Start((Start))\n    A{Disease&lt;BR&gt;present?}\n    B{Highly&lt;BR&gt;damaging?}\n    C[Risk Analysis]\n    D{Damaging?}\n    E{Frequent&lt;BR&gt;epidemics?}\n    F[Scheduled treatments]\n    G[Warning System]\n    H{Economic&lt;BR&gt;control?}\n    I[Host Resistance]\n\n    Start --&gt; A\n    A --&gt;|N| B\n    B --&gt;|N| Start\n    D --&gt;|N| Start\n    A --&gt;|Y| D\n    D --&gt;|Y| E\n    E --&gt;|Y| H\n    B --&gt;|Y| C\n    E --&gt;|N| G\n    H --&gt;|N| I\n    H --&gt;|Y| F\n    \n\n\n\nFigure 18.3: Decision chart for the need of risk analysis, warning systems, schedule treatment or host resistance in disease management\n\n\n\n\n\n\n\n\n\n\nWhen is a warning system useful?\n\n\n\nFor operational and economical use, warning systems must fulfill four criteria to be practical. A more comprehensive discussion on this topic is presented in (Campbell and Madden. L. 1990):\n\nThey should target diseases that are prevalent and economically damaging, affecting yield or quality. Merely having severe economic consequences doesn’t guarantee a system’s practical use or attention from growers and advisors.\nThe diseases should exhibit variability across seasons in terms of onset, epidemic growth rate, severity, or another aspect, creating uncertainty in decision-making. Diseases with predictable patterns provide minimal information and little management advantage, reducing the relevance of warning systems.\nUsers should be capable of acting on the system’s alerts, necessitating available and effective control measures and sufficient response time to prevent crop damage. Systems are unhelpful if practitioners can’t adapt their strategies promptly.\nThe system must encompass comprehensive knowledge about the disease, synthesizing accurate risk estimates. Understanding the specific interactions between host, pathogen, and environment is crucial for the system’s effectiveness and relevance."
  },
  {
    "objectID": "warning-systems.html#interactive-app",
    "href": "warning-systems.html#interactive-app",
    "title": "18  Warning systems",
    "section": "18.5 Interactive app",
    "text": "18.5 Interactive app\nFor educational purposes, an interactive app was developed using R Shiny. This app demonstrates the application of the Wallin (forecast for late blight of potato) and FAST (Forecast for Alternaria solani on Tomatoes) rules for calculating DSV, determining the appropriate timing for fungicide sprays (based on a defined threshold), and counting the total sprays during a selected period (Figure 18.4).\nTo utilize the system, users should select the model, input the latitude and longitude (or choose a location from the map), and specify the time period for the simulation, such as from plant emergence to harvest. The weather data for this simulation is sourced from the NASA Power project via the {nasapower} R package.\n\n\n\nFigure 18.4: Screenshot of a web-based warning system for plant diseases based on Wallin’s and FAST rules"
  },
  {
    "objectID": "warning-systems.html#bonus-code",
    "href": "warning-systems.html#bonus-code",
    "title": "18  Warning systems",
    "section": "18.6 Bonus code",
    "text": "18.6 Bonus code\nHere is a code to calculate daily DSV values based on the FAST table (Madden 1978) that relates the hours of relative humidity &gt; 90% (wet period) and temperature during the wet period during a 24-hour period.\n\n\n\nMean temp (°C)\n0\n1\n2\n3\n4\n\n\n\n\n13-17\n0-6\n7-15\n16-20\n21+\n23+\n\n\n18-20\n0-3\n4-8\n9-15\n16-22\n23+\n\n\n21-25\n0-2\n3-5\n6-12\n13-20\n21+\n\n\n26-29\n0-3\n4-8\n9-15\n16-22\n23+\n\n\n\n\n df_fast &lt;- weather2 %&gt;% \n      mutate(\n        DSV = case_when(\n          # Temperature Range: 13 &lt;= T &lt; 18\n          Air_LWD &gt;= 13 & Air_LWD &lt; 18 & LWD &gt;= 0 & LWD &lt;= 6 ~ 0,\n          Air_LWD &gt;= 13 & Air_LWD &lt; 18 & LWD &gt;= 7 & LWD &lt;= 15 ~ 1,\n          Air_LWD &gt;= 13 & Air_LWD &lt; 18 & LWD &gt;= 16 & LWD &lt;= 20 ~ 2,\n          Air_LWD &gt;= 13 & Air_LWD &lt; 18 & LWD &gt; 20 ~ 3,\n          \n          # Temperature Range: 18 &lt;= T &lt; 21\n          Air_LWD &gt;= 18 & Air_LWD &lt; 21 & LWD &gt;= 0 & LWD &lt;= 3 ~ 0,\n          Air_LWD &gt;= 18 & Air_LWD &lt; 21 & LWD &gt;= 4 & LWD &lt;= 8 ~ 1,\n          Air_LWD &gt;= 18 & Air_LWD &lt; 21 & LWD &gt;= 9 & LWD &lt;= 15 ~ 2,\n          Air_LWD &gt;= 18 & Air_LWD &lt; 21 & LWD &gt;= 16 & LWD &lt;= 22 ~ 3,\n          Air_LWD &gt;= 18 & Air_LWD &lt; 21 & LWD &gt; 22 ~ 4,\n          \n          # Temperature Range: 21 &lt;= T &lt; 26\n          Air_LWD &gt;= 21 & Air_LWD &lt; 26 & LWD &gt;= 0 & LWD &lt;= 2 ~ 0,\n          Air_LWD &gt;= 21 & Air_LWD &lt; 26 & LWD &gt;= 3 & LWD &lt;= 5 ~ 1,\n          Air_LWD &gt;= 21 & Air_LWD &lt; 26 & LWD &gt;= 6 & LWD &lt;= 12 ~ 2,\n          Air_LWD &gt;= 21 & Air_LWD &lt; 26 & LWD &gt;= 13 & LWD &lt;= 20 ~ 3,\n          Air_LWD &gt;= 21 & Air_LWD &lt; 26 & LWD &gt; 20 ~ 4,\n          \n          # Temperature Range: 26 &lt;= T &lt; 30\n          Air_LWD &gt;= 26 & Air_LWD &lt; 30 & LWD &gt;= 0 & LWD &lt;= 3 ~ 0,\n          Air_LWD &gt;= 26 & Air_LWD &lt; 30 & LWD &gt;= 4 & LWD &lt;= 8 ~ 1,\n          Air_LWD &gt;= 26 & Air_LWD &lt; 30 & LWD &gt;= 9 & LWD &lt;= 15 ~ 2,\n          Air_LWD &gt;= 26 & Air_LWD &lt; 30 & LWD &gt;= 16 & LWD &lt;= 22 ~ 3,\n          Air_LWD &gt;= 26 & Air_LWD &lt; 30 & LWD &gt; 22 ~ 4,\n          \n          # Default (For temperatures out of the specified ranges or any other scenarios)\n          TRUE ~ 0  # Assigning a default value of 0\n        )\n      )\ndf_fast\n\n# A tibble: 31 × 6\n# Groups:   YEAR, MO [1]\n    YEAR    MO    DY Air_LWD   LWD   DSV\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1  2022     3     1    20.2     3     0\n 2  2022     3     2    19.2    10     2\n 3  2022     3     3    19.7    12     2\n 4  2022     3     4    19.9    10     2\n 5  2022     3     5    19.5     7     1\n 6  2022     3     6    19.0     9     2\n 7  2022     3     7    18.4     7     1\n 8  2022     3     8    19.2    10     2\n 9  2022     3     9    19.6    12     2\n10  2022     3    10    19.6    13     2\n# ℹ 21 more rows\n\ndf_fast2 &lt;- df_fast |&gt; \n  mutate(DSV2 = cumsum(DSV),\n  date = as.Date(sprintf('%04d-%02d-%02d', YEAR, MO, DY)))\n\ndf_fast2 |&gt; \n  ggplot(aes(date, DSV))+\n  geom_col(fill = \"#339966\")+\n  geom_line(aes(date, DSV2))+\n  geom_hline(yintercept = 20, linetype = 2)+\n  annotate(geom = \"text\", x = as.Date(\"2022-03-04\"), y = 21.5, label = \"Action threshold\")+\n  r4pde::theme_r4pde()+\n  labs(x = \"Date\", y = \"Daily and cumulative DSV\")\n\n\n\n\n\n\n\n\nCampbell, C. L., and Madden. L., V. 1990. Introduction to plant disease epidemiology. Wiley.\n\n\nDe Rossi, R. L., Guerra, F. A., Plazas, M. C., Vuletic, E. E., Brücher, E., Guerra, G. D., et al. 2022. Crop damage, economic losses, and the economic damage threshold for northern corn leaf blight. Crop Protection. 154:105901 Available at: http://dx.doi.org/10.1016/j.cropro.2021.105901.\n\n\nKrause, R. A., Massie, L. B., and Hyre, R. A. 1975. BLITECAST: A computerized forecast of potato late blight. The Plant Disease Reporter. 59:95.\n\n\nMadden, L. 1978. FAST, a forecast system for alternaria solani on tomato. Phytopathology. 68:1354 Available at: http://dx.doi.org/10.1094/phyto-68-1354.\n\n\nMumford, J. D., and Norton, G. A. 1984. Economics of Decision Making in Pest Management. Annual Review of Entomology. 29:157–174 Available at: http://dx.doi.org/10.1146/annurev.en.29.010184.001105.\n\n\nReis, E. M., Hoffmann, L. L., and Blum, M. 2002. Modelo de ponto crítico para estimar os danos causados pelo oídio em cevada. Fitopatologia Brasileira. 27:644–646.\n\n\nWallin, J. R. 1962. Summary of recent progress in predicting late blight epidemics in United States and Canada. American Potato Journal. 39:306–312 Available at: http://dx.doi.org/10.1007/bf02862155."
  },
  {
    "objectID": "warning-systems.html#risk-assessment-and-decision-framework",
    "href": "warning-systems.html#risk-assessment-and-decision-framework",
    "title": "18  Warning systems",
    "section": "18.2 Risk assessment and decision framework",
    "text": "18.2 Risk assessment and decision framework\nThere are basically two types of decisions related to plant disease management: tactical and strategical and these can be related to the distinct time frames of information (i.e., historical data, pre-season, growing season, and future seasons) used in risk assessment. A risk assessment and decision framework with associated terminology can be proposed based on such relations (Figure 18.2) and its components are explained next.\n\n\n\nFigure 18.2: Framework and associated terminology for plant disease risk assessment\n\n\nHistorical prediction refers to those obtained from or simulated for previous years or seasons. This can include past observed weather patterns, observed or simulated disease outbreaks. In the pre-season period, predictions are made before the actual planting or growing season starts. This could be based on early predictions of seasonal weather patterns. During the growing season, real-time data is used to provide short-term predictions. This is the period when crops are in the fields and are actively monitored. Finally, for future seasons, projections (e.g. climate change scenarios) can be made for subsequent years or planting seasons.\nStrategical are decisions that have a long-term impact and are typically based on historical data and forecasts for future seasons. Examples might include choosing where to plant the crops in the next years, which cultivar to plant, and making infrastructure investments. Tactical are short-term decisions typically based on real-time data or short-term predictions, especially those relevant to the current growing season. A key example include the application of fungicide sprays.\nAs to terminology, risk analysis is a comprehensive assessment of potential disease risks, considering both historical data and projections of the future. The goal is to understand and mitigate potential threats to agricultural production. Risk prediction can be broken down into a) outlook: Broad predictions or estimations about potential risks, often based on pre-season data. Example: El Niño Southern Oscilation (ENSO) information; b) forecasting: Short-term predictions, typically for the growing season, about specific risks such as critical weather events; and c) warning: Immediate alerts about imminent risks, like an upcoming risk of plant infection, crucial for tactical decisions.\nIn this chapter we will focus on warning system that make use of disease monitoring or seasonal weather to provide risk information for tactical decisions."
  }
]